{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"GenAI IDP Accelerator for Terraform","text":"<p>Welcome to the GenAI Intelligent Document Processing (IDP) Accelerator for Terraform documentation. This accelerator provides a comprehensive set of Terraform modules and configurations to deploy intelligent document processing solutions on AWS.</p>"},{"location":"index.html#what-is-genai-idp-accelerator","title":"What is GenAI IDP Accelerator?","text":"<p>The GenAI IDP Accelerator is a collection of Terraform modules that enables you to quickly deploy and configure AWS services for intelligent document processing using generative AI capabilities. It provides pre-built infrastructure components that can be easily customized and deployed to process various document types using AWS AI/ML services.</p>"},{"location":"index.html#key-features","title":"Key Features","text":"<ul> <li>Quick Deployment: Pre-configured Terraform modules for rapid deployment</li> <li>Modular Architecture: Flexible, reusable modules that can be combined as needed</li> <li>Multi-format Support: Process PDFs, images, and various document formats</li> <li>AI-Powered: Leverage AWS AI services like Textract, Comprehend, and Bedrock</li> <li>Security First: Built-in security best practices and compliance features</li> <li>Scalable: Auto-scaling capabilities to handle varying workloads</li> <li>Cost-Optimized: Efficient resource utilization and cost management</li> </ul>"},{"location":"index.html#architecture-overview","title":"Architecture Overview","text":"<p>The GenAI IDP Accelerator consists of several key components:</p> <ul> <li>Document Ingestion: S3-based document storage and event-driven processing</li> <li>Text Extraction: Amazon Textract for OCR and document analysis</li> <li>AI Processing: Amazon Bedrock for generative AI capabilities</li> <li>Data Processing: Lambda functions for custom processing logic</li> <li>Storage &amp; Analytics: DynamoDB and other AWS services for data persistence</li> <li>Monitoring: CloudWatch and X-Ray for observability</li> </ul>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Ready to get started? Check out our Getting Started Guide to deploy your first GenAI IDP solution.</p>"},{"location":"index.html#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started - Deploy your first solution</li> <li>Terraform Modules - Explore available modules</li> <li>Examples - See real-world implementations</li> <li>Deployment Guides - Step-by-step deployment instructions</li> <li>FAQs - Common questions and answers</li> <li>Contributing - How to contribute to the project</li> </ul>"},{"location":"index.html#support","title":"Support","text":"<p>If you encounter any issues or have questions, please:</p> <ol> <li>Check our FAQs for common solutions</li> <li>Review the troubleshooting guide</li> <li>Open an issue in the repository</li> </ol>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the Apache License 2.0. See the LICENSE file for details.</p>"},{"location":"contributing/index.html","title":"Contributing","text":"<p>We welcome contributions to the GenAI IDP Accelerator for Terraform! This guide will help you get started with contributing to the project, whether you're fixing bugs, adding features, improving documentation, or sharing examples.</p>"},{"location":"contributing/index.html#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"contributing/index.html#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>Help us improve by reporting bugs you encounter: - Use the issue template - Provide detailed reproduction steps - Include relevant logs and error messages - Specify your environment details</p>"},{"location":"contributing/index.html#feature-requests","title":"\u2728 Feature Requests","text":"<p>Suggest new features or improvements: - Describe the use case and problem - Explain the proposed solution - Consider backward compatibility - Provide examples if possible</p>"},{"location":"contributing/index.html#code-contributions","title":"Code Contributions","text":"<p>Contribute code improvements: - Bug fixes - New modules or features - Performance optimizations - Security enhancements</p>"},{"location":"contributing/index.html#documentation","title":"Documentation","text":"<p>Help improve documentation: - Fix typos and errors - Add missing information - Create new guides and examples - Improve existing content</p>"},{"location":"contributing/index.html#testing","title":"\ud83e\uddea Testing","text":"<p>Enhance testing coverage: - Add unit tests - Create integration tests - Test in different environments - Validate examples and guides</p>"},{"location":"contributing/index.html#getting-started","title":"Getting Started","text":""},{"location":"contributing/index.html#1-fork-the-repository","title":"1. Fork the Repository","text":"<pre><code># Fork the repository on GitLab\n# Then clone your fork\ngit clone https://gitlab.aws.dev/your-username/genaiic-idp-accelerator-terraform.git\ncd genaiic-idp-accelerator-terraform\n</code></pre>"},{"location":"contributing/index.html#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Install dependencies\nterraform --version  # Ensure 1.5.0+\naws --version        # Ensure AWS CLI v2\n\n# Set up pre-commit hooks (optional but recommended)\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"contributing/index.html#3-create-a-branch","title":"3. Create a Branch","text":"<pre><code># Create a feature branch\ngit checkout -b feature/your-feature-name\n\n# Or a bug fix branch\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/index.html#4-make-changes","title":"4. Make Changes","text":"<p>Follow our development guidelines: - Development Guide - Testing Guide - Documentation Guide</p>"},{"location":"contributing/index.html#5-test-your-changes","title":"5. Test Your Changes","text":"<pre><code># Run terraform validation\nterraform init\nterraform validate\n\n# Run tests (if available)\n./scripts/test.sh\n\n# Test examples\ncd examples/bedrock-llm-processor\nterraform init\nterraform plan\n</code></pre>"},{"location":"contributing/index.html#6-submit-a-merge-request","title":"6. Submit a Merge Request","text":"<ol> <li>Push your changes to your fork</li> <li>Create a merge request on GitLab</li> <li>Fill out the merge request template</li> <li>Wait for review and address feedback</li> </ol>"},{"location":"contributing/index.html#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/index.html#code-standards","title":"Code Standards","text":""},{"location":"contributing/index.html#terraform-code-style","title":"Terraform Code Style","text":"<pre><code># Use consistent formatting\nresource \"aws_s3_bucket\" \"example\" {\n  bucket = var.bucket_name\n\n  tags = merge(var.tags, {\n    Name = \"${var.project_name}-${var.environment}-bucket\"\n  })\n}\n\n# Use descriptive variable names\nvariable \"document_processing_timeout\" {\n  description = \"Timeout in seconds for document processing Lambda function\"\n  type        = number\n  default     = 300\n\n  validation {\n    condition     = var.document_processing_timeout &gt;= 30 &amp;&amp; var.document_processing_timeout &lt;= 900\n    error_message = \"Timeout must be between 30 and 900 seconds.\"\n  }\n}\n</code></pre>"},{"location":"contributing/index.html#file-organization","title":"File Organization","text":"<pre><code>modules/\n\u251c\u2500\u2500 module-name/\n\u2502   \u251c\u2500\u2500 main.tf           # Main resources\n\u2502   \u251c\u2500\u2500 variables.tf      # Input variables\n\u2502   \u251c\u2500\u2500 outputs.tf        # Output values\n\u2502   \u251c\u2500\u2500 versions.tf       # Provider requirements\n\u2502   \u251c\u2500\u2500 README.md         # Module documentation\n\u2502   \u2514\u2500\u2500 examples/         # Usage examples\n</code></pre>"},{"location":"contributing/index.html#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/index.html#module-documentation","title":"Module Documentation","text":"<p>Each module must include: - Clear description and purpose - Usage examples - Variable documentation - Output documentation - Requirements and dependencies</p>"},{"location":"contributing/index.html#code-comments","title":"Code Comments","text":"<pre><code># Create S3 bucket for document storage with encryption enabled\n# This bucket will store incoming documents and trigger processing\nresource \"aws_s3_bucket\" \"documents\" {\n  bucket = local.bucket_name\n\n  # Prevent accidental deletion in production\n  lifecycle {\n    prevent_destroy = var.environment == \"prod\"\n  }\n}\n</code></pre>"},{"location":"contributing/index.html#testing-requirements","title":"Testing Requirements","text":""},{"location":"contributing/index.html#module-testing","title":"Module Testing","text":"<ul> <li>All modules must have basic validation tests</li> <li>Examples must be tested and working</li> <li>Integration tests for complex workflows</li> </ul>"},{"location":"contributing/index.html#example-testing","title":"Example Testing","text":"<pre><code># Each example should include a test script\n#!/bin/bash\nset -e\n\necho \"Testing bedrock-llm-processor example...\"\n\n# Initialize and validate\nterraform init\nterraform validate\n\n# Plan deployment\nterraform plan -out=tfplan\n\n# Optional: Apply and test (for CI/CD)\nif [ \"$RUN_APPLY_TESTS\" = \"true\" ]; then\n  terraform apply tfplan\n  # Run functional tests\n  ./test-functionality.sh\n  # Clean up\n  terraform destroy -auto-approve\nfi\n\necho \"Test completed successfully!\"\n</code></pre>"},{"location":"contributing/index.html#contribution-process","title":"Contribution Process","text":""},{"location":"contributing/index.html#issue-workflow","title":"Issue Workflow","text":"<ol> <li>Issue Creation</li> <li>Use appropriate issue templates</li> <li>Provide detailed information</li> <li> <p>Add relevant labels</p> </li> <li> <p>Issue Triage</p> </li> <li>Maintainers review and label issues</li> <li>Priority and complexity assessment</li> <li> <p>Assignment to contributors</p> </li> <li> <p>Development</p> </li> <li>Create branch from main</li> <li>Implement changes</li> <li> <p>Test thoroughly</p> </li> <li> <p>Review Process</p> </li> <li>Submit merge request</li> <li>Code review by maintainers</li> <li>Address feedback</li> <li>Final approval and merge</li> </ol>"},{"location":"contributing/index.html#merge-request-guidelines","title":"Merge Request Guidelines","text":""},{"location":"contributing/index.html#title-format","title":"Title Format","text":"<pre><code>type(scope): brief description\n\nExamples:\nfeat(modules): add new processing-environment module\nfix(examples): correct variable reference in bedrock-llm-processor\ndocs(guides): update deployment guide for security\n</code></pre>"},{"location":"contributing/index.html#description-template","title":"Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Documentation update\n- [ ] Performance improvement\n- [ ] Other (please describe)\n\n## Testing\n- [ ] Terraform validate passes\n- [ ] Examples tested\n- [ ] Integration tests pass\n- [ ] Documentation updated\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] Tests added/updated\n</code></pre>"},{"location":"contributing/index.html#community-guidelines","title":"Community Guidelines","text":""},{"location":"contributing/index.html#code-of-conduct","title":"Code of Conduct","text":"<p>We follow the AWS Open Source Code of Conduct. Please:</p> <ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> <li>Maintain professional communication</li> </ul>"},{"location":"contributing/index.html#communication-channels","title":"Communication Channels","text":"<ul> <li>Issues: Bug reports and feature requests</li> <li>Merge Requests: Code discussions and reviews</li> <li>Discussions: General questions and ideas</li> </ul>"},{"location":"contributing/index.html#recognition","title":"Recognition","text":"<p>Contributors are recognized through: - Contributor list in README - Release notes acknowledgments - Community highlights</p>"},{"location":"contributing/index.html#getting-help","title":"Getting Help","text":""},{"location":"contributing/index.html#for-contributors","title":"For Contributors","text":"<ul> <li>Review existing issues and merge requests</li> <li>Ask questions in issue comments</li> <li>Reach out to maintainers for guidance</li> </ul>"},{"location":"contributing/index.html#for-maintainers","title":"For Maintainers","text":"<ul> <li>Provide timely feedback on contributions</li> <li>Help new contributors get started</li> <li>Maintain project quality and standards</li> </ul>"},{"location":"contributing/index.html#release-process","title":"Release Process","text":""},{"location":"contributing/index.html#versioning","title":"Versioning","text":"<p>We follow Semantic Versioning: - MAJOR: Breaking changes - MINOR: New features (backward compatible) - PATCH: Bug fixes (backward compatible)</p>"},{"location":"contributing/index.html#release-checklist","title":"Release Checklist","text":"<ul> <li>[ ] All tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] CHANGELOG updated</li> <li>[ ] Version bumped</li> <li>[ ] Release notes prepared</li> <li>[ ] Examples tested</li> </ul>"},{"location":"contributing/index.html#next-steps","title":"Next Steps","text":"<p>Ready to contribute? Here's how to get started:</p> <ol> <li>Browse Issues: Look for issues labeled <code>good first issue</code> or <code>help wanted</code></li> <li>Read Guidelines: Review the development guide</li> <li>Start Small: Begin with documentation or small bug fixes</li> <li>Ask Questions: Don't hesitate to ask for help or clarification</li> </ol> <p>Thank you for contributing to the GenAI IDP Accelerator!</p>"},{"location":"contributing/development.html","title":"Macro Rendering Error","text":"<p>File: <code>contributing/development.md</code></p> <p>UndefinedError: 'secrets' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 318, in top-level template code\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'secrets' is undefined\n</code></pre>"},{"location":"deployment-guides/index.html","title":"Deployment Guides","text":"<p>This section provides comprehensive guides for deploying the GenAI IDP Accelerator in different environments and scenarios. Whether you're setting up a development environment or deploying to production, these guides will help you implement best practices and avoid common pitfalls.</p>"},{"location":"deployment-guides/index.html#guide-overview","title":"Guide Overview","text":""},{"location":"deployment-guides/index.html#environment-setup","title":"Environment Setup","text":"<p>Purpose: Configure different environments (dev, staging, production)</p> <p>Topics Covered: - Multi-environment architecture - Environment-specific configurations - State management strategies - Network and VPC setup</p> <p>Best For: Teams setting up multiple environments</p>"},{"location":"deployment-guides/index.html#monitoring","title":"Monitoring","text":"<p>Purpose: Set up comprehensive monitoring and observability</p> <p>Topics Covered: - CloudWatch metrics and alarms - X-Ray distributed tracing - Custom dashboards and reports - Log aggregation and analysis</p> <p>Best For: Operations teams and production monitoring</p>"},{"location":"deployment-guides/index.html#cost-optimization","title":"Cost Optimization","text":"<p>Purpose: Optimize costs while maintaining performance</p> <p>Topics Covered: - Resource right-sizing - Bedrock model optimization - Lifecycle policies and data archiving - Cost monitoring and alerts</p> <p>Best For: Cost-conscious deployments and budget management</p>"},{"location":"deployment-guides/index.html#best-practices","title":"\u26a1 Best Practices","text":"<p>Purpose: Follow recommended practices for deployment and operations</p> <p>Topics Covered: - Security best practices - Performance optimization - Operational procedures - Testing and validation</p> <p>Best For: All deployments, especially production</p>"},{"location":"deployment-guides/index.html#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":"<p>Purpose: Diagnose and resolve common deployment issues</p> <p>Topics Covered: - Permission and access errors - Resource limit issues - Performance problems - Debugging techniques</p> <p>Best For: When things go wrong or for preventive planning</p>"},{"location":"deployment-guides/index.html#getting-started","title":"Getting Started","text":"<p>If you're new to the GenAI IDP Accelerator, we recommend following this sequence:</p> <ol> <li>Start with Environment Setup to configure your deployment environment</li> <li>Review Best Practices to understand recommended approaches</li> <li>Set up Monitoring to track your deployment health</li> <li>Implement Cost Optimization to manage expenses</li> <li>Keep Troubleshooting handy for when issues arise</li> </ol>"},{"location":"deployment-guides/index.html#quick-reference","title":"Quick Reference","text":""},{"location":"deployment-guides/index.html#common-tasks","title":"Common Tasks","text":"<p>Initial Deployment: <pre><code># 1. Configure environment\ncp terraform.tfvars.example terraform.tfvars\nvim terraform.tfvars\n\n# 2. Initialize Terraform\nterraform init\n\n# 3. Plan deployment\nterraform plan\n\n# 4. Deploy\nterraform apply\n</code></pre></p> <p>Environment Promotion: <pre><code># 1. Test in development\ncd environments/dev\nterraform apply\n\n# 2. Promote to staging\ncd ../staging\nterraform apply\n\n# 3. Deploy to production\ncd ../prod\nterraform apply\n</code></pre></p> <p>Monitoring Setup: <pre><code># Enable detailed monitoring\nterraform apply -var=\"enable_detailed_monitoring=true\"\n\n# Set up cost alerts\nterraform apply -var=\"enable_cost_alerts=true\"\n</code></pre></p>"},{"location":"deployment-guides/index.html#environment-variables","title":"Environment Variables","text":"<p>Key environment variables for deployment:</p> <pre><code># AWS Configuration\nexport AWS_REGION=us-east-1\nexport AWS_PROFILE=your-profile\n\n# Terraform Configuration\nexport TF_VAR_environment=dev\nexport TF_VAR_project_name=my-idp-project\n\n# Debugging\nexport TF_LOG=INFO\n</code></pre>"},{"location":"deployment-guides/index.html#resource-naming-conventions","title":"Resource Naming Conventions","text":"<p>Follow consistent naming patterns:</p> <pre><code># Format: {environment}-{project}-{resource-type}-{purpose}\nresource \"aws_s3_bucket\" \"documents\" {\n  bucket = \"${var.environment}-idp-documents-${random_id.suffix.hex}\"\n}\n\nresource \"aws_lambda_function\" \"processor\" {\n  function_name = \"${var.environment}-idp-document-processor\"\n}\n</code></pre>"},{"location":"deployment-guides/index.html#support-and-resources","title":"Support and Resources","text":""},{"location":"deployment-guides/index.html#documentation","title":"Documentation","text":"<ul> <li>Getting Started Guide</li> <li>Examples and Tutorials</li> <li>FAQ Section</li> </ul>"},{"location":"deployment-guides/index.html#community","title":"Community","text":"<ul> <li>Contributing Guidelines</li> <li>GitHub Issues and Discussions</li> <li>AWS Community Forums</li> </ul>"},{"location":"deployment-guides/index.html#professional-support","title":"Professional Support","text":"<ul> <li>AWS Support Plans</li> <li>AWS Professional Services</li> <li>Partner Solutions</li> </ul> <p>Next Steps: Choose the guide that best fits your current needs, or start with Environment Setup if you're just getting started.</p>"},{"location":"deployment-guides/index.html#scaling","title":"\ud83d\udcc8 Scaling","text":"<p>Purpose: Design for scalability and high availability</p> <p>Topics Covered: - Auto-scaling configurations - Load balancing strategies - Multi-region deployments - Performance optimization</p> <p>Best For: High-volume processing and enterprise deployments</p>"},{"location":"deployment-guides/index.html#troubleshooting_1","title":"\ud83d\udd27 Troubleshooting","text":"<p>Purpose: Diagnose and resolve common issues</p> <p>Topics Covered: - Common error scenarios - Debugging techniques - Performance troubleshooting - Recovery procedures</p> <p>Best For: Operations teams and issue resolution</p>"},{"location":"deployment-guides/index.html#best-practices_1","title":"\u2705 Best Practices","text":"<p>Purpose: Follow industry best practices and lessons learned</p> <p>Topics Covered: - Infrastructure as Code principles - Testing strategies - Documentation standards - Operational procedures</p> <p>Best For: All deployments seeking to follow best practices</p>"},{"location":"deployment-guides/index.html#deployment-scenarios","title":"Deployment Scenarios","text":""},{"location":"deployment-guides/index.html#development-environment","title":"Development Environment","text":"<p>Characteristics: - Single developer or small team - Cost optimization prioritized - Simplified security model - Quick iteration cycles</p> <p>Recommended Guides: 1. Environment Setup - Basic configuration 2. Cost Optimization - Minimize development costs 3. Troubleshooting - Debug common issues</p>"},{"location":"deployment-guides/index.html#staging-environment","title":"Staging Environment","text":"<p>Characteristics: - Production-like configuration - Testing and validation focus - Moderate security requirements - Performance testing capabilities</p> <p>Recommended Guides: 1. Environment Setup - Multi-environment setup 2. Security - Basic security hardening 3. Monitoring - Testing observability 4. Best Practices - Quality assurance</p>"},{"location":"deployment-guides/index.html#production-environment","title":"Production Environment","text":"<p>Characteristics: - High availability requirements - Strict security and compliance - Comprehensive monitoring - Cost efficiency at scale</p> <p>Recommended Guides: 1. Security - Complete security implementation 2. Monitoring - Full observability stack 3. Scaling - High availability design 4. Cost Optimization - Production cost management 5. Best Practices - Operational excellence</p>"},{"location":"deployment-guides/index.html#quick-reference_1","title":"Quick Reference","text":""},{"location":"deployment-guides/index.html#pre-deployment-checklist","title":"Pre-Deployment Checklist","text":"<ul> <li>[ ] AWS account and credentials configured</li> <li>[ ] Terraform installed and configured</li> <li>[ ] Required AWS service quotas verified</li> <li>[ ] Security requirements documented</li> <li>[ ] Monitoring requirements defined</li> <li>[ ] Cost budget established</li> <li>[ ] Disaster recovery plan created</li> </ul>"},{"location":"deployment-guides/index.html#post-deployment-checklist","title":"Post-Deployment Checklist","text":"<ul> <li>[ ] All resources deployed successfully</li> <li>[ ] Security configurations verified</li> <li>[ ] Monitoring and alerting configured</li> <li>[ ] Performance baselines established</li> <li>[ ] Documentation updated</li> <li>[ ] Team training completed</li> <li>[ ] Operational procedures documented</li> </ul>"},{"location":"deployment-guides/index.html#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"deployment-guides/index.html#single-region-deployment","title":"Single Region Deployment","text":"<pre><code>graph TB\n    A[Users] --&gt; B[API Gateway]\n    B --&gt; C[Lambda Functions]\n    C --&gt; D[S3 Buckets]\n    C --&gt; E[DynamoDB]\n    C --&gt; F[Textract]\n    C --&gt; G[Bedrock]\n    H[CloudWatch] --&gt; C\n    I[IAM] --&gt; C</code></pre> <p>Use Cases: Most deployments, cost-effective, simpler management</p>"},{"location":"deployment-guides/index.html#multi-region-deployment","title":"Multi-Region Deployment","text":"<pre><code>graph TB\n    A[Users] --&gt; B[Route 53]\n    B --&gt; C[Region 1]\n    B --&gt; D[Region 2]\n    C --&gt; E[Primary Stack]\n    D --&gt; F[Secondary Stack]\n    G[Cross-Region Replication] --&gt; E\n    G --&gt; F</code></pre> <p>Use Cases: High availability, disaster recovery, global distribution</p>"},{"location":"deployment-guides/index.html#hybrid-deployment","title":"Hybrid Deployment","text":"<pre><code>graph TB\n    A[On-Premises] --&gt; B[VPN/Direct Connect]\n    B --&gt; C[AWS VPC]\n    C --&gt; D[GenAI IDP Stack]\n    E[Existing Systems] --&gt; A\n    F[Cloud-Native Apps] --&gt; C</code></pre> <p>Use Cases: Legacy system integration, compliance requirements, gradual migration</p>"},{"location":"deployment-guides/index.html#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"deployment-guides/index.html#development","title":"Development","text":"<pre><code># terraform.tfvars\nenvironment = \"dev\"\ninstance_types = {\n  lambda_memory = 512\n  lambda_timeout = 30\n}\nenable_detailed_monitoring = false\nbackup_retention_days = 7\n</code></pre>"},{"location":"deployment-guides/index.html#staging","title":"Staging","text":"<pre><code># terraform.tfvars\nenvironment = \"staging\"\ninstance_types = {\n  lambda_memory = 1024\n  lambda_timeout = 60\n}\nenable_detailed_monitoring = true\nbackup_retention_days = 30\n</code></pre>"},{"location":"deployment-guides/index.html#production","title":"Production","text":"<pre><code># terraform.tfvars\nenvironment = \"prod\"\ninstance_types = {\n  lambda_memory = 2048\n  lambda_timeout = 300\n}\nenable_detailed_monitoring = true\nbackup_retention_days = 90\nenable_multi_az = true\n</code></pre>"},{"location":"deployment-guides/index.html#deployment-automation","title":"Deployment Automation","text":""},{"location":"deployment-guides/index.html#cicd-pipeline-example","title":"CI/CD Pipeline Example","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - validate\n  - plan\n  - deploy\n\nterraform-validate:\n  stage: validate\n  script:\n    - terraform init\n    - terraform validate\n    - terraform fmt -check\n\nterraform-plan:\n  stage: plan\n  script:\n    - terraform plan -out=tfplan\n  artifacts:\n    paths:\n      - tfplan\n\nterraform-deploy:\n  stage: deploy\n  script:\n    - terraform apply tfplan\n  when: manual\n  only:\n    - main\n</code></pre>"},{"location":"deployment-guides/index.html#support-and-resources_1","title":"Support and Resources","text":""},{"location":"deployment-guides/index.html#getting-help","title":"Getting Help","text":"<ol> <li>Documentation: Start with relevant guide sections</li> <li>Troubleshooting: Check the troubleshooting guide</li> <li>Community: Join discussions in the repository</li> <li>Support: Open issues for bugs or feature requests</li> </ol>"},{"location":"deployment-guides/index.html#additional-resources","title":"Additional Resources","text":"<ul> <li>AWS Well-Architected Framework</li> <li>Terraform Best Practices</li> <li>AWS Security Best Practices</li> </ul>"},{"location":"deployment-guides/index.html#next-steps","title":"Next Steps","text":"<ol> <li>Assess Requirements: Determine your deployment scenario and requirements</li> <li>Choose Guides: Select relevant guides based on your needs</li> <li>Plan Deployment: Create a deployment plan with timelines and responsibilities</li> <li>Execute: Follow the guides step-by-step</li> <li>Validate: Verify deployment success and performance</li> <li>Optimize: Continuously improve based on operational experience</li> </ol> <p>Ready to deploy? Start with the Environment Setup guide or jump to the specific guide that matches your needs.</p>"},{"location":"deployment-guides/best-practices.html","title":"Best Practices","text":"<p>This guide covers recommended practices for deploying and operating the GenAI IDP Accelerator with Terraform.</p>"},{"location":"deployment-guides/best-practices.html#deployment-best-practices","title":"Deployment Best Practices","text":""},{"location":"deployment-guides/best-practices.html#environment-management","title":"Environment Management","text":"<p>Use Separate Environments: <pre><code># terraform/environments/dev/terraform.tfvars\nenvironment = \"dev\"\nregion = \"us-east-1\"\n\n# Reduced resources for development\nlambda_memory_size = 512\nlambda_timeout = 60\nenable_detailed_monitoring = false\n</code></pre></p> <p>Consistent Tagging: <pre><code>locals {\n  common_tags = {\n    Environment = var.environment\n    Project     = \"genai-idp-accelerator\"\n    Owner       = var.owner\n    Terraform   = \"true\"\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#state-management","title":"State Management","text":"<p>Remote State Backend: <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"terraform-state-${var.environment}\"\n    key            = \"idp-accelerator/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre></p> <p>State Locking: <pre><code>resource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name           = \"terraform-locks\"\n  billing_mode   = \"PAY_PER_REQUEST\"\n  hash_key       = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#security-best-practices","title":"Security Best Practices","text":""},{"location":"deployment-guides/best-practices.html#iam-permissions","title":"IAM Permissions","text":"<p>Least Privilege Principle: <pre><code>resource \"aws_iam_policy\" \"lambda_policy\" {\n  name = \"${var.environment}-idp-lambda-policy\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"textract:DetectDocumentText\",\n          \"textract:AnalyzeDocument\"\n        ]\n        Resource = \"*\"\n      },\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"s3:GetObject\",\n          \"s3:PutObject\"\n        ]\n        Resource = \"${aws_s3_bucket.documents.arn}/*\"\n      }\n    ]\n  })\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#data-protection","title":"Data Protection","text":"<p>Encryption at Rest: <pre><code>resource \"aws_s3_bucket_server_side_encryption_configuration\" \"documents\" {\n  bucket = aws_s3_bucket.documents.id\n\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = \"AES256\"\n    }\n  }\n}\n</code></pre></p> <p>Encryption in Transit: - Use HTTPS for all API endpoints - Enable SSL/TLS for data transfer - Use VPC endpoints where possible</p>"},{"location":"deployment-guides/best-practices.html#performance-best-practices","title":"Performance Best Practices","text":""},{"location":"deployment-guides/best-practices.html#lambda-optimization","title":"Lambda Optimization","text":"<p>Memory and Timeout Settings: <pre><code>resource \"aws_lambda_function\" \"document_processor\" {\n  memory_size = 1024  # Adjust based on workload\n  timeout     = 300   # 5 minutes for document processing\n\n  environment {\n    variables = {\n      POWERTOOLS_LOG_LEVEL = \"INFO\"\n    }\n  }\n}\n</code></pre></p> <p>Connection Reuse: <pre><code>import boto3\n\n# Initialize clients outside handler for reuse\ns3_client = boto3.client('s3')\ntextract_client = boto3.client('textract')\n\ndef lambda_handler(event, context):\n    # Use pre-initialized clients\n    response = textract_client.detect_document_text(...)\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#storage-optimization","title":"Storage Optimization","text":"<p>S3 Lifecycle Policies: <pre><code>resource \"aws_s3_bucket_lifecycle_configuration\" \"documents_lifecycle\" {\n  bucket = aws_s3_bucket.documents.id\n\n  rule {\n    id     = \"transition_to_ia\"\n    status = \"Enabled\"\n\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#cost-optimization","title":"Cost Optimization","text":""},{"location":"deployment-guides/best-practices.html#resource-right-sizing","title":"Resource Right-Sizing","text":"<p>Environment-Specific Configurations: <pre><code>locals {\n  config = {\n    dev = {\n      lambda_memory = 512\n      lambda_timeout = 60\n      enable_monitoring = false\n    }\n    prod = {\n      lambda_memory = 2048\n      lambda_timeout = 900\n      enable_monitoring = true\n    }\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<p>Cost Budgets: <pre><code>resource \"aws_budgets_budget\" \"monthly_budget\" {\n  name         = \"${var.environment}-idp-budget\"\n  budget_type  = \"COST\"\n  limit_amount = var.monthly_budget_limit\n  limit_unit   = \"USD\"\n  time_unit    = \"MONTHLY\"\n\n  cost_filters = {\n    Tag = [\"Environment:${var.environment}\"]\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#monitoring-best-practices","title":"Monitoring Best Practices","text":""},{"location":"deployment-guides/best-practices.html#cloudwatch-configuration","title":"CloudWatch Configuration","text":"<p>Log Groups: <pre><code>resource \"aws_cloudwatch_log_group\" \"lambda_logs\" {\n  name              = \"/aws/lambda/${var.environment}-idp-processor\"\n  retention_in_days = var.log_retention_days\n}\n</code></pre></p> <p>Alarms: <pre><code>resource \"aws_cloudwatch_metric_alarm\" \"lambda_errors\" {\n  alarm_name          = \"${var.environment}-lambda-errors\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"5\"\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#operational-best-practices","title":"Operational Best Practices","text":""},{"location":"deployment-guides/best-practices.html#deployment-process","title":"Deployment Process","text":"<p>Validation Steps: 1. Run <code>terraform plan</code> and review changes 2. Test in development environment first 3. Use gradual rollout for production 4. Monitor deployment metrics</p> <p>Rollback Strategy: <pre><code># Keep previous state file versions\nterraform state pull &gt; terraform.tfstate.backup\n\n# Quick rollback if needed\nterraform apply -target=aws_lambda_function.processor \\\n  -var=\"lambda_version=previous\"\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#documentation","title":"Documentation","text":"<p>Infrastructure Documentation: - Document all custom configurations - Maintain architecture diagrams - Keep runbooks updated - Record operational procedures</p>"},{"location":"deployment-guides/best-practices.html#backup-and-recovery","title":"Backup and Recovery","text":"<p>State File Backup: <pre><code>resource \"aws_s3_bucket_versioning\" \"state_bucket_versioning\" {\n  bucket = aws_s3_bucket.terraform_state.id\n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n</code></pre></p> <p>Data Backup: <pre><code>resource \"aws_dynamodb_table\" \"documents\" {\n  point_in_time_recovery {\n    enabled = true\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#testing-best-practices","title":"Testing Best Practices","text":""},{"location":"deployment-guides/best-practices.html#infrastructure-testing","title":"Infrastructure Testing","text":"<p>Validation: <pre><code># Validate Terraform configuration\nterraform validate\n\n# Check formatting\nterraform fmt -check\n\n# Security scanning\ntfsec .\n</code></pre></p> <p>Integration Testing: <pre><code># Test deployment in staging\nterraform plan -var-file=\"staging.tfvars\"\nterraform apply -var-file=\"staging.tfvars\"\n\n# Run smoke tests\n./scripts/smoke-tests.sh staging\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment-guides/best-practices.html#common-issues","title":"Common Issues","text":"<p>Permission Errors: - Check IAM policies and roles - Verify service-linked roles exist - Review resource policies</p> <p>Resource Limits: - Check AWS service quotas - Monitor resource utilization - Request quota increases if needed</p> <p>State Issues: - Use <code>terraform refresh</code> to sync state - Import existing resources if needed - Handle state lock conflicts</p>"},{"location":"deployment-guides/best-practices.html#debugging-tools","title":"Debugging Tools","text":"<p>Terraform Debugging: <pre><code>export TF_LOG=DEBUG\nterraform apply\n</code></pre></p> <p>AWS CLI Debugging: <pre><code>aws logs describe-log-groups --debug\naws lambda get-function --function-name processor --debug\n</code></pre></p>"},{"location":"deployment-guides/best-practices.html#maintenance","title":"Maintenance","text":""},{"location":"deployment-guides/best-practices.html#regular-tasks","title":"Regular Tasks","text":"<p>Weekly: - Review CloudWatch alarms and metrics - Check cost and usage reports - Update documentation as needed</p> <p>Monthly: - Review and update IAM permissions - Analyze performance metrics - Plan capacity adjustments</p> <p>Quarterly: - Security review and updates - Disaster recovery testing - Architecture review</p>"},{"location":"deployment-guides/best-practices.html#updates-and-patches","title":"Updates and Patches","text":"<p>Terraform Updates: <pre><code># Update Terraform version\nterraform version\nterraform init -upgrade\n\n# Update provider versions\nterraform init -upgrade\n</code></pre></p> <p>Lambda Runtime Updates: <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  runtime = \"python3.11\"  # Keep updated\n}\n</code></pre></p> <p>For more detailed information, see: - Environment Setup - Monitoring Guide - Cost Optimization - Troubleshooting Guide</p>"},{"location":"deployment-guides/cost-optimization.html","title":"Cost Optimization Guide","text":"<p>This guide provides strategies and best practices for optimizing costs when running the GenAI IDP Accelerator.</p>"},{"location":"deployment-guides/cost-optimization.html#cost-structure-overview","title":"Cost Structure Overview","text":""},{"location":"deployment-guides/cost-optimization.html#primary-cost-components","title":"Primary Cost Components","text":"<ol> <li>Amazon Bedrock: AI model inference costs (typically 40-60% of total)</li> <li>AWS Lambda: Compute costs for processing functions (15-25%)</li> <li>Amazon S3: Storage costs for documents and results (10-20%)</li> <li>Amazon Textract: Document analysis costs (10-15%)</li> <li>Amazon DynamoDB: Metadata storage costs (5-10%)</li> <li>Data Transfer: Network costs (2-5%)</li> </ol>"},{"location":"deployment-guides/cost-optimization.html#cost-breakdown-by-environment","title":"Cost Breakdown by Environment","text":"<pre><code>pie title Cost Distribution by Service\n    \"Amazon Bedrock\" : 50\n    \"AWS Lambda\" : 20\n    \"Amazon S3\" : 15\n    \"Amazon Textract\" : 10\n    \"Other Services\" : 5</code></pre>"},{"location":"deployment-guides/cost-optimization.html#amazon-bedrock-optimization","title":"Amazon Bedrock Optimization","text":""},{"location":"deployment-guides/cost-optimization.html#model-selection-strategy","title":"Model Selection Strategy","text":"<p>Cost-Performance Matrix: <pre><code># Variable model configuration based on use case\nvariable \"bedrock_model_config\" {\n  description = \"Bedrock model configuration by use case\"\n  type = map(object({\n    model_id     = string\n    max_tokens   = number\n    cost_per_1k  = number\n    use_case     = string\n  }))\n\n  default = {\n    \"simple_extraction\" = {\n      model_id     = \"amazon.titan-text-lite-v1\"\n      max_tokens   = 500\n      cost_per_1k  = 0.0003\n      use_case     = \"Basic text extraction and simple classification\"\n    }\n    \"complex_analysis\" = {\n      model_id     = \"anthropic.claude-3-haiku-20240307-v1:0\"\n      max_tokens   = 1000\n      cost_per_1k  = 0.00025\n      use_case     = \"Complex document analysis and reasoning\"\n    }\n    \"premium_processing\" = {\n      model_id     = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n      max_tokens   = 2000\n      cost_per_1k  = 0.003\n      use_case     = \"High-accuracy processing for critical documents\"\n    }\n  }\n}\n\n# Dynamic model selection based on document type\nlocals {\n  model_selection_logic = {\n    \"invoice\"    = var.bedrock_model_config[\"simple_extraction\"]\n    \"contract\"   = var.bedrock_model_config[\"complex_analysis\"]\n    \"legal_doc\"  = var.bedrock_model_config[\"premium_processing\"]\n    \"default\"    = var.bedrock_model_config[\"simple_extraction\"]\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/cost-optimization.html#token-optimization","title":"Token Optimization","text":"<p>Prompt Engineering for Cost Efficiency: <pre><code># Cost-optimized prompt templates\nCOST_OPTIMIZED_PROMPTS = {\n    \"invoice_extraction\": {\n        \"prompt\": \"\"\"Extract key data from this invoice:\n- Invoice number\n- Date\n- Total amount\n- Vendor name\n\nText: {document_text}\n\nFormat as JSON.\"\"\",\n        \"max_tokens\": 200,\n        \"estimated_cost\": \"$0.00006\"\n    },\n\n    \"contract_analysis\": {\n        \"prompt\": \"\"\"Analyze contract for:\n1. Key terms\n2. Dates\n3. Parties involved\n4. Financial obligations\n\nText: {document_text}\n\nProvide concise summary.\"\"\",\n        \"max_tokens\": 500,\n        \"estimated_cost\": \"$0.00015\"\n    }\n}\n\ndef optimize_prompt_for_cost(document_type, document_text):\n    \"\"\"Select cost-optimized prompt based on document type\"\"\"\n    template = COST_OPTIMIZED_PROMPTS.get(document_type, COST_OPTIMIZED_PROMPTS[\"invoice_extraction\"])\n\n    # Truncate input if too long\n    max_input_tokens = 2000  # Adjust based on model limits\n    if len(document_text.split()) &gt; max_input_tokens:\n        document_text = ' '.join(document_text.split()[:max_input_tokens])\n\n    return template[\"prompt\"].format(document_text=document_text), template[\"max_tokens\"]\n</code></pre></p>"},{"location":"deployment-guides/cost-optimization.html#caching-strategy","title":"Caching Strategy","text":"<pre><code># DynamoDB table for caching Bedrock responses\nresource \"aws_dynamodb_table\" \"bedrock_cache\" {\n  name           = \"${var.environment}-idp-bedrock-cache\"\n  billing_mode   = \"PAY_PER_REQUEST\"\n  hash_key       = \"content_hash\"\n\n  attribute {\n    name = \"content_hash\"\n    type = \"S\"\n  }\n\n  ttl {\n    attribute_name = \"expires_at\"\n    enabled        = true\n  }\n\n  tags = merge(local.common_tags, {\n    Purpose = \"Cost optimization through response caching\"\n  })\n}\n</code></pre> <pre><code>import hashlib\nimport json\nimport time\nfrom typing import Optional\n\ndef get_content_hash(text: str, model_id: str, prompt: str) -&gt; str:\n    \"\"\"Generate hash for caching purposes\"\"\"\n    content = f\"{text}:{model_id}:{prompt}\"\n    return hashlib.sha256(content.encode()).hexdigest()\n\ndef check_bedrock_cache(content_hash: str) -&gt; Optional[dict]:\n    \"\"\"Check if response is cached\"\"\"\n    try:\n        response = dynamodb.get_item(\n            TableName=CACHE_TABLE_NAME,\n            Key={'content_hash': {'S': content_hash}}\n        )\n\n        if 'Item' in response:\n            # Check if not expired\n            expires_at = int(response['Item']['expires_at']['N'])\n            if expires_at &gt; int(time.time()):\n                return json.loads(response['Item']['response']['S'])\n    except Exception as e:\n        logger.warning(f\"Cache check failed: {e}\")\n\n    return None\n\ndef cache_bedrock_response(content_hash: str, response: dict, ttl_hours: int = 24):\n    \"\"\"Cache Bedrock response\"\"\"\n    try:\n        expires_at = int(time.time()) + (ttl_hours * 3600)\n\n        dynamodb.put_item(\n            TableName=CACHE_TABLE_NAME,\n            Item={\n                'content_hash': {'S': content_hash},\n                'response': {'S': json.dumps(response)},\n                'expires_at': {'N': str(expires_at)},\n                'created_at': {'N': str(int(time.time()))}\n            }\n        )\n    except Exception as e:\n        logger.warning(f\"Cache write failed: {e}\")\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#lambda-optimization","title":"Lambda Optimization","text":""},{"location":"deployment-guides/cost-optimization.html#right-sizing-functions","title":"Right-Sizing Functions","text":"<pre><code># Environment-specific Lambda configurations\nlocals {\n  lambda_configs = {\n    dev = {\n      memory_size = 512\n      timeout     = 60\n      reserved_concurrency = 10\n    }\n    staging = {\n      memory_size = 1024\n      timeout     = 300\n      reserved_concurrency = 50\n    }\n    prod = {\n      memory_size = 2048\n      timeout     = 900\n      reserved_concurrency = 100\n    }\n  }\n\n  current_config = local.lambda_configs[var.environment]\n}\n\nresource \"aws_lambda_function\" \"document_processor\" {\n  function_name = \"${var.environment}-idp-document-processor\"\n\n  # Optimized configuration\n  memory_size                    = local.current_config.memory_size\n  timeout                       = local.current_config.timeout\n  reserved_concurrent_executions = local.current_config.reserved_concurrency\n\n  # Cost optimization settings\n  dead_letter_config {\n    target_arn = aws_sqs_queue.dlq.arn\n  }\n\n  environment {\n    variables = {\n      POWERTOOLS_LOG_LEVEL = var.environment == \"prod\" ? \"INFO\" : \"DEBUG\"\n      ENABLE_DETAILED_LOGGING = var.environment == \"dev\" ? \"true\" : \"false\"\n    }\n  }\n\n  tags = merge(local.common_tags, {\n    CostOptimization = \"enabled\"\n  })\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#provisioned-concurrency-optimization","title":"Provisioned Concurrency Optimization","text":"<pre><code># Conditional provisioned concurrency for production\nresource \"aws_lambda_provisioned_concurrency_config\" \"document_processor\" {\n  count                     = var.environment == \"prod\" &amp;&amp; var.enable_provisioned_concurrency ? 1 : 0\n  function_name            = aws_lambda_function.document_processor.function_name\n  provisioned_concurrent_executions = var.provisioned_concurrency_count\n  qualifier                = aws_lambda_function.document_processor.version\n}\n\n# Auto-scaling for provisioned concurrency\nresource \"aws_appautoscaling_target\" \"lambda_concurrency\" {\n  count              = var.environment == \"prod\" &amp;&amp; var.enable_provisioned_concurrency ? 1 : 0\n  max_capacity       = var.max_provisioned_concurrency\n  min_capacity       = var.min_provisioned_concurrency\n  resource_id        = \"function:${aws_lambda_function.document_processor.function_name}:provisioned\"\n  scalable_dimension = \"lambda:function:ProvisionedConcurrency\"\n  service_namespace  = \"lambda\"\n}\n\nresource \"aws_appautoscaling_policy\" \"lambda_concurrency_policy\" {\n  count              = var.environment == \"prod\" &amp;&amp; var.enable_provisioned_concurrency ? 1 : 0\n  name               = \"${var.environment}-idp-lambda-concurrency-policy\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.lambda_concurrency[0].resource_id\n  scalable_dimension = aws_appautoscaling_target.lambda_concurrency[0].scalable_dimension\n  service_namespace  = aws_appautoscaling_target.lambda_concurrency[0].service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    target_value = 70.0\n\n    predefined_metric_specification {\n      predefined_metric_type = \"LambdaProvisionedConcurrencyUtilization\"\n    }\n  }\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#storage-optimization","title":"Storage Optimization","text":""},{"location":"deployment-guides/cost-optimization.html#s3-cost-optimization","title":"S3 Cost Optimization","text":"<pre><code># Intelligent tiering and lifecycle policies\nresource \"aws_s3_bucket_lifecycle_configuration\" \"documents_lifecycle\" {\n  bucket = aws_s3_bucket.documents.id\n\n  rule {\n    id     = \"cost_optimization\"\n    status = \"Enabled\"\n\n    # Transition to IA after 30 days\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n\n    # Transition to Glacier after 90 days\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n\n    # Transition to Deep Archive after 365 days\n    transition {\n      days          = 365\n      storage_class = \"DEEP_ARCHIVE\"\n    }\n\n    # Delete incomplete multipart uploads\n    abort_incomplete_multipart_upload {\n      days_after_initiation = 7\n    }\n\n    # Delete old versions\n    noncurrent_version_transition {\n      noncurrent_days = 30\n      storage_class   = \"STANDARD_IA\"\n    }\n\n    noncurrent_version_expiration {\n      noncurrent_days = 90\n    }\n  }\n\n  rule {\n    id     = \"processed_documents_cleanup\"\n    status = \"Enabled\"\n\n    filter {\n      prefix = \"processed/\"\n    }\n\n    # Delete processed documents after retention period\n    expiration {\n      days = var.processed_document_retention_days\n    }\n  }\n}\n\n# Intelligent tiering for automatic cost optimization\nresource \"aws_s3_bucket_intelligent_tiering_configuration\" \"documents_intelligent_tiering\" {\n  bucket = aws_s3_bucket.documents.id\n  name   = \"EntireBucket\"\n\n  tiering {\n    access_tier = \"ARCHIVE_ACCESS\"\n    days        = 90\n  }\n\n  tiering {\n    access_tier = \"DEEP_ARCHIVE_ACCESS\"\n    days        = 180\n  }\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#dynamodb-optimization","title":"DynamoDB Optimization","text":"<pre><code># On-demand billing for variable workloads\nresource \"aws_dynamodb_table\" \"document_metadata\" {\n  name         = \"${var.environment}-idp-document-metadata\"\n  billing_mode = var.environment == \"prod\" ? \"PROVISIONED\" : \"PAY_PER_REQUEST\"\n\n  # Provisioned capacity for production (with auto-scaling)\n  read_capacity  = var.environment == \"prod\" ? var.dynamodb_read_capacity : null\n  write_capacity = var.environment == \"prod\" ? var.dynamodb_write_capacity : null\n\n  hash_key = \"document_id\"\n\n  attribute {\n    name = \"document_id\"\n    type = \"S\"\n  }\n\n  attribute {\n    name = \"status\"\n    type = \"S\"\n  }\n\n  attribute {\n    name = \"created_at\"\n    type = \"S\"\n  }\n\n  # GSI for status queries\n  global_secondary_index {\n    name            = \"status-created-index\"\n    hash_key        = \"status\"\n    range_key       = \"created_at\"\n    projection_type = \"KEYS_ONLY\"\n\n    read_capacity  = var.environment == \"prod\" ? var.dynamodb_gsi_read_capacity : null\n    write_capacity = var.environment == \"prod\" ? var.dynamodb_gsi_write_capacity : null\n  }\n\n  # TTL for automatic cleanup\n  ttl {\n    attribute_name = \"expires_at\"\n    enabled        = true\n  }\n\n  tags = local.common_tags\n}\n\n# Auto-scaling for production\nresource \"aws_appautoscaling_target\" \"dynamodb_table_read_target\" {\n  count              = var.environment == \"prod\" ? 1 : 0\n  max_capacity       = var.dynamodb_max_read_capacity\n  min_capacity       = var.dynamodb_min_read_capacity\n  resource_id        = \"table/${aws_dynamodb_table.document_metadata.name}\"\n  scalable_dimension = \"dynamodb:table:ReadCapacityUnits\"\n  service_namespace  = \"dynamodb\"\n}\n\nresource \"aws_appautoscaling_policy\" \"dynamodb_table_read_policy\" {\n  count              = var.environment == \"prod\" ? 1 : 0\n  name               = \"${var.environment}-idp-dynamodb-read-policy\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.dynamodb_table_read_target[0].resource_id\n  scalable_dimension = aws_appautoscaling_target.dynamodb_table_read_target[0].scalable_dimension\n  service_namespace  = aws_appautoscaling_target.dynamodb_table_read_target[0].service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    target_value = 70.0\n\n    predefined_metric_specification {\n      predefined_metric_type = \"DynamoDBReadCapacityUtilization\"\n    }\n  }\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#monitoring-and-alerting-for-cost-control","title":"Monitoring and Alerting for Cost Control","text":""},{"location":"deployment-guides/cost-optimization.html#cost-budgets","title":"Cost Budgets","text":"<pre><code># Monthly budget with alerts\nresource \"aws_budgets_budget\" \"idp_monthly_budget\" {\n  name         = \"${var.environment}-idp-monthly-budget\"\n  budget_type  = \"COST\"\n  limit_amount = var.monthly_budget_limit\n  limit_unit   = \"USD\"\n  time_unit    = \"MONTHLY\"\n\n  cost_filters = {\n    Tag = [\n      \"Environment:${var.environment}\",\n      \"Project:genai-idp-accelerator\"\n    ]\n  }\n\n  # 80% threshold\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                 = 80\n    threshold_type            = \"PERCENTAGE\"\n    notification_type         = \"ACTUAL\"\n    subscriber_email_addresses = var.budget_alert_emails\n  }\n\n  # 100% threshold\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                 = 100\n    threshold_type            = \"PERCENTAGE\"\n    notification_type          = \"ACTUAL\"\n    subscriber_email_addresses = var.budget_alert_emails\n  }\n\n  # 120% forecast\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                 = 120\n    threshold_type            = \"PERCENTAGE\"\n    notification_type          = \"FORECASTED\"\n    subscriber_email_addresses = var.budget_alert_emails\n  }\n}\n\n# Service-specific budgets\nresource \"aws_budgets_budget\" \"bedrock_budget\" {\n  name         = \"${var.environment}-idp-bedrock-budget\"\n  budget_type  = \"COST\"\n  limit_amount = var.bedrock_budget_limit\n  limit_unit   = \"USD\"\n  time_unit    = \"MONTHLY\"\n\n  cost_filters = {\n    Service = [\"Amazon Bedrock\"]\n    Tag = [\"Environment:${var.environment}\"]\n  }\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                 = 90\n    threshold_type            = \"PERCENTAGE\"\n    notification_type          = \"ACTUAL\"\n    subscriber_email_addresses = var.budget_alert_emails\n  }\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#cost-anomaly-detection","title":"Cost Anomaly Detection","text":"<pre><code>resource \"aws_ce_anomaly_detector\" \"idp_cost_anomaly\" {\n  name         = \"${var.environment}-idp-cost-anomaly-detector\"\n  monitor_type = \"DIMENSIONAL\"\n\n  specification = jsonencode({\n    Dimension = \"SERVICE\"\n    MatchOptions = [\"EQUALS\"]\n    Values = [\n      \"Amazon Bedrock\",\n      \"AWS Lambda\",\n      \"Amazon S3\",\n      \"Amazon Textract\",\n      \"Amazon DynamoDB\"\n    ]\n  })\n\n  tags = local.common_tags\n}\n\nresource \"aws_ce_anomaly_subscription\" \"idp_cost_anomaly_subscription\" {\n  name      = \"${var.environment}-idp-cost-anomaly-subscription\"\n  frequency = \"DAILY\"\n\n  monitor_arn_list = [aws_ce_anomaly_detector.idp_cost_anomaly.arn]\n\n  subscriber {\n    type    = \"EMAIL\"\n    address = var.cost_anomaly_alert_email\n  }\n\n  threshold_expression {\n    and {\n      dimension {\n        key           = \"ANOMALY_TOTAL_IMPACT_ABSOLUTE\"\n        values        = [var.cost_anomaly_threshold]\n        match_options = [\"GREATER_THAN_OR_EQUAL\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#batch-processing-optimization","title":"Batch Processing Optimization","text":""},{"location":"deployment-guides/cost-optimization.html#cost-effective-batch-processing","title":"Cost-Effective Batch Processing","text":"<pre><code># SQS queue for batch processing\nresource \"aws_sqs_queue\" \"batch_processing_queue\" {\n  name                       = \"${var.environment}-idp-batch-processing\"\n  visibility_timeout_seconds = 900  # 15 minutes\n  message_retention_seconds  = 1209600  # 14 days\n\n  # Cost optimization: longer polling\n  receive_wait_time_seconds = 20\n\n  tags = local.common_tags\n}\n\n# Lambda for batch processing (larger memory, longer timeout)\nresource \"aws_lambda_function\" \"batch_processor\" {\n  function_name = \"${var.environment}-idp-batch-processor\"\n  memory_size   = 3008  # Maximum memory for better price/performance\n  timeout       = 900   # 15 minutes\n\n  # Process multiple documents per invocation\n  environment {\n    variables = {\n      BATCH_SIZE = \"10\"\n      ENABLE_PARALLEL_PROCESSING = \"true\"\n    }\n  }\n\n  tags = merge(local.common_tags, {\n    ProcessingType = \"batch\"\n  })\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#scheduled-processing","title":"Scheduled Processing","text":"<pre><code># EventBridge rule for off-peak processing\nresource \"aws_cloudwatch_event_rule\" \"batch_processing_schedule\" {\n  name                = \"${var.environment}-idp-batch-processing-schedule\"\n  description         = \"Trigger batch processing during off-peak hours\"\n  schedule_expression = \"cron(0 2 * * ? *)\"  # 2 AM daily\n\n  tags = local.common_tags\n}\n\nresource \"aws_cloudwatch_event_target\" \"batch_processing_target\" {\n  rule      = aws_cloudwatch_event_rule.batch_processing_schedule.name\n  target_id = \"BatchProcessingTarget\"\n  arn       = aws_lambda_function.batch_processor.arn\n\n  input = jsonencode({\n    processing_mode = \"batch\"\n    max_documents   = 100\n    priority        = \"low\"\n  })\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#development-environment-cost-controls","title":"Development Environment Cost Controls","text":""},{"location":"deployment-guides/cost-optimization.html#resource-limits-for-development","title":"Resource Limits for Development","text":"<pre><code># Development environment with strict limits\nlocals {\n  dev_limits = {\n    lambda_memory_size = 512\n    lambda_timeout     = 60\n    lambda_concurrency = 5\n    s3_lifecycle_days  = 7\n    dynamodb_billing   = \"PAY_PER_REQUEST\"\n    enable_monitoring  = false\n    log_retention_days = 3\n  }\n}\n\n# Conditional resource creation\nresource \"aws_lambda_function\" \"dev_document_processor\" {\n  count = var.environment == \"dev\" ? 1 : 0\n\n  function_name = \"${var.environment}-idp-document-processor\"\n  memory_size   = local.dev_limits.lambda_memory_size\n  timeout       = local.dev_limits.lambda_timeout\n\n  reserved_concurrent_executions = local.dev_limits.lambda_concurrency\n\n  tags = merge(local.common_tags, {\n    Environment = \"dev\"\n    AutoShutdown = \"enabled\"\n  })\n}\n\n# Auto-shutdown for development resources\nresource \"aws_cloudwatch_event_rule\" \"dev_auto_shutdown\" {\n  count               = var.environment == \"dev\" ? 1 : 0\n  name                = \"${var.environment}-idp-auto-shutdown\"\n  description         = \"Auto-shutdown development resources\"\n  schedule_expression = \"cron(0 18 * * MON-FRI *)\"  # 6 PM weekdays\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#cost-reporting-and-analysis","title":"Cost Reporting and Analysis","text":""},{"location":"deployment-guides/cost-optimization.html#custom-cost-dashboard","title":"Custom Cost Dashboard","text":"<pre><code>resource \"aws_cloudwatch_dashboard\" \"cost_dashboard\" {\n  dashboard_name = \"${var.environment}-idp-cost-dashboard\"\n\n  dashboard_body = jsonencode({\n    widgets = [\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 0\n        width  = 12\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/Billing\", \"EstimatedCharges\", \"ServiceName\", \"AmazonBedrock\", \"Currency\", \"USD\"],\n            [\".\", \".\", \"ServiceName\", \"AWSLambda\", \".\", \".\"],\n            [\".\", \".\", \"ServiceName\", \"AmazonS3\", \".\", \".\"],\n            [\".\", \".\", \"ServiceName\", \"AmazonTextract\", \".\", \".\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = \"us-east-1\"  # Billing metrics are only in us-east-1\n          title   = \"Estimated Charges by Service\"\n          period  = 86400\n        }\n      }\n    ]\n  })\n}\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#cost-optimization-recommendations","title":"Cost Optimization Recommendations","text":"<pre><code># Lambda function for cost optimization recommendations\nimport boto3\nimport json\nfrom datetime import datetime, timedelta\n\ndef generate_cost_recommendations(event, context):\n    \"\"\"Generate cost optimization recommendations\"\"\"\n\n    ce_client = boto3.client('ce')\n\n    # Get cost data for last 30 days\n    end_date = datetime.now().strftime('%Y-%m-%d')\n    start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n\n    response = ce_client.get_cost_and_usage(\n        TimePeriod={\n            'Start': start_date,\n            'End': end_date\n        },\n        Granularity='DAILY',\n        Metrics=['BlendedCost'],\n        GroupBy=[\n            {\n                'Type': 'DIMENSION',\n                'Key': 'SERVICE'\n            }\n        ],\n        Filter={\n            'Tags': {\n                'Key': 'Project',\n                'Values': ['genai-idp-accelerator']\n            }\n        }\n    )\n\n    recommendations = []\n\n    # Analyze Bedrock costs\n    bedrock_cost = get_service_cost(response, 'Amazon Bedrock')\n    if bedrock_cost &gt; 100:  # $100 threshold\n        recommendations.append({\n            'service': 'Amazon Bedrock',\n            'recommendation': 'Consider using smaller models for simple tasks',\n            'potential_savings': '20-40%'\n        })\n\n    # Analyze Lambda costs\n    lambda_cost = get_service_cost(response, 'AWS Lambda')\n    if lambda_cost &gt; 50:  # $50 threshold\n        recommendations.append({\n            'service': 'AWS Lambda',\n            'recommendation': 'Review memory allocation and consider ARM-based functions',\n            'potential_savings': '10-20%'\n        })\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'recommendations': recommendations,\n            'generated_at': datetime.now().isoformat()\n        })\n    }\n\ndef get_service_cost(response, service_name):\n    \"\"\"Extract cost for specific service\"\"\"\n    total_cost = 0\n    for result in response['ResultsByTime']:\n        for group in result['Groups']:\n            if group['Keys'][0] == service_name:\n                total_cost += float(group['Metrics']['BlendedCost']['Amount'])\n    return total_cost\n</code></pre>"},{"location":"deployment-guides/cost-optimization.html#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"deployment-guides/cost-optimization.html#cost-optimization-checklist","title":"Cost Optimization Checklist","text":"<p>Bedrock Optimization: - [ ] Use appropriate model sizes for each use case - [ ] Implement response caching - [ ] Optimize prompts for token efficiency - [ ] Monitor token usage patterns</p> <p>Lambda Optimization: - [ ] Right-size memory allocation - [ ] Use ARM-based processors when possible - [ ] Implement proper error handling to avoid retries - [ ] Use provisioned concurrency only when needed</p> <p>Storage Optimization: - [ ] Implement S3 lifecycle policies - [ ] Use intelligent tiering - [ ] Clean up old processed documents - [ ] Optimize DynamoDB capacity settings</p> <p>Monitoring and Alerts: - [ ] Set up cost budgets and alerts - [ ] Monitor cost anomalies - [ ] Regular cost reviews and optimization - [ ] Track cost per document processed</p> <p>Environment Management: - [ ] Use smaller resources for development - [ ] Implement auto-shutdown for dev environments - [ ] Regular cleanup of unused resources - [ ] Tag all resources for cost tracking</p> <p>Next: Best Practices | Troubleshooting</p>"},{"location":"deployment-guides/environment-setup.html","title":"Environment Setup Guide","text":"<p>This guide walks you through setting up different environments for the GenAI IDP Accelerator using Terraform.</p>"},{"location":"deployment-guides/environment-setup.html#environment-types","title":"Environment Types","text":""},{"location":"deployment-guides/environment-setup.html#development-environment","title":"Development Environment","text":"<p>Purpose: Testing and development work Characteristics: - Lower resource limits - Reduced redundancy - Cost-optimized settings - Simplified monitoring</p> <p>Configuration: <pre><code># terraform/environments/dev/terraform.tfvars\nenvironment = \"dev\"\nregion = \"us-east-1\"\n\n# Cost optimization\nlambda_memory_size = 512\nlambda_timeout = 60\nenable_xray_tracing = false\n\n# Simplified storage\ns3_versioning_enabled = false\ns3_lifecycle_enabled = true\n\n# Basic monitoring\nenable_detailed_monitoring = false\nlog_retention_days = 7\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#staging-environment","title":"Staging Environment","text":"<p>Purpose: Pre-production testing and validation Characteristics: - Production-like configuration - Full feature testing - Performance validation - Security testing</p> <p>Configuration: <pre><code># terraform/environments/staging/terraform.tfvars\nenvironment = \"staging\"\nregion = \"us-east-1\"\n\n# Production-like settings\nlambda_memory_size = 1024\nlambda_timeout = 300\nenable_xray_tracing = true\n\n# Enhanced storage\ns3_versioning_enabled = true\ns3_lifecycle_enabled = true\n\n# Full monitoring\nenable_detailed_monitoring = true\nlog_retention_days = 30\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#production-environment","title":"Production Environment","text":"<p>Purpose: Live production workloads Characteristics: - High availability - Full redundancy - Comprehensive monitoring - Security hardening</p> <p>Configuration: <pre><code># terraform/environments/prod/terraform.tfvars\nenvironment = \"prod\"\nregion = \"us-east-1\"\n\n# Production settings\nlambda_memory_size = 2048\nlambda_timeout = 900\nenable_xray_tracing = true\n\n# Production storage\ns3_versioning_enabled = true\ns3_lifecycle_enabled = true\ns3_mfa_delete = true\n\n# Comprehensive monitoring\nenable_detailed_monitoring = true\nlog_retention_days = 90\nenable_cloudtrail = true\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#multi-region-setup","title":"Multi-Region Setup","text":""},{"location":"deployment-guides/environment-setup.html#primary-region-configuration","title":"Primary Region Configuration","text":"<pre><code># terraform/environments/prod/main.tf\nmodule \"idp_primary\" {\n  source = \"../../modules/idp-accelerator\"\n\n  region = \"us-east-1\"\n  environment = \"prod\"\n\n  # Primary region settings\n  is_primary_region = true\n  enable_cross_region_replication = true\n\n  # Disaster recovery\n  backup_region = \"us-west-2\"\n  enable_cross_region_backup = true\n}\n</code></pre>"},{"location":"deployment-guides/environment-setup.html#secondary-region-configuration","title":"Secondary Region Configuration","text":"<pre><code># terraform/environments/prod-dr/main.tf\nmodule \"idp_secondary\" {\n  source = \"../../modules/idp-accelerator\"\n\n  region = \"us-west-2\"\n  environment = \"prod-dr\"\n\n  # Secondary region settings\n  is_primary_region = false\n  primary_region = \"us-east-1\"\n\n  # Reduced capacity for DR\n  lambda_reserved_concurrency = 50\n  enable_auto_scaling = true\n}\n</code></pre>"},{"location":"deployment-guides/environment-setup.html#network-configuration","title":"Network Configuration","text":""},{"location":"deployment-guides/environment-setup.html#vpc-setup","title":"VPC Setup","text":"<p>New VPC (Recommended): <pre><code># Network configuration\ncreate_vpc = true\nvpc_cidr = \"10.0.0.0/16\"\n\n# Subnet configuration\nprivate_subnet_cidrs = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\npublic_subnet_cidrs = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n\n# NAT Gateway for private subnets\nenable_nat_gateway = true\nsingle_nat_gateway = false  # One per AZ for HA\n</code></pre></p> <p>Existing VPC: <pre><code># Use existing VPC\ncreate_vpc = false\nvpc_id = \"vpc-12345678\"\n\n# Existing subnets\nprivate_subnet_ids = [\"subnet-12345678\", \"subnet-87654321\"]\npublic_subnet_ids = [\"subnet-abcdef12\", \"subnet-21fedcba\"]\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#security-groups","title":"Security Groups","text":"<p>Default Security Groups: <pre><code># Lambda security group\nlambda_security_group_rules = [\n  {\n    type        = \"egress\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n    description = \"HTTPS outbound for AWS APIs\"\n  }\n]\n\n# API Gateway security group (if using VPC endpoints)\napi_gateway_security_group_rules = [\n  {\n    type        = \"ingress\"\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"10.0.0.0/16\"]\n    description = \"HTTPS from VPC\"\n  }\n]\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#storage-configuration","title":"Storage Configuration","text":""},{"location":"deployment-guides/environment-setup.html#s3-bucket-setup","title":"S3 Bucket Setup","text":"<p>Development: <pre><code>s3_bucket_configuration = {\n  versioning = {\n    enabled = false\n  }\n\n  lifecycle_configuration = {\n    rules = [\n      {\n        id     = \"delete_old_objects\"\n        status = \"Enabled\"\n\n        expiration = {\n          days = 30\n        }\n      }\n    ]\n  }\n\n  server_side_encryption_configuration = {\n    rule = {\n      apply_server_side_encryption_by_default = {\n        sse_algorithm = \"AES256\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Production: <pre><code>s3_bucket_configuration = {\n  versioning = {\n    enabled = true\n    mfa_delete = true\n  }\n\n  lifecycle_configuration = {\n    rules = [\n      {\n        id     = \"intelligent_tiering\"\n        status = \"Enabled\"\n\n        transition = [\n          {\n            days          = 30\n            storage_class = \"STANDARD_IA\"\n          },\n          {\n            days          = 90\n            storage_class = \"GLACIER\"\n          }\n        ]\n      }\n    ]\n  }\n\n  server_side_encryption_configuration = {\n    rule = {\n      apply_server_side_encryption_by_default = {\n        sse_algorithm     = \"aws:kms\"\n        kms_master_key_id = aws_kms_key.s3_key.arn\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#dynamodb-configuration","title":"DynamoDB Configuration","text":"<p>Development: <pre><code>dynamodb_configuration = {\n  billing_mode = \"PAY_PER_REQUEST\"\n\n  point_in_time_recovery = {\n    enabled = false\n  }\n\n  server_side_encryption = {\n    enabled = true\n  }\n}\n</code></pre></p> <p>Production: <pre><code>dynamodb_configuration = {\n  billing_mode   = \"PROVISIONED\"\n  read_capacity  = 100\n  write_capacity = 100\n\n  point_in_time_recovery = {\n    enabled = true\n  }\n\n  server_side_encryption = {\n    enabled     = true\n    kms_key_arn = aws_kms_key.dynamodb_key.arn\n  }\n\n  global_secondary_indexes = [\n    {\n      name            = \"status-index\"\n      hash_key        = \"status\"\n      projection_type = \"ALL\"\n      read_capacity   = 50\n      write_capacity  = 50\n    }\n  ]\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#security-configuration","title":"Security Configuration","text":""},{"location":"deployment-guides/environment-setup.html#iam-roles-and-policies","title":"IAM Roles and Policies","text":"<p>Least Privilege Principle: <pre><code># Lambda execution role\nlambda_execution_role_policies = [\n  \"arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole\",\n  aws_iam_policy.lambda_custom_policy.arn\n]\n\n# Custom policy for specific permissions\nresource \"aws_iam_policy\" \"lambda_custom_policy\" {\n  name = \"${var.environment}-lambda-custom-policy\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"textract:DetectDocumentText\",\n          \"textract:AnalyzeDocument\",\n          \"bedrock:InvokeModel\"\n        ]\n        Resource = \"*\"\n      },\n      {\n        Effect = \"Allow\"\n        Action = [\n          \"s3:GetObject\",\n          \"s3:PutObject\"\n        ]\n        Resource = \"${aws_s3_bucket.documents.arn}/*\"\n      }\n    ]\n  })\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#kms-key-management","title":"KMS Key Management","text":"<p>Environment-Specific Keys: <pre><code># KMS key for S3 encryption\nresource \"aws_kms_key\" \"s3_key\" {\n  description = \"${var.environment} S3 encryption key\"\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [\n      {\n        Sid    = \"Enable IAM User Permissions\"\n        Effect = \"Allow\"\n        Principal = {\n          AWS = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:root\"\n        }\n        Action   = \"kms:*\"\n        Resource = \"*\"\n      },\n      {\n        Sid    = \"Allow Lambda Service\"\n        Effect = \"Allow\"\n        Principal = {\n          Service = \"lambda.amazonaws.com\"\n        }\n        Action = [\n          \"kms:Decrypt\",\n          \"kms:GenerateDataKey\"\n        ]\n        Resource = \"*\"\n      }\n    ]\n  })\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"deployment-guides/environment-setup.html#cloudwatch-configuration","title":"CloudWatch Configuration","text":"<p>Log Groups: <pre><code># Lambda log groups\nresource \"aws_cloudwatch_log_group\" \"lambda_logs\" {\n  for_each = var.lambda_functions\n\n  name              = \"/aws/lambda/${var.environment}-${each.key}\"\n  retention_in_days = var.log_retention_days\n\n  kms_key_id = aws_kms_key.cloudwatch_key.arn\n}\n</code></pre></p> <p>Alarms: <pre><code># Error rate alarm\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_error_rate\" {\n  alarm_name          = \"${var.environment}-lambda-error-rate\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"10\"\n  alarm_description   = \"This metric monitors lambda error rate\"\n\n  dimensions = {\n    FunctionName = aws_lambda_function.processor.function_name\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#x-ray-tracing","title":"X-Ray Tracing","text":"<p>Configuration: <pre><code># Enable X-Ray tracing\nlambda_tracing_config = {\n  mode = var.enable_xray_tracing ? \"Active\" : \"PassThrough\"\n}\n\n# X-Ray service map\nresource \"aws_xray_sampling_rule\" \"idp_sampling\" {\n  rule_name      = \"${var.environment}-idp-sampling\"\n  priority       = 9000\n  version        = 1\n  reservoir_size = 1\n  fixed_rate     = 0.1\n  url_path       = \"*\"\n  host           = \"*\"\n  http_method    = \"*\"\n  service_type   = \"*\"\n  service_name   = \"*\"\n  resource_arn   = \"*\"\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#environment-promotion","title":"Environment Promotion","text":""},{"location":"deployment-guides/environment-setup.html#development-to-staging","title":"Development to Staging","text":"<p>Validation Checklist: - [ ] All tests pass - [ ] Security scan completed - [ ] Performance benchmarks met - [ ] Documentation updated</p> <p>Promotion Process: <pre><code># 1. Tag the release\ngit tag -a v1.2.3 -m \"Release v1.2.3\"\ngit push origin v1.2.3\n\n# 2. Deploy to staging\ncd terraform/environments/staging\nterraform plan -var-file=\"terraform.tfvars\"\nterraform apply\n\n# 3. Run integration tests\n./scripts/run-integration-tests.sh staging\n\n# 4. Validate deployment\n./scripts/validate-deployment.sh staging\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#staging-to-production","title":"Staging to Production","text":"<p>Additional Validation: - [ ] Load testing completed - [ ] Security review approved - [ ] Disaster recovery tested - [ ] Rollback plan prepared</p> <p>Production Deployment: <pre><code># 1. Create deployment branch\ngit checkout -b release/v1.2.3\n\n# 2. Deploy with approval\ncd terraform/environments/prod\nterraform plan -var-file=\"terraform.tfvars\" -out=tfplan\n# Review plan carefully\nterraform apply tfplan\n\n# 3. Monitor deployment\n./scripts/monitor-deployment.sh prod\n\n# 4. Smoke tests\n./scripts/smoke-tests.sh prod\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"deployment-guides/environment-setup.html#state-file-backup","title":"State File Backup","text":"<p>S3 Backend Configuration: <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"terraform-state-${var.environment}\"\n    key            = \"idp-accelerator/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"terraform-locks-${var.environment}\"\n\n    # Versioning enabled on bucket\n    versioning = true\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#data-backup","title":"Data Backup","text":"<p>Automated Backups: <pre><code># DynamoDB backup\nresource \"aws_dynamodb_table\" \"documents\" {\n  point_in_time_recovery {\n    enabled = true\n  }\n}\n\n# S3 cross-region replication\nresource \"aws_s3_bucket_replication_configuration\" \"documents\" {\n  count = var.enable_cross_region_replication ? 1 : 0\n\n  role   = aws_iam_role.replication.arn\n  bucket = aws_s3_bucket.documents.id\n\n  rule {\n    id     = \"replicate-all\"\n    status = \"Enabled\"\n\n    destination {\n      bucket        = aws_s3_bucket.documents_replica.arn\n      storage_class = \"STANDARD_IA\"\n    }\n  }\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#cost-management","title":"Cost Management","text":""},{"location":"deployment-guides/environment-setup.html#resource-tagging","title":"Resource Tagging","text":"<p>Consistent Tagging Strategy: <pre><code>locals {\n  common_tags = {\n    Environment = var.environment\n    Project     = \"genai-idp-accelerator\"\n    Owner       = var.owner\n    CostCenter  = var.cost_center\n    Terraform   = \"true\"\n  }\n}\n\n# Apply to all resources\nresource \"aws_s3_bucket\" \"documents\" {\n  bucket = \"${var.environment}-idp-documents\"\n\n  tags = local.common_tags\n}\n</code></pre></p>"},{"location":"deployment-guides/environment-setup.html#cost-monitoring","title":"Cost Monitoring","text":"<p>Budget Alerts: <pre><code>resource \"aws_budgets_budget\" \"environment_budget\" {\n  name         = \"${var.environment}-idp-budget\"\n  budget_type  = \"COST\"\n  limit_amount = var.monthly_budget_limit\n  limit_unit   = \"USD\"\n  time_unit    = \"MONTHLY\"\n\n  cost_filters = {\n    Tag = [\"Environment:${var.environment}\"]\n  }\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                 = 80\n    threshold_type            = \"PERCENTAGE\"\n    notification_type         = \"ACTUAL\"\n    subscriber_email_addresses = [var.budget_alert_email]\n  }\n}\n</code></pre></p> <p>Next: Monitoring Setup | Cost Optimization</p>"},{"location":"deployment-guides/monitoring.html","title":"Monitoring and Observability Guide","text":"<p>This guide covers comprehensive monitoring, logging, and observability setup for the GenAI IDP Accelerator.</p>"},{"location":"deployment-guides/monitoring.html#monitoring-architecture","title":"Monitoring Architecture","text":""},{"location":"deployment-guides/monitoring.html#three-pillars-of-observability","title":"Three Pillars of Observability","text":"<ol> <li>Metrics: Quantitative measurements of system behavior</li> <li>Logs: Detailed records of events and transactions</li> <li>Traces: Request flow through distributed components</li> </ol>"},{"location":"deployment-guides/monitoring.html#monitoring-stack","title":"Monitoring Stack","text":"<pre><code>graph TB\n    App[Application] --&gt; CW[CloudWatch]\n    App --&gt; XRay[X-Ray]\n    CW --&gt; Alarms[CloudWatch Alarms]\n    CW --&gt; Dashboards[CloudWatch Dashboards]\n    Alarms --&gt; SNS[SNS Topics]\n    SNS --&gt; Email[Email Notifications]\n    SNS --&gt; Slack[Slack Integration]\n    XRay --&gt; ServiceMap[Service Map]\n    XRay --&gt; Traces[Distributed Traces]</code></pre>"},{"location":"deployment-guides/monitoring.html#cloudwatch-configuration","title":"CloudWatch Configuration","text":""},{"location":"deployment-guides/monitoring.html#log-groups","title":"Log Groups","text":"<pre><code># Lambda function log groups\nresource \"aws_cloudwatch_log_group\" \"lambda_logs\" {\n  for_each = var.lambda_functions\n\n  name              = \"/aws/lambda/${var.environment}-idp-${each.key}\"\n  retention_in_days = var.log_retention_days\n  kms_key_id        = aws_kms_key.cloudwatch_logs_key.arn\n\n  tags = local.common_tags\n}\n\n# API Gateway log group\nresource \"aws_cloudwatch_log_group\" \"api_gateway_logs\" {\n  name              = \"/aws/apigateway/${var.environment}-idp-api\"\n  retention_in_days = var.log_retention_days\n  kms_key_id        = aws_kms_key.cloudwatch_logs_key.arn\n\n  tags = local.common_tags\n}\n\n# Step Functions log group\nresource \"aws_cloudwatch_log_group\" \"step_functions_logs\" {\n  name              = \"/aws/stepfunctions/${var.environment}-idp-workflow\"\n  retention_in_days = var.log_retention_days\n  kms_key_id        = aws_kms_key.cloudwatch_logs_key.arn\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#custom-metrics","title":"Custom Metrics","text":"<pre><code># Custom metric filters\nresource \"aws_cloudwatch_log_metric_filter\" \"document_processing_duration\" {\n  name           = \"${var.environment}-document-processing-duration\"\n  log_group_name = aws_cloudwatch_log_group.lambda_logs[\"document_processor\"].name\n  pattern        = \"[timestamp, request_id, \\\"PROCESSING_DURATION\\\", duration]\"\n\n  metric_transformation {\n    name      = \"DocumentProcessingDuration\"\n    namespace = \"IDP/Performance\"\n    value     = \"$duration\"\n    unit      = \"Seconds\"\n  }\n}\n\nresource \"aws_cloudwatch_log_metric_filter\" \"document_processing_errors\" {\n  name           = \"${var.environment}-document-processing-errors\"\n  log_group_name = aws_cloudwatch_log_group.lambda_logs[\"document_processor\"].name\n  pattern        = \"[timestamp, request_id, \\\"ERROR\\\", ...]\"\n\n  metric_transformation {\n    name      = \"DocumentProcessingErrors\"\n    namespace = \"IDP/Errors\"\n    value     = \"1\"\n  }\n}\n\nresource \"aws_cloudwatch_log_metric_filter\" \"bedrock_api_calls\" {\n  name           = \"${var.environment}-bedrock-api-calls\"\n  log_group_name = aws_cloudwatch_log_group.lambda_logs[\"ai_processor\"].name\n  pattern        = \"[timestamp, request_id, \\\"BEDROCK_API_CALL\\\", model, tokens]\"\n\n  metric_transformation {\n    name      = \"BedrockAPICalls\"\n    namespace = \"IDP/Usage\"\n    value     = \"1\"\n  }\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#cloudwatch-alarms","title":"CloudWatch Alarms","text":""},{"location":"deployment-guides/monitoring.html#performance-alarms","title":"Performance Alarms","text":"<pre><code># Lambda function duration alarm\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_duration\" {\n  for_each = var.lambda_functions\n\n  alarm_name          = \"${var.environment}-idp-${each.key}-duration\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"Duration\"\n  namespace           = \"AWS/Lambda\"\n  period              = \"300\"\n  statistic           = \"Average\"\n  threshold           = each.value.timeout_threshold\n  alarm_description   = \"Lambda function ${each.key} duration is too high\"\n\n  dimensions = {\n    FunctionName = aws_lambda_function.functions[each.key].function_name\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n  ok_actions    = [aws_sns_topic.alerts.arn]\n\n  tags = local.common_tags\n}\n\n# Lambda function error rate alarm\nresource \"aws_cloudwatch_metric_alarm\" \"lambda_error_rate\" {\n  for_each = var.lambda_functions\n\n  alarm_name          = \"${var.environment}-idp-${each.key}-error-rate\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"Errors\"\n  namespace           = \"AWS/Lambda\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"5\"\n  alarm_description   = \"Lambda function ${each.key} error rate is too high\"\n\n  dimensions = {\n    FunctionName = aws_lambda_function.functions[each.key].function_name\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n\n  tags = local.common_tags\n}\n\n# API Gateway 4xx errors\nresource \"aws_cloudwatch_metric_alarm\" \"api_gateway_4xx_errors\" {\n  alarm_name          = \"${var.environment}-idp-api-4xx-errors\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"4XXError\"\n  namespace           = \"AWS/ApiGateway\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"10\"\n  alarm_description   = \"API Gateway 4xx error rate is too high\"\n\n  dimensions = {\n    ApiName = aws_api_gateway_rest_api.idp_api.name\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n\n  tags = local.common_tags\n}\n\n# API Gateway 5xx errors\nresource \"aws_cloudwatch_metric_alarm\" \"api_gateway_5xx_errors\" {\n  alarm_name          = \"${var.environment}-idp-api-5xx-errors\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"1\"\n  metric_name         = \"5XXError\"\n  namespace           = \"AWS/ApiGateway\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"1\"\n  alarm_description   = \"API Gateway 5xx error detected\"\n\n  dimensions = {\n    ApiName = aws_api_gateway_rest_api.idp_api.name\n  }\n\n  alarm_actions = [aws_sns_topic.critical_alerts.arn]\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#resource-utilization-alarms","title":"Resource Utilization Alarms","text":"<pre><code># DynamoDB throttling alarm\nresource \"aws_cloudwatch_metric_alarm\" \"dynamodb_throttling\" {\n  alarm_name          = \"${var.environment}-idp-dynamodb-throttling\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"ThrottledRequests\"\n  namespace           = \"AWS/DynamoDB\"\n  period              = \"300\"\n  statistic           = \"Sum\"\n  threshold           = \"0\"\n  alarm_description   = \"DynamoDB requests are being throttled\"\n\n  dimensions = {\n    TableName = aws_dynamodb_table.document_metadata.name\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n\n  tags = local.common_tags\n}\n\n# S3 bucket size alarm\nresource \"aws_cloudwatch_metric_alarm\" \"s3_bucket_size\" {\n  alarm_name          = \"${var.environment}-idp-s3-bucket-size\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"1\"\n  metric_name         = \"BucketSizeBytes\"\n  namespace           = \"AWS/S3\"\n  period              = \"86400\"  # Daily\n  statistic           = \"Average\"\n  threshold           = var.s3_size_threshold_bytes\n  alarm_description   = \"S3 bucket size is approaching limits\"\n\n  dimensions = {\n    BucketName = aws_s3_bucket.documents.bucket\n    StorageType = \"StandardStorage\"\n  }\n\n  alarm_actions = [aws_sns_topic.alerts.arn]\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#cloudwatch-dashboards","title":"CloudWatch Dashboards","text":""},{"location":"deployment-guides/monitoring.html#main-dashboard","title":"Main Dashboard","text":"<pre><code>resource \"aws_cloudwatch_dashboard\" \"idp_main_dashboard\" {\n  dashboard_name = \"${var.environment}-idp-main-dashboard\"\n\n  dashboard_body = jsonencode({\n    widgets = [\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 0\n        width  = 12\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/Lambda\", \"Invocations\", \"FunctionName\", aws_lambda_function.document_processor.function_name],\n            [\".\", \"Duration\", \".\", \".\"],\n            [\".\", \"Errors\", \".\", \".\"],\n            [\".\", \"Throttles\", \".\", \".\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"Document Processor Lambda Metrics\"\n          period  = 300\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 12\n        y      = 0\n        width  = 12\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/ApiGateway\", \"Count\", \"ApiName\", aws_api_gateway_rest_api.idp_api.name],\n            [\".\", \"Latency\", \".\", \".\"],\n            [\".\", \"4XXError\", \".\", \".\"],\n            [\".\", \"5XXError\", \".\", \".\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"API Gateway Metrics\"\n          period  = 300\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 6\n        width  = 8\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/DynamoDB\", \"ConsumedReadCapacityUnits\", \"TableName\", aws_dynamodb_table.document_metadata.name],\n            [\".\", \"ConsumedWriteCapacityUnits\", \".\", \".\"],\n            [\".\", \"ThrottledRequests\", \".\", \".\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"DynamoDB Metrics\"\n          period  = 300\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 8\n        y      = 6\n        width  = 8\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"IDP/Performance\", \"DocumentProcessingDuration\"],\n            [\"IDP/Errors\", \"DocumentProcessingErrors\"],\n            [\"IDP/Usage\", \"BedrockAPICalls\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"Custom Application Metrics\"\n          period  = 300\n        }\n      },\n      {\n        type   = \"log\"\n        x      = 16\n        y      = 6\n        width  = 8\n        height = 6\n\n        properties = {\n          query   = \"SOURCE '/aws/lambda/${var.environment}-idp-document-processor' | fields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20\"\n          region  = var.region\n          title   = \"Recent Errors\"\n          view    = \"table\"\n        }\n      }\n    ]\n  })\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#performance-dashboard","title":"Performance Dashboard","text":"<pre><code>resource \"aws_cloudwatch_dashboard\" \"idp_performance_dashboard\" {\n  dashboard_name = \"${var.environment}-idp-performance-dashboard\"\n\n  dashboard_body = jsonencode({\n    widgets = [\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 0\n        width  = 24\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"IDP/Performance\", \"DocumentProcessingDuration\", { \"stat\": \"Average\" }],\n            [\".\", \".\", { \"stat\": \"p95\" }],\n            [\".\", \".\", { \"stat\": \"p99\" }]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"Document Processing Duration (Average, P95, P99)\"\n          period  = 300\n          yAxis = {\n            left = {\n              min = 0\n            }\n          }\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 0\n        y      = 6\n        width  = 12\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/Lambda\", \"ConcurrentExecutions\", \"FunctionName\", aws_lambda_function.document_processor.function_name],\n            [\".\", \".\", \"FunctionName\", aws_lambda_function.ai_processor.function_name]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"Lambda Concurrent Executions\"\n          period  = 300\n        }\n      },\n      {\n        type   = \"metric\"\n        x      = 12\n        y      = 6\n        width  = 12\n        height = 6\n\n        properties = {\n          metrics = [\n            [\"AWS/S3\", \"NumberOfObjects\", \"BucketName\", aws_s3_bucket.documents.bucket, \"StorageType\", \"AllStorageTypes\"],\n            [\".\", \"BucketSizeBytes\", \".\", \".\", \".\", \"StandardStorage\"]\n          ]\n          view    = \"timeSeries\"\n          stacked = false\n          region  = var.region\n          title   = \"S3 Storage Metrics\"\n          period  = 86400\n        }\n      }\n    ]\n  })\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#x-ray-tracing","title":"X-Ray Tracing","text":""},{"location":"deployment-guides/monitoring.html#configuration","title":"Configuration","text":"<pre><code># X-Ray sampling rule\nresource \"aws_xray_sampling_rule\" \"idp_sampling_rule\" {\n  rule_name      = \"${var.environment}-idp-sampling-rule\"\n  priority       = 9000\n  version        = 1\n  reservoir_size = 1\n  fixed_rate     = var.xray_sampling_rate\n  url_path       = \"*\"\n  host           = \"*\"\n  http_method    = \"*\"\n  service_type   = \"*\"\n  service_name   = \"${var.environment}-idp-*\"\n  resource_arn   = \"*\"\n\n  tags = local.common_tags\n}\n\n# Lambda tracing configuration\nresource \"aws_lambda_function\" \"document_processor\" {\n  # ... other configuration ...\n\n  tracing_config {\n    mode = var.enable_xray_tracing ? \"Active\" : \"PassThrough\"\n  }\n}\n\n# API Gateway tracing\nresource \"aws_api_gateway_stage\" \"idp_api_stage\" {\n  deployment_id = aws_api_gateway_deployment.idp_api_deployment.id\n  rest_api_id   = aws_api_gateway_rest_api.idp_api.id\n  stage_name    = var.environment\n\n  xray_tracing_enabled = var.enable_xray_tracing\n\n  # ... other configuration ...\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#custom-tracing","title":"Custom Tracing","text":"<pre><code># Lambda function with custom X-Ray tracing\nimport json\nimport boto3\nfrom aws_xray_sdk.core import xray_recorder\nfrom aws_xray_sdk.core import patch_all\n\n# Patch AWS SDK calls\npatch_all()\n\n@xray_recorder.capture('document_processor')\ndef lambda_handler(event, context):\n\n    @xray_recorder.capture('validate_input')\n    def validate_input(event_data):\n        # Input validation logic\n        return validated_data\n\n    @xray_recorder.capture('process_with_textract')\n    def process_with_textract(document_key):\n        textract = boto3.client('textract')\n\n        # Add custom metadata to trace\n        xray_recorder.current_subsegment().put_metadata('document_key', document_key)\n\n        response = textract.detect_document_text(\n            Document={'S3Object': {'Bucket': bucket_name, 'Name': document_key}}\n        )\n\n        # Add result metadata\n        xray_recorder.current_subsegment().put_metadata('text_blocks_count', len(response['Blocks']))\n\n        return response\n\n    @xray_recorder.capture('process_with_bedrock')\n    def process_with_bedrock(extracted_text):\n        bedrock = boto3.client('bedrock-runtime')\n\n        # Add custom annotation for filtering\n        xray_recorder.current_subsegment().put_annotation('model_used', 'claude-3-sonnet')\n\n        response = bedrock.invoke_model(\n            modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n            body=json.dumps({\n                'anthropic_version': 'bedrock-2023-05-31',\n                'max_tokens': 1000,\n                'messages': [{'role': 'user', 'content': f'Analyze this text: {extracted_text}'}]\n            })\n        )\n\n        return response\n\n    try:\n        # Process the document\n        validated_input = validate_input(event)\n        textract_result = process_with_textract(validated_input['document_key'])\n        bedrock_result = process_with_bedrock(textract_result['text'])\n\n        # Add success annotation\n        xray_recorder.current_subsegment().put_annotation('processing_status', 'success')\n\n        return {\n            'statusCode': 200,\n            'body': json.dumps({'result': 'success'})\n        }\n\n    except Exception as e:\n        # Add error annotation\n        xray_recorder.current_subsegment().put_annotation('processing_status', 'error')\n        xray_recorder.current_subsegment().add_exception(e)\n\n        raise e\n</code></pre>"},{"location":"deployment-guides/monitoring.html#application-insights","title":"Application Insights","text":""},{"location":"deployment-guides/monitoring.html#cloudwatch-application-insights","title":"CloudWatch Application Insights","text":"<pre><code>resource \"aws_applicationinsights_application\" \"idp_application\" {\n  resource_group_name = aws_resourcegroups_group.idp_resources.name\n  auto_config_enabled = true\n  auto_create         = true\n\n  log_pattern {\n    pattern_name = \"LambdaErrors\"\n    pattern      = \"[timestamp, request_id, level=\\\"ERROR\\\", ...]\"\n    rank         = 1\n  }\n\n  log_pattern {\n    pattern_name = \"APIGatewayErrors\"\n    pattern      = \"[timestamp, request_id, ip, user, timestamp2, method, resource, protocol, status_code=5*, ...]\"\n    rank         = 1\n  }\n\n  tags = local.common_tags\n}\n\n# Resource group for application insights\nresource \"aws_resourcegroups_group\" \"idp_resources\" {\n  name = \"${var.environment}-idp-resources\"\n\n  resource_query {\n    query = jsonencode({\n      ResourceTypeFilters = [\"AWS::AllSupported\"]\n      TagFilters = [\n        {\n          Key    = \"Environment\"\n          Values = [var.environment]\n        },\n        {\n          Key    = \"Project\"\n          Values = [\"genai-idp-accelerator\"]\n        }\n      ]\n    })\n  }\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#notification-setup","title":"Notification Setup","text":""},{"location":"deployment-guides/monitoring.html#sns-topics","title":"SNS Topics","text":"<pre><code># General alerts topic\nresource \"aws_sns_topic\" \"alerts\" {\n  name = \"${var.environment}-idp-alerts\"\n\n  tags = local.common_tags\n}\n\n# Critical alerts topic\nresource \"aws_sns_topic\" \"critical_alerts\" {\n  name = \"${var.environment}-idp-critical-alerts\"\n\n  tags = local.common_tags\n}\n\n# Email subscriptions\nresource \"aws_sns_topic_subscription\" \"email_alerts\" {\n  count     = length(var.alert_email_addresses)\n  topic_arn = aws_sns_topic.alerts.arn\n  protocol  = \"email\"\n  endpoint  = var.alert_email_addresses[count.index]\n}\n\nresource \"aws_sns_topic_subscription\" \"critical_email_alerts\" {\n  count     = length(var.critical_alert_email_addresses)\n  topic_arn = aws_sns_topic.critical_alerts.arn\n  protocol  = \"email\"\n  endpoint  = var.critical_alert_email_addresses[count.index]\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#slack-integration","title":"Slack Integration","text":"<pre><code># Lambda function for Slack notifications\nresource \"aws_lambda_function\" \"slack_notifier\" {\n  count = var.slack_webhook_url != \"\" ? 1 : 0\n\n  filename         = \"slack_notifier.zip\"\n  function_name    = \"${var.environment}-idp-slack-notifier\"\n  role            = aws_iam_role.slack_notifier_role[0].arn\n  handler         = \"index.handler\"\n  runtime         = \"python3.9\"\n  timeout         = 30\n\n  environment {\n    variables = {\n      SLACK_WEBHOOK_URL = var.slack_webhook_url\n      ENVIRONMENT       = var.environment\n    }\n  }\n\n  tags = local.common_tags\n}\n\n# SNS subscription for Slack notifications\nresource \"aws_sns_topic_subscription\" \"slack_alerts\" {\n  count     = var.slack_webhook_url != \"\" ? 1 : 0\n  topic_arn = aws_sns_topic.alerts.arn\n  protocol  = \"lambda\"\n  endpoint  = aws_lambda_function.slack_notifier[0].arn\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#log-analysis","title":"Log Analysis","text":""},{"location":"deployment-guides/monitoring.html#cloudwatch-insights-queries","title":"CloudWatch Insights Queries","text":"<p>Common Queries:</p> <pre><code>-- Find all errors in the last hour\nfields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 100\n\n-- Analyze processing duration\nfields @timestamp, @message\n| filter @message like /PROCESSING_DURATION/\n| parse @message \"PROCESSING_DURATION: * seconds\" as duration\n| stats avg(duration), max(duration), min(duration) by bin(5m)\n\n-- Track API Gateway response times\nfields @timestamp, @message\n| filter @message like /\\d+ms/\n| parse @message /(?&lt;responseTime&gt;\\d+)ms/\n| stats avg(responseTime), max(responseTime), p95(responseTime) by bin(5m)\n\n-- Monitor Bedrock API usage\nfields @timestamp, @message\n| filter @message like /BEDROCK_API_CALL/\n| parse @message \"BEDROCK_API_CALL: model=* tokens=*\" as model, tokens\n| stats sum(tokens) by model, bin(1h)\n</code></pre>"},{"location":"deployment-guides/monitoring.html#automated-log-analysis","title":"Automated Log Analysis","text":"<pre><code># CloudWatch Insights scheduled query\nresource \"aws_cloudwatch_query_definition\" \"error_analysis\" {\n  name = \"${var.environment}-idp-error-analysis\"\n\n  log_group_names = [\n    aws_cloudwatch_log_group.lambda_logs[\"document_processor\"].name,\n    aws_cloudwatch_log_group.lambda_logs[\"ai_processor\"].name,\n    aws_cloudwatch_log_group.api_gateway_logs.name\n  ]\n\n  query_string = &lt;&lt;EOF\nfields @timestamp, @message, @logStream\n| filter @message like /ERROR/\n| stats count() by @logStream, bin(1h)\n| sort @timestamp desc\nEOF\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"deployment-guides/monitoring.html#cost-and-usage-tracking","title":"Cost and Usage Tracking","text":"<pre><code># Cost anomaly detection\nresource \"aws_ce_anomaly_detector\" \"idp_cost_anomaly\" {\n  name         = \"${var.environment}-idp-cost-anomaly\"\n  monitor_type = \"DIMENSIONAL\"\n\n  specification = jsonencode({\n    Dimension = \"SERVICE\"\n    MatchOptions = [\"EQUALS\"]\n    Values = [\"Amazon Bedrock\", \"AWS Lambda\", \"Amazon S3\", \"Amazon DynamoDB\"]\n  })\n\n  tags = local.common_tags\n}\n\nresource \"aws_ce_anomaly_subscription\" \"idp_cost_anomaly_subscription\" {\n  name      = \"${var.environment}-idp-cost-anomaly-subscription\"\n  frequency = \"DAILY\"\n\n  monitor_arn_list = [\n    aws_ce_anomaly_detector.idp_cost_anomaly.arn\n  ]\n\n  subscriber {\n    type    = \"EMAIL\"\n    address = var.cost_alert_email\n  }\n\n  threshold_expression {\n    and {\n      dimension {\n        key           = \"ANOMALY_TOTAL_IMPACT_ABSOLUTE\"\n        values        = [\"100\"]\n        match_options = [\"GREATER_THAN_OR_EQUAL\"]\n      }\n    }\n  }\n\n  tags = local.common_tags\n}\n</code></pre>"},{"location":"deployment-guides/monitoring.html#health-checks","title":"Health Checks","text":""},{"location":"deployment-guides/monitoring.html#application-health-monitoring","title":"Application Health Monitoring","text":"<pre><code># Route 53 health check for API endpoint\nresource \"aws_route53_health_check\" \"idp_api_health\" {\n  count                           = var.enable_health_checks ? 1 : 0\n  fqdn                           = var.api_domain_name\n  port                           = 443\n  type                           = \"HTTPS\"\n  resource_path                  = \"/health\"\n  failure_threshold              = \"3\"\n  request_interval               = \"30\"\n  cloudwatch_alarm_region        = var.region\n  cloudwatch_alarm_name          = aws_cloudwatch_metric_alarm.api_health_alarm[0].alarm_name\n  insufficient_data_health_status = \"Failure\"\n\n  tags = local.common_tags\n}\n\n# Health check alarm\nresource \"aws_cloudwatch_metric_alarm\" \"api_health_alarm\" {\n  count               = var.enable_health_checks ? 1 : 0\n  alarm_name          = \"${var.environment}-idp-api-health\"\n  comparison_operator = \"LessThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"HealthCheckStatus\"\n  namespace           = \"AWS/Route53\"\n  period              = \"60\"\n  statistic           = \"Minimum\"\n  threshold           = \"1\"\n  alarm_description   = \"API health check failed\"\n\n  dimensions = {\n    HealthCheckId = aws_route53_health_check.idp_api_health[0].id\n  }\n\n  alarm_actions = [aws_sns_topic.critical_alerts.arn]\n\n  tags = local.common_tags\n}\n</code></pre> <p>Next: Cost Optimization | Best Practices</p>"},{"location":"deployment-guides/troubleshooting.html","title":"Troubleshooting Guide","text":"<p>This guide helps you diagnose and resolve common issues when deploying the GenAI IDP Accelerator with Terraform.</p>"},{"location":"deployment-guides/troubleshooting.html#common-deployment-issues","title":"Common Deployment Issues","text":""},{"location":"deployment-guides/troubleshooting.html#permission-errors","title":"Permission Errors","text":"<p>Symptom: Access denied errors during Terraform deployment <pre><code>Error: AccessDenied: User is not authorized to perform action\n</code></pre></p> <p>Solutions: 1. Check IAM Permissions: Ensure your AWS credentials have the required permissions 2. Verify Service Roles: Check that service-linked roles exist for required services 3. Review Resource Policies: Ensure bucket policies and resource policies allow access</p> <pre><code># Check current AWS identity\naws sts get-caller-identity\n\n# Verify required permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::ACCOUNT:user/USERNAME \\\n  --action-names s3:CreateBucket,lambda:CreateFunction\n</code></pre>"},{"location":"deployment-guides/troubleshooting.html#resource-limits","title":"Resource Limits","text":"<p>Symptom: Service quotas exceeded <pre><code>Error: LimitExceededException: Account has reached the maximum number of functions\n</code></pre></p> <p>Solutions: 1. Check Service Quotas: Review current usage in AWS Console 2. Request Quota Increases: Submit requests through AWS Support 3. Clean Up Unused Resources: Remove old deployments</p> <pre><code># Check Lambda function count\naws lambda list-functions --query 'Functions[].FunctionName' --output table\n\n# Check S3 bucket count\naws s3api list-buckets --query 'Buckets[].Name' --output table\n</code></pre>"},{"location":"deployment-guides/troubleshooting.html#deployment-failures","title":"Deployment Failures","text":"<p>Symptom: Terraform apply fails with resource creation errors</p> <p>Common Causes: - Network Issues: VPC/subnet configuration problems - Dependency Issues: Resources created in wrong order - Configuration Errors: Invalid parameter values</p> <p>Debugging Steps: 1. Enable Detailed Logging: <pre><code>export TF_LOG=DEBUG\nterraform apply\n</code></pre></p> <ol> <li> <p>Check Resource Dependencies: <pre><code>terraform graph | dot -Tpng &gt; dependency-graph.png\n</code></pre></p> </li> <li> <p>Validate Configuration: <pre><code>terraform validate\nterraform plan -detailed-exitcode\n</code></pre></p> </li> </ol>"},{"location":"deployment-guides/troubleshooting.html#service-specific-issues","title":"Service-Specific Issues","text":""},{"location":"deployment-guides/troubleshooting.html#amazon-bedrock","title":"Amazon Bedrock","text":"<p>Issue: Model access denied <pre><code>Error: AccessDeniedException: Your account is not authorized to invoke this model\n</code></pre></p> <p>Solution: Request model access in Bedrock console 1. Go to Amazon Bedrock console 2. Navigate to Model access 3. Request access for required models (Claude, Titan, etc.)</p>"},{"location":"deployment-guides/troubleshooting.html#amazon-textract","title":"Amazon Textract","text":"<p>Issue: Document processing failures <pre><code>Error: InvalidParameterException: Document format not supported\n</code></pre></p> <p>Solutions: - Verify document format (PDF, PNG, JPEG, TIFF) - Check document size limits (10MB for synchronous, 500MB for asynchronous) - Ensure proper S3 permissions for document access</p>"},{"location":"deployment-guides/troubleshooting.html#lambda-functions","title":"Lambda Functions","text":"<p>Issue: Function timeout or memory errors <pre><code>Error: Task timed out after 15.00 seconds\n</code></pre></p> <p>Solutions: 1. Increase Timeout: <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  timeout = 300  # 5 minutes\n  memory_size = 1024  # 1GB\n}\n</code></pre></p> <ol> <li>Optimize Code: Review function logic for efficiency</li> <li>Use Step Functions: For long-running processes</li> </ol>"},{"location":"deployment-guides/troubleshooting.html#state-management-issues","title":"State Management Issues","text":""},{"location":"deployment-guides/troubleshooting.html#state-lock-conflicts","title":"State Lock Conflicts","text":"<p>Issue: Terraform state is locked <pre><code>Error: Error acquiring the state lock\n</code></pre></p> <p>Solutions: 1. Wait for Lock Release: Another operation may be in progress 2. Force Unlock (use carefully): <pre><code>terraform force-unlock LOCK_ID\n</code></pre></p> <ol> <li>Check DynamoDB Table: Verify state lock table exists and is accessible</li> </ol>"},{"location":"deployment-guides/troubleshooting.html#state-corruption","title":"State Corruption","text":"<p>Issue: State file corruption or inconsistency</p> <p>Solutions: 1. Import Existing Resources: <pre><code>terraform import aws_s3_bucket.example bucket-name\n</code></pre></p> <ol> <li> <p>Refresh State: <pre><code>terraform refresh\n</code></pre></p> </li> <li> <p>Restore from Backup: Use versioned S3 backend</p> </li> </ol>"},{"location":"deployment-guides/troubleshooting.html#network-and-security-issues","title":"Network and Security Issues","text":""},{"location":"deployment-guides/troubleshooting.html#vpc-configuration","title":"VPC Configuration","text":"<p>Issue: Resources cannot communicate</p> <p>Checklist: - [ ] Subnets in correct AZs - [ ] Route tables configured - [ ] Security groups allow required traffic - [ ] NACLs not blocking traffic - [ ] NAT Gateway for private subnets</p>"},{"location":"deployment-guides/troubleshooting.html#security-group-rules","title":"Security Group Rules","text":"<p>Issue: Connection timeouts</p> <p>Debug Steps: 1. Check Security Group Rules: <pre><code>aws ec2 describe-security-groups --group-ids sg-12345678\n</code></pre></p> <ol> <li> <p>Test Connectivity: <pre><code># From EC2 instance\ntelnet target-host 443\n</code></pre></p> </li> <li> <p>Review VPC Flow Logs: Check for rejected connections</p> </li> </ol>"},{"location":"deployment-guides/troubleshooting.html#performance-issues","title":"Performance Issues","text":""},{"location":"deployment-guides/troubleshooting.html#slow-processing","title":"Slow Processing","text":"<p>Symptoms: - Long document processing times - Lambda function timeouts - High costs</p> <p>Optimization Strategies: 1. Parallel Processing: Use Step Functions for concurrent execution 2. Batch Processing: Process multiple documents together 3. Caching: Store processed results to avoid reprocessing 4. Right-sizing: Adjust Lambda memory and timeout settings</p>"},{"location":"deployment-guides/troubleshooting.html#cost-optimization","title":"Cost Optimization","text":"<p>High Cost Indicators: - Excessive Lambda invocations - Large S3 storage costs - High Bedrock API usage</p> <p>Cost Reduction Tips: 1. Implement Caching: Avoid duplicate processing 2. Use Lifecycle Policies: Archive old documents 3. Monitor Usage: Set up billing alerts 4. Optimize Models: Use smaller models when appropriate</p>"},{"location":"deployment-guides/troubleshooting.html#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"deployment-guides/troubleshooting.html#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>Key Log Groups to Monitor: - <code>/aws/lambda/idp-processor-*</code> - <code>/aws/stepfunctions/idp-workflow</code> - <code>/aws/apigateway/idp-api</code></p> <p>Useful Log Queries: <pre><code>-- Find errors in last hour\nfields @timestamp, @message\n| filter @message like /ERROR/\n| sort @timestamp desc\n| limit 100\n</code></pre></p>"},{"location":"deployment-guides/troubleshooting.html#x-ray-tracing","title":"X-Ray Tracing","text":"<p>Enable Tracing: <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  tracing_config {\n    mode = \"Active\"\n  }\n}\n</code></pre></p> <p>Analyze Traces: Look for bottlenecks and errors in service map</p>"},{"location":"deployment-guides/troubleshooting.html#getting-help","title":"Getting Help","text":""},{"location":"deployment-guides/troubleshooting.html#aws-support","title":"AWS Support","text":"<p>For production issues: 1. Create Support Case: Include error messages and logs 2. Provide Context: Terraform configuration and deployment details 3. Include Diagnostics: CloudWatch logs and X-Ray traces</p>"},{"location":"deployment-guides/troubleshooting.html#community-resources","title":"Community Resources","text":"<ul> <li>AWS Forums: Search for similar issues</li> <li>GitHub Issues: Check project repository for known issues</li> <li>Documentation: Review AWS service documentation</li> </ul>"},{"location":"deployment-guides/troubleshooting.html#emergency-procedures","title":"Emergency Procedures","text":"<p>Critical Production Issues: 1. Rollback: Use previous Terraform state 2. Scale Down: Reduce resource usage 3. Enable Monitoring: Increase logging verbosity 4. Contact Support: Open high-priority support case</p>"},{"location":"deployment-guides/troubleshooting.html#prevention-best-practices","title":"Prevention Best Practices","text":""},{"location":"deployment-guides/troubleshooting.html#pre-deployment-checks","title":"Pre-Deployment Checks","text":"<ul> <li>[ ] Run <code>terraform plan</code> and review changes</li> <li>[ ] Test in development environment first</li> <li>[ ] Verify IAM permissions</li> <li>[ ] Check service quotas</li> <li>[ ] Review security configurations</li> </ul>"},{"location":"deployment-guides/troubleshooting.html#monitoring-setup","title":"Monitoring Setup","text":"<ul> <li>[ ] CloudWatch alarms for key metrics</li> <li>[ ] Log aggregation and analysis</li> <li>[ ] Cost monitoring and alerts</li> <li>[ ] Performance baseline establishment</li> </ul>"},{"location":"deployment-guides/troubleshooting.html#documentation","title":"Documentation","text":"<ul> <li>[ ] Document custom configurations</li> <li>[ ] Maintain runbooks for common issues</li> <li>[ ] Keep architecture diagrams updated</li> <li>[ ] Record lessons learned</li> </ul> <p>For additional help, see our FAQ section or contact support.</p>"},{"location":"examples/index.html","title":"Examples","text":"<p>This section provides practical examples of how to use the GenAI IDP Accelerator modules to build different types of document processing solutions. Each example references the real, working examples available in the repository.</p>"},{"location":"examples/index.html#available-examples","title":"Available Examples","text":""},{"location":"examples/index.html#bedrock-llm-processor","title":"Bedrock LLM Processor","text":"<p>Complexity: Intermediate Deployment Time: 15-20 minutes Based on: examples/bedrock-llm-processor/</p> <p>A flexible document processing pipeline using Amazon Bedrock foundation models for custom document analysis.</p> <p>What it includes: - Document upload to S3 - Text extraction with Textract - AI analysis with Bedrock LLMs - Custom prompt engineering - Results storage and tracking - Optional web UI</p> <p>Use cases: - Custom document processing - Flexible AI model selection - Advanced prompt engineering</p>"},{"location":"examples/index.html#bda-processor","title":"BDA Processor","text":"<p>Complexity: Intermediate Deployment Time: 15-20 minutes Based on: BDA Processor Example</p> <p>Uses Amazon Bedrock Data Automation for processing standard document types with managed schemas.</p> <p>What it includes: - Bedrock Data Automation integration - Pre-built document schemas - Managed processing pipeline - Web UI for document management</p> <p>Use cases: - Standard document types (invoices, forms, etc.) - Managed processing workflows - Production-ready deployments</p>"},{"location":"examples/index.html#sagemaker-udop-processor","title":"SageMaker UDOP Processor","text":"<p>Complexity: Advanced Deployment Time: 25-35 minutes Based on: SageMaker UDOP Processor</p> <p>Advanced document processing using a fine-tuned UDOP model on Amazon SageMaker for specialized document classification.</p> <p>What it includes: - SageMaker endpoint deployment - Fine-tuned UDOP model - Custom document classification - Advanced processing pipeline</p> <p>Use cases: - Specialized document types - Custom model requirements - High-accuracy classification needs</p>"},{"location":"examples/index.html#processing-environment","title":"Processing Environment","text":"<p>Complexity: Intermediate Deployment Time: 15-20 minutes Based on: Processing Environment</p> <p>Core infrastructure setup without specific processors - perfect for building custom solutions.</p> <p>What it includes: - Core processing infrastructure - DynamoDB tables for tracking - GraphQL API for management - Foundation for custom processors</p> <p>Use cases: - Custom processor development - Infrastructure foundation - Modular deployments</p>"},{"location":"examples/index.html#processing-environment-api","title":"Processing Environment API","text":"<p>Complexity: Intermediate Deployment Time: 10-15 minutes Based on: Processing Environment API</p> <p>Standalone API deployment for document processing management and monitoring.</p> <p>What it includes: - GraphQL API endpoints - Document status tracking - User authentication integration - API documentation</p> <p>Use cases: - API-first deployments - Integration with existing systems - Microservices architecture</p>"},{"location":"examples/index.html#user-identity-standalone","title":"User Identity Standalone","text":"<p>Complexity: Beginner Deployment Time: 5-10 minutes Based on: User Identity Standalone</p> <p>Standalone user authentication and authorization setup using Amazon Cognito.</p> <p>What it includes: - Cognito User Pool - User Pool Client - Admin user setup - Domain configuration</p> <p>Use cases: - Authentication foundation - User management setup - Security infrastructure</p>"},{"location":"examples/index.html#core-tables","title":"Core Tables","text":"<p>Complexity: Beginner Deployment Time: 5-10 minutes Based on: Core Tables</p> <p>Essential DynamoDB tables for document tracking and configuration management.</p> <p>What it includes: - Document tracking table - Configuration table - Concurrency management table - Basic monitoring setup</p> <p>Use cases: - Infrastructure foundation - Custom implementations - Modular deployments</p>"},{"location":"examples/index.html#repository-examples","title":"Repository Examples","text":"<p>All examples are available in the repository at: <pre><code>genaiic-idp-accelerator-terraform/examples/\n</code></pre></p> <p>Each example includes: - Complete Terraform configurations - Multiple configuration options (minimal, comprehensive, template) - Comprehensive README documentation - Cost estimates and optimization tips - Testing and validation instructions</p>"},{"location":"examples/index.html#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"examples/index.html#1-choose-an-example","title":"1. Choose an Example","text":"<p>Select an example based on your needs and complexity requirements.</p>"},{"location":"examples/index.html#2-navigate-to-example-directory","title":"2. Navigate to Example Directory","text":"<pre><code>cd examples/[example-name]\n</code></pre>"},{"location":"examples/index.html#3-configure-variables","title":"3. Configure Variables","text":"<p>Most examples provide multiple configuration options:</p> <pre><code># Use minimal configuration (ready to deploy)\n# terraform.tfvars is already provided\n\n# Or use comprehensive configuration\ncp terraform.tfvars.comprehensive terraform.tfvars\n\n# Or start from template\ncp terraform.tfvars.example terraform.tfvars\n# Edit terraform.tfvars with your specific values\n</code></pre>"},{"location":"examples/index.html#4-deploy","title":"4. Deploy","text":"<pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre>"},{"location":"examples/index.html#5-test","title":"5. Test","text":"<p>Each example includes specific testing instructions in its README file.</p>"},{"location":"examples/index.html#example-comparison","title":"Example Comparison","text":"Example Complexity Main Use Case Key Features Bedrock LLM Processor Intermediate Custom document processing Flexible AI models, custom prompts BDA Processor Intermediate Standard document types Managed schemas, production-ready SageMaker UDOP Advanced Specialized classification Custom models, high accuracy Processing Environment Intermediate Infrastructure foundation Core services, modular Processing Environment API Intermediate API-first approach GraphQL API, integration User Identity Beginner Authentication setup Cognito, user management Core Tables Beginner Data foundation DynamoDB, tracking"},{"location":"examples/index.html#prerequisites","title":"Prerequisites","text":"<p>Before running any example, ensure you have:</p> <ul> <li>Prerequisites completed</li> <li>Installation finished</li> <li>AWS credentials configured</li> <li>Terraform &gt;= 1.5.0 installed</li> </ul>"},{"location":"examples/index.html#cost-considerations","title":"Cost Considerations","text":"<p>Each example includes cost estimates and optimization tips:</p> Example Estimated Monthly Cost* Primary Cost Drivers Bedrock LLM Processor $50-200 Bedrock API calls, Lambda execution BDA Processor $100-400 BDA processing, managed services SageMaker UDOP $200-800 SageMaker endpoints, model hosting Processing Environment $20-100 DynamoDB, Lambda, API Gateway Processing Environment API $10-50 API Gateway, Lambda User Identity $5-20 Cognito users, authentication Core Tables $5-30 DynamoDB storage and operations <p>*Estimates based on moderate usage (1,000 documents/month). Actual costs depend on document volume, processing frequency, and AWS region.</p>"},{"location":"examples/index.html#support-and-troubleshooting","title":"Support and Troubleshooting","text":""},{"location":"examples/index.html#getting-help","title":"Getting Help","text":"<ul> <li>Check individual example README files for specific guidance</li> <li>Review troubleshooting guide</li> <li>Consult FAQs</li> <li>Open an issue in the repository</li> </ul>"},{"location":"examples/index.html#next-steps","title":"Next Steps","text":"<ol> <li>Start Simple: Begin with the Bedrock LLM Processor example</li> <li>Explore Repository: Browse the actual examples in the <code>/examples</code> directory</li> <li>Customize: Modify examples for your specific use cases</li> <li>Scale Up: Combine multiple examples for complex solutions</li> </ol> <p>Ready to get started? Choose an example and begin your GenAI IDP journey!</p>"},{"location":"examples/bda-processor.html","title":"BDA Processor Example","text":"<p>This example demonstrates how to use Amazon Bedrock Data Automation (BDA) for processing standard document types with managed schemas. It's ideal for production deployments with well-defined document types.</p>"},{"location":"examples/bda-processor.html#overview","title":"Overview","text":"<p>The BDA Processor example uses Amazon Bedrock Data Automation to provide: 1. Managed Processing \u2192 Pre-built schemas for common document types 2. Production Ready \u2192 Enterprise-grade processing pipeline 3. Standardized Output \u2192 Consistent data extraction formats 4. Scalable Architecture \u2192 Handles high document volumes</p>"},{"location":"examples/bda-processor.html#key-differences-from-bedrock-llm-processor","title":"Key Differences from Bedrock LLM Processor","text":"Aspect BDA Processor Bedrock LLM Processor Setup Requires BDA project creation Fully automated Document Types Predefined schemas Fully customizable Processing Managed service Multi-stage pipeline Customization Schema-based Full prompt control Best For Standard documents Custom requirements"},{"location":"examples/bda-processor.html#quick-start","title":"Quick Start","text":""},{"location":"examples/bda-processor.html#1-navigate-to-the-example","title":"1. Navigate to the Example","text":"<pre><code>cd examples/bda-processor\n</code></pre>"},{"location":"examples/bda-processor.html#2-review-the-configuration","title":"2. Review the Configuration","text":"<p>The example includes multiple configuration files:</p> <pre><code># Minimal configuration (ready to use)\ncat terraform.tfvars\n\n# Comprehensive configuration\ncat terraform.tfvars.comprehensive\n\n# Template with all options\ncat terraform.tfvars.example\n</code></pre>"},{"location":"examples/bda-processor.html#3-prerequisites","title":"3. Prerequisites","text":"<p>Before deploying, you need to:</p> <ol> <li>Create BDA Project: Set up a BDA project in the AWS Console</li> <li>Configure Document Types: Define the document schemas you want to process</li> <li>Enable Model Access: Ensure Bedrock model access is enabled</li> </ol>"},{"location":"examples/bda-processor.html#4-deploy","title":"4. Deploy","text":"<pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre>"},{"location":"examples/bda-processor.html#configuration-options","title":"Configuration Options","text":""},{"location":"examples/bda-processor.html#basic-configuration","title":"Basic Configuration","text":"<pre><code># terraform.tfvars\nregion = \"us-east-1\"\nprefix = \"bda-idp\"\nbda_project_name = \"your-bda-project-name\"\nadmin_email = \"admin@example.com\"\n</code></pre>"},{"location":"examples/bda-processor.html#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Enhanced settings for production\nregion = \"us-east-1\"\nprefix = \"prod-bda-idp\"\nbda_project_name = \"production-bda-project\"\nadmin_email = \"admin@company.com\"\n\n# Performance tuning\nmax_processing_concurrency = 100\nlog_retention_days = 30\n\n# Security settings\nenable_logging = true\nenable_guardrails = true\n\n# Web UI configuration\nenable_web_ui = true\n</code></pre>"},{"location":"examples/bda-processor.html#features","title":"Features","text":""},{"location":"examples/bda-processor.html#managed-processing","title":"Managed Processing","text":"<ul> <li>Pre-built document schemas</li> <li>Automatic data extraction</li> <li>Standardized output formats</li> <li>Enterprise-grade reliability</li> </ul>"},{"location":"examples/bda-processor.html#document-types-supported","title":"Document Types Supported","text":"<ul> <li>Invoices and receipts</li> <li>Forms and applications</li> <li>Financial statements</li> <li>Insurance documents</li> <li>Healthcare records</li> </ul>"},{"location":"examples/bda-processor.html#production-features","title":"Production Features","text":"<ul> <li>High availability architecture</li> <li>Automatic scaling</li> <li>Error handling and retry logic</li> <li>Comprehensive monitoring</li> </ul>"},{"location":"examples/bda-processor.html#integration-ready","title":"Integration Ready","text":"<ul> <li>GraphQL API for status tracking</li> <li>Web UI for document management</li> <li>S3 integration for document storage</li> <li>DynamoDB for metadata tracking</li> </ul>"},{"location":"examples/bda-processor.html#architecture","title":"Architecture","text":"<pre><code>graph TB\n    A[Document Upload] --&gt; B[S3 Input Bucket]\n    B --&gt; C[BDA Processing]\n    C --&gt; D[Structured Output]\n    D --&gt; E[S3 Output Bucket]\n    F[DynamoDB Tracking] --&gt; C\n    G[Web UI] --&gt; B\n    G --&gt; E\n    H[GraphQL API] --&gt; F</code></pre>"},{"location":"examples/bda-processor.html#usage-workflow","title":"Usage Workflow","text":""},{"location":"examples/bda-processor.html#1-document-upload","title":"1. Document Upload","text":"<pre><code># Upload documents to the input bucket\nINPUT_BUCKET=$(terraform output -raw buckets | jq -r '.input_bucket.bucket_name')\naws s3 cp invoice.pdf s3://$INPUT_BUCKET/\n</code></pre>"},{"location":"examples/bda-processor.html#2-automatic-processing","title":"2. Automatic Processing","text":"<p>The BDA processor automatically: - Detects new documents - Applies appropriate schema - Extracts structured data - Stores results with metadata</p>"},{"location":"examples/bda-processor.html#3-monitor-progress","title":"3. Monitor Progress","text":"<pre><code># Check processing status\nGRAPHQL_URL=$(terraform output -raw processing_environment | jq -r '.api.graphql_url')\ncurl -X POST $GRAPHQL_URL \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"query { listDocuments { id status } }\"}'\n</code></pre>"},{"location":"examples/bda-processor.html#4-retrieve-results","title":"4. Retrieve Results","text":"<pre><code># Download processed results\nOUTPUT_BUCKET=$(terraform output -raw buckets | jq -r '.output_bucket.bucket_name')\naws s3 cp s3://$OUTPUT_BUCKET/processed/ . --recursive\n</code></pre>"},{"location":"examples/bda-processor.html#bda-project-setup","title":"BDA Project Setup","text":""},{"location":"examples/bda-processor.html#1-create-bda-project-in-aws-console","title":"1. Create BDA Project in AWS Console","text":"<ol> <li>Navigate to Amazon Bedrock \u2192 Data Automation</li> <li>Create a new project</li> <li>Configure document types and schemas</li> <li>Note the project name for Terraform configuration</li> </ol>"},{"location":"examples/bda-processor.html#2-configure-document-schemas","title":"2. Configure Document Schemas","text":"<p>Define the document types you want to process: - Invoice processing schema - Form extraction schema - Custom document types</p>"},{"location":"examples/bda-processor.html#3-test-bda-project","title":"3. Test BDA Project","text":"<p>Validate your BDA project works correctly before deploying the Terraform infrastructure.</p>"},{"location":"examples/bda-processor.html#cost-considerations","title":"Cost Considerations","text":""},{"location":"examples/bda-processor.html#monthly-cost-breakdown-1000-documents","title":"Monthly Cost Breakdown (1,000 documents)","text":"Service Usage Estimated Cost BDA Processing 1,000 documents $50-100 Amazon S3 Storage + requests $10-20 AWS Lambda Processing functions $5-15 DynamoDB Tracking data $5-10 API Gateway GraphQL API $5-10 Total $75-155/month"},{"location":"examples/bda-processor.html#cost-optimization-tips","title":"Cost Optimization Tips","text":"<ol> <li>Batch Processing: Process documents in batches during off-peak hours</li> <li>Schema Optimization: Use efficient BDA schemas</li> <li>Storage Lifecycle: Implement S3 lifecycle policies</li> <li>Monitoring: Set up cost alerts and monitoring</li> </ol>"},{"location":"examples/bda-processor.html#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"examples/bda-processor.html#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<ul> <li>BDA processing success/failure rates</li> <li>Document processing latency</li> <li>Error rates and types</li> <li>Cost tracking</li> </ul>"},{"location":"examples/bda-processor.html#common-issues","title":"Common Issues","text":""},{"location":"examples/bda-processor.html#bda-project-not-found","title":"BDA Project Not Found","text":"<p><pre><code>Error: BDA project 'project-name' not found\n</code></pre> Solution: Verify BDA project exists and name is correct</p>"},{"location":"examples/bda-processor.html#schema-mismatch","title":"Schema Mismatch","text":"<p><pre><code>Error: Document doesn't match expected schema\n</code></pre> Solution: Review document format and BDA schema configuration</p>"},{"location":"examples/bda-processor.html#processing-delays","title":"Processing Delays","text":"<p>Symptoms: Documents uploaded but not processed Solution: Check BDA project status and quotas</p>"},{"location":"examples/bda-processor.html#security-features","title":"Security Features","text":""},{"location":"examples/bda-processor.html#data-protection","title":"Data Protection","text":"<ul> <li>End-to-end encryption</li> <li>Secure document storage</li> <li>Access logging and monitoring</li> <li>Compliance-ready architecture</li> </ul>"},{"location":"examples/bda-processor.html#access-control","title":"Access Control","text":"<ul> <li>IAM-based permissions</li> <li>Cognito user authentication</li> <li>API-level security</li> <li>Resource-based policies</li> </ul>"},{"location":"examples/bda-processor.html#comparison-with-other-examples","title":"Comparison with Other Examples","text":""},{"location":"examples/bda-processor.html#when-to-use-bda-processor","title":"When to Use BDA Processor","text":"<p>Choose BDA Processor when you need: - Standard document types (invoices, forms, etc.) - Managed processing with minimal customization - Production-ready solution out of the box - Enterprise-grade reliability and support</p> <p>Choose Bedrock LLM Processor when you need: - Custom document types or processing logic - Full control over AI prompts and models - Flexible processing pipeline - Custom business rules</p>"},{"location":"examples/bda-processor.html#next-steps","title":"Next Steps","text":""},{"location":"examples/bda-processor.html#production-deployment","title":"Production Deployment","text":"<ol> <li>Multi-Environment Setup: Deploy to dev/staging/prod</li> <li>Monitoring: Set up comprehensive monitoring and alerting</li> <li>Backup: Implement backup and disaster recovery</li> <li>Security: Review and harden security configurations</li> </ol>"},{"location":"examples/bda-processor.html#integration","title":"Integration","text":"<ol> <li>API Integration: Connect with existing systems</li> <li>Workflow Integration: Integrate with business processes</li> <li>Data Pipeline: Set up data processing pipelines</li> <li>Reporting: Create dashboards and reports</li> </ol>"},{"location":"examples/bda-processor.html#support","title":"Support","text":"<ul> <li>Example Documentation: Check the BDA Processor README</li> <li>Troubleshooting: Review troubleshooting guide</li> <li>Community: Open issues in the repository</li> </ul>"},{"location":"examples/bda-processor.html#related-examples","title":"Related Examples","text":"<ul> <li>Bedrock LLM Processor - Alternative with custom processing</li> <li>Processing Environment - Core infrastructure</li> <li>SageMaker UDOP - Advanced custom models</li> </ul>"},{"location":"examples/bedrock-llm-processor.html","title":"Bedrock LLM Processor Example","text":"<p>This example demonstrates how to use the Bedrock LLM processor to create a flexible document processing pipeline using Amazon Bedrock foundation models. It's perfect for custom document processing with advanced AI capabilities.</p>"},{"location":"examples/bedrock-llm-processor.html#overview","title":"Overview","text":"<p>The Bedrock LLM Processor example provides: 1. Document Upload \u2192 S3 bucket triggers processing 2. OCR Processing \u2192 Amazon Textract extracts text 3. AI Analysis \u2192 Amazon Bedrock analyzes content with custom prompts 4. Results Storage \u2192 Processed results stored in S3 5. Web UI \u2192 Optional interface for document management</p>"},{"location":"examples/bedrock-llm-processor.html#quick-start","title":"Quick Start","text":""},{"location":"examples/bedrock-llm-processor.html#1-navigate-to-the-bedrock-llm-processor-example","title":"1. Navigate to the Bedrock LLM Processor Example","text":"<pre><code>cd examples/bedrock-llm-processor\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#2-use-the-minimal-configuration","title":"2. Use the Minimal Configuration","text":"<p>The example includes a ready-to-use <code>terraform.tfvars</code> file for basic deployment:</p> <pre><code># terraform.tfvars (already provided)\nregion = \"us-east-1\"\nprefix = \"idp\"\nadmin_email = \"admin@example.com\"\nlog_level = \"INFO\"\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#3-deploy","title":"3. Deploy","text":"<pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#4-test-the-pipeline","title":"4. Test the Pipeline","text":"<pre><code># Upload a test document\nINPUT_BUCKET=$(terraform output -raw buckets | jq -r '.input_bucket.bucket_name')\necho \"Test invoice content\" &gt; test-invoice.txt\naws s3 cp test-invoice.txt s3://$INPUT_BUCKET/\n\n# Check results\naws s3 ls s3://$(terraform output -raw buckets | jq -r '.output_bucket.bucket_name')/ --recursive\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#configuration-options","title":"Configuration Options","text":"<p>The Bedrock LLM Processor example provides three configuration levels:</p>"},{"location":"examples/bedrock-llm-processor.html#minimal-default","title":"Minimal (Default)","text":"<ul> <li>Uses <code>terraform.tfvars</code> as-is</li> <li>Basic functionality enabled</li> <li>Suitable for testing and learning</li> </ul>"},{"location":"examples/bedrock-llm-processor.html#comprehensive","title":"Comprehensive","text":"<ul> <li>Copy <code>terraform.tfvars.comprehensive</code> to <code>terraform.tfvars</code></li> <li>All features enabled</li> <li>Production-ready configuration</li> </ul>"},{"location":"examples/bedrock-llm-processor.html#custom","title":"Custom","text":"<ul> <li>Start with <code>terraform.tfvars.example</code></li> <li>Customize for your specific needs</li> <li>Full control over all settings</li> </ul>"},{"location":"examples/bedrock-llm-processor.html#features-available","title":"Features Available","text":"<ul> <li>Document processing pipeline</li> <li>Web UI for document management</li> <li>GraphQL API for status tracking</li> <li>Multiple Bedrock models support</li> <li>Custom document classes</li> <li>Evaluation framework (optional)</li> <li>Document summarization (optional)</li> <li>Custom prompt engineering</li> <li>Flexible AI model selection</li> </ul>"},{"location":"examples/bedrock-llm-processor.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"examples/bedrock-llm-processor.html#custom-prompts","title":"Custom Prompts","text":"<p>Modify the processor configuration to use custom prompts for specific document types:</p> <pre><code># In terraform.tfvars\ncustom_prompts = {\n  invoice = \"Extract invoice details including vendor, amount, and date...\"\n  contract = \"Identify key contract terms, parties, and obligations...\"\n}\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#model-selection","title":"Model Selection","text":"<p>Choose different Bedrock models for different processing tasks:</p> <pre><code># In terraform.tfvars\nbedrock_models = {\n  classification = \"anthropic.claude-3-haiku-20240307-v1:0\"\n  extraction = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n  summarization = \"anthropic.claude-3-opus-20240229-v1:0\"\n}\n</code></pre>"},{"location":"examples/bedrock-llm-processor.html#next-steps","title":"Next Steps","text":"<ol> <li>Explore the Example: Review the Bedrock LLM Processor documentation</li> <li>Customize: Modify the configuration for your use case</li> <li>Scale Up: Move to more advanced examples when ready</li> </ol>"},{"location":"examples/bedrock-llm-processor.html#related-examples","title":"Related Examples","text":"<ul> <li>BDA Processor - Alternative using Bedrock Data Automation</li> <li>SageMaker UDOP Processor - Custom model approach</li> <li>Processing Environment - Core infrastructure only</li> </ul>"},{"location":"faqs/index.html","title":"Frequently Asked Questions","text":"<p>Find answers to common questions about the GenAI IDP Accelerator for Terraform. If you don't find your answer here, please check our troubleshooting guide or open an issue in the repository.</p>"},{"location":"faqs/index.html#quick-navigation","title":"Quick Navigation","text":"<ul> <li>General Questions - About the project, features, and capabilities</li> <li>Deployment Questions - Installation, configuration, and setup</li> <li>Troubleshooting - Common errors and solutions</li> <li>Cost Questions - Pricing, optimization, and billing</li> </ul>"},{"location":"faqs/index.html#most-common-questions","title":"Most Common Questions","text":""},{"location":"faqs/index.html#what-is-the-genai-idp-accelerator","title":"What is the GenAI IDP Accelerator?","text":"<p>The GenAI IDP Accelerator is a collection of Terraform modules that enables rapid deployment of intelligent document processing solutions on AWS. It combines services like Amazon Textract, Amazon Bedrock, and other AWS AI services to create end-to-end document processing workflows.</p>"},{"location":"faqs/index.html#how-much-does-it-cost-to-run","title":"How much does it cost to run?","text":"<p>Costs vary based on usage, but typical deployments range from $10-50/month for development to $200-1000+/month for production environments. See our cost guide for detailed breakdowns and optimization tips.</p>"},{"location":"faqs/index.html#what-document-formats-are-supported","title":"What document formats are supported?","text":"<p>The accelerator supports: - PDFs (text and image-based) - Images (JPEG, PNG, TIFF) - Office documents (Word, Excel, PowerPoint) - Text files - Handwritten documents</p>"},{"location":"faqs/index.html#which-aws-regions-are-supported","title":"Which AWS regions are supported?","text":"<p>The accelerator works in regions where all required services are available. Recommended regions include: - <code>us-east-1</code> (N. Virginia) - Best service availability - <code>us-west-2</code> (Oregon) - <code>eu-west-1</code> (Ireland) - <code>ap-southeast-1</code> (Singapore)</p>"},{"location":"faqs/index.html#how-long-does-deployment-take","title":"How long does deployment take?","text":"<ul> <li>Basic pipeline: 10-15 minutes</li> <li>Advanced workflow: 20-30 minutes</li> <li>Production setup: 45-60 minutes</li> </ul>"},{"location":"faqs/index.html#can-i-customize-the-processing-logic","title":"Can I customize the processing logic?","text":"<p>Yes! The accelerator is designed to be highly customizable. You can: - Modify existing Lambda functions - Add custom processing steps - Integrate with external APIs - Create custom AI prompts - Add business-specific logic</p>"},{"location":"faqs/index.html#is-it-production-ready","title":"Is it production-ready?","text":"<p>Yes, when deployed with appropriate security and monitoring configurations. The accelerator includes: - Security best practices - Monitoring and alerting - Error handling and retry logic - Scalability features - Cost optimization</p>"},{"location":"faqs/index.html#how-do-i-get-support","title":"How do I get support?","text":"<ol> <li>Check this FAQ and documentation</li> <li>Review the troubleshooting guide</li> <li>Search existing issues in the repository</li> <li>Open a new issue with detailed information</li> </ol>"},{"location":"faqs/index.html#popular-topics","title":"Popular Topics","text":""},{"location":"faqs/index.html#getting-started","title":"Getting Started","text":"<ul> <li>Prerequisites</li> <li>Quick Start Guide</li> <li>First Deployment</li> </ul>"},{"location":"faqs/index.html#common-issues","title":"Common Issues","text":"<ul> <li>Permission Errors</li> <li>Resource Limits</li> <li>Deployment Failures</li> </ul>"},{"location":"faqs/index.html#cost-management","title":"Cost Management","text":"<ul> <li>Cost Estimation</li> <li>Optimization Tips</li> <li>Billing Alerts</li> </ul>"},{"location":"faqs/index.html#customization","title":"Customization","text":"<ul> <li>Custom Processing Logic</li> <li>Integration Options</li> <li>Scaling Considerations</li> </ul>"},{"location":"faqs/index.html#cant-find-your-answer","title":"Can't Find Your Answer?","text":"<p>If your question isn't answered here:</p> <ol> <li>Search the documentation - Use the search function to find relevant information</li> <li>Check examples - Review our examples for similar use cases</li> <li>Review deployment guides - See our deployment guides for detailed instructions</li> <li>Open an issue - Create a detailed issue in the repository</li> </ol>"},{"location":"faqs/index.html#contributing-to-faqs","title":"Contributing to FAQs","text":"<p>Help improve this FAQ by: - Suggesting new questions based on your experience - Providing better answers to existing questions - Sharing solutions to problems you've encountered - Contributing examples and use cases</p> <p>See our contributing guide for more information.</p>"},{"location":"faqs/cost.html","title":"Cost Questions","text":"<p>Frequently asked questions about costs and pricing for the GenAI IDP Accelerator.</p>"},{"location":"faqs/cost.html#cost-estimation","title":"Cost Estimation","text":""},{"location":"faqs/cost.html#what-are-the-main-cost-components","title":"What are the main cost components?","text":"<p>The primary costs include:</p> <ol> <li>Amazon Bedrock (40-60% of total cost)</li> <li>Pay per API call and tokens processed</li> <li>Varies by model (Claude, Titan, etc.)</li> <li> <p>Typically $0.0003-$0.003 per 1K tokens</p> </li> <li> <p>Amazon Textract (15-25% of total cost)</p> </li> <li>Pay per page processed</li> <li>$0.0015 per page for basic text detection</li> <li> <p>$0.05-$0.065 per page for advanced features</p> </li> <li> <p>AWS Lambda (10-20% of total cost)</p> </li> <li>Pay per invocation and duration</li> <li>$0.0000166667 per GB-second</li> <li> <p>$0.0000002 per request</p> </li> <li> <p>Amazon S3 (5-15% of total cost)</p> </li> <li>Storage costs: $0.023 per GB/month</li> <li> <p>Request costs: $0.0004-$0.0005 per 1K requests</p> </li> <li> <p>Amazon DynamoDB (5-10% of total cost)</p> </li> <li>On-demand: $0.25 per million read/write requests</li> <li>Storage: $0.25 per GB/month</li> </ol>"},{"location":"faqs/cost.html#how-much-will-it-cost-per-document","title":"How much will it cost per document?","text":"<p>Typical costs per document:</p> <p>Simple documents (1-2 pages, basic extraction): - Textract: $0.003-$0.006 - Bedrock: $0.001-$0.005 - Lambda/Storage: $0.001 - Total: $0.005-$0.012 per document</p> <p>Complex documents (5-10 pages, detailed analysis): - Textract: $0.025-$0.065 - Bedrock: $0.010-$0.030 - Lambda/Storage: $0.002 - Total: $0.037-$0.097 per document</p>"},{"location":"faqs/cost.html#whats-a-typical-monthly-cost","title":"What's a typical monthly cost?","text":"<p>Monthly costs depend on volume:</p> <p>Low volume (100 documents/month): - Total: $5-15/month</p> <p>Medium volume (1,000 documents/month): - Total: $50-150/month</p> <p>High volume (10,000 documents/month): - Total: $500-1,500/month</p> <p>Enterprise volume (100,000+ documents/month): - Total: $5,000-15,000+/month</p>"},{"location":"faqs/cost.html#optimization-tips","title":"Optimization Tips","text":""},{"location":"faqs/cost.html#how-can-i-reduce-bedrock-costs","title":"How can I reduce Bedrock costs?","text":"<ol> <li>Choose appropriate models:</li> <li>Use Titan for simple tasks ($0.0003/1K tokens)</li> <li>Use Claude Haiku for moderate complexity ($0.00025/1K tokens)</li> <li> <p>Reserve Claude Sonnet for complex analysis ($0.003/1K tokens)</p> </li> <li> <p>Optimize prompts:</p> </li> <li>Keep prompts concise and specific</li> <li>Limit output token requirements</li> <li> <p>Use structured output formats</p> </li> <li> <p>Implement caching:</p> </li> <li>Cache similar document results</li> <li>Avoid reprocessing identical content</li> <li> <p>Use DynamoDB for response caching</p> </li> <li> <p>Batch processing:</p> </li> <li>Process multiple documents together</li> <li>Use scheduled processing during off-peak hours</li> <li>Implement queue-based processing</li> </ol>"},{"location":"faqs/cost.html#how-can-i-reduce-lambda-costs","title":"How can I reduce Lambda costs?","text":"<ol> <li>Right-size memory allocation:</li> <li>Start with 512MB and adjust based on performance</li> <li>Higher memory = faster execution = potentially lower cost</li> <li> <p>Monitor duration vs. memory usage</p> </li> <li> <p>Optimize code performance:</p> </li> <li>Reuse connections and clients</li> <li>Minimize cold starts</li> <li> <p>Use efficient algorithms and data structures</p> </li> <li> <p>Use ARM-based processors:</p> </li> <li>Up to 20% cost savings</li> <li> <p>Set <code>architecture = [\"arm64\"]</code> in Terraform</p> </li> <li> <p>Implement proper error handling:</p> </li> <li>Avoid unnecessary retries</li> <li>Use dead letter queues</li> <li>Set appropriate timeouts</li> </ol>"},{"location":"faqs/cost.html#how-can-i-reduce-storage-costs","title":"How can I reduce storage costs?","text":"<ol> <li> <p>S3 lifecycle policies:    <pre><code>lifecycle_configuration {\n  rule {\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Intelligent tiering:</p> </li> <li>Automatically moves objects to cost-effective tiers</li> <li> <p>No retrieval fees for frequent access tier</p> </li> <li> <p>Compress documents:</p> </li> <li>Use compression for stored results</li> <li> <p>Optimize image formats and quality</p> </li> <li> <p>Clean up old data:</p> </li> <li>Implement retention policies</li> <li>Delete processed temporary files</li> <li>Archive old documents</li> </ol>"},{"location":"faqs/cost.html#billing-alerts","title":"Billing Alerts","text":""},{"location":"faqs/cost.html#how-do-i-set-up-cost-monitoring","title":"How do I set up cost monitoring?","text":"<ol> <li> <p>AWS Budgets:    <pre><code>resource \"aws_budgets_budget\" \"monthly_budget\" {\n  name         = \"idp-monthly-budget\"\n  budget_type  = \"COST\"\n  limit_amount = \"100\"\n  limit_unit   = \"USD\"\n  time_unit    = \"MONTHLY\"\n}\n</code></pre></p> </li> <li> <p>CloudWatch billing alarms:    <pre><code>resource \"aws_cloudwatch_metric_alarm\" \"billing_alarm\" {\n  alarm_name          = \"idp-billing-alarm\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"1\"\n  metric_name         = \"EstimatedCharges\"\n  namespace           = \"AWS/Billing\"\n  period              = \"86400\"\n  statistic           = \"Maximum\"\n  threshold           = \"50\"\n}\n</code></pre></p> </li> <li> <p>Cost anomaly detection:</p> </li> <li>Automatically detect unusual spending patterns</li> <li>Get alerts for unexpected cost increases</li> <li>Set up notifications via email or SNS</li> </ol>"},{"location":"faqs/cost.html#what-billing-alerts-should-i-set-up","title":"What billing alerts should I set up?","text":"<p>Essential alerts: - Monthly budget threshold (80% and 100%) - Daily spending anomalies - Service-specific cost spikes - Unusual API call patterns</p> <p>Recommended thresholds: - Development: $10-50/month - Staging: $50-200/month - Production: Based on expected volume</p>"},{"location":"faqs/cost.html#cost-analysis","title":"Cost Analysis","text":""},{"location":"faqs/cost.html#how-do-i-track-costs-by-environment","title":"How do I track costs by environment?","text":"<p>Use consistent tagging: <pre><code>locals {\n  common_tags = {\n    Environment = var.environment\n    Project     = \"genai-idp-accelerator\"\n    CostCenter  = var.cost_center\n  }\n}\n</code></pre></p> <p>Then use AWS Cost Explorer to filter by tags.</p>"},{"location":"faqs/cost.html#how-do-i-analyze-cost-trends","title":"How do I analyze cost trends?","text":"<ol> <li>AWS Cost Explorer:</li> <li>View costs by service, time period, and tags</li> <li>Create custom reports and dashboards</li> <li> <p>Analyze usage patterns and trends</p> </li> <li> <p>CloudWatch dashboards:</p> </li> <li>Monitor real-time usage metrics</li> <li>Track API calls and processing volumes</li> <li> <p>Correlate usage with costs</p> </li> <li> <p>Custom cost tracking:    <pre><code># Lambda function to track processing costs\ndef track_processing_cost(document_pages, tokens_used):\n    textract_cost = document_pages * 0.0015\n    bedrock_cost = tokens_used * 0.0003 / 1000\n    total_cost = textract_cost + bedrock_cost\n\n    # Store in DynamoDB for analysis\n    store_cost_metrics(total_cost, document_pages, tokens_used)\n</code></pre></p> </li> </ol>"},{"location":"faqs/cost.html#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"faqs/cost.html#development-environment-cost-control","title":"Development environment cost control","text":"<ol> <li> <p>Use smaller resources:    <pre><code>lambda_memory_size = var.environment == \"dev\" ? 512 : 1024\n</code></pre></p> </li> <li> <p>Implement auto-shutdown:</p> </li> <li>Schedule Lambda functions to stop after hours</li> <li>Use lifecycle policies for temporary storage</li> <li> <p>Clean up development resources regularly</p> </li> <li> <p>Limit processing volume:</p> </li> <li>Set concurrency limits for development</li> <li>Use smaller test datasets</li> <li>Implement rate limiting</li> </ol>"},{"location":"faqs/cost.html#production-cost-optimization","title":"Production cost optimization","text":"<ol> <li>Reserved capacity (for predictable workloads):</li> <li>DynamoDB reserved capacity</li> <li>Lambda provisioned concurrency (if needed)</li> <li> <p>S3 storage commitments</p> </li> <li> <p>Spot instances (for batch processing):</p> </li> <li>Use EC2 Spot instances for large batch jobs</li> <li>Implement fault-tolerant processing</li> <li> <p>Consider AWS Batch for complex workflows</p> </li> <li> <p>Multi-region optimization:</p> </li> <li>Use regions with lower pricing</li> <li>Implement data locality to reduce transfer costs</li> <li>Consider regional service availability</li> </ol>"},{"location":"faqs/cost.html#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"faqs/cost.html#why-is-my-bedrock-bill-so-high","title":"Why is my Bedrock bill so high?","text":"<p>Common causes: - Using expensive models for simple tasks - Inefficient prompts generating too many tokens - Processing duplicate content without caching - Not optimizing input text length</p>"},{"location":"faqs/cost.html#can-i-get-volume-discounts","title":"Can I get volume discounts?","text":"<ul> <li>AWS offers Enterprise Discount Programs for large customers</li> <li>Consider AWS Savings Plans for predictable usage</li> <li>Contact AWS sales for custom pricing discussions</li> <li>Use AWS Credits if available</li> </ul>"},{"location":"faqs/cost.html#how-do-i-forecast-future-costs","title":"How do I forecast future costs?","text":"<ol> <li>Analyze current usage patterns</li> <li>Project document volume growth</li> <li>Use AWS Cost Calculator</li> <li>Monitor cost per document trends</li> <li>Plan for seasonal variations</li> </ol>"},{"location":"faqs/cost.html#what-if-i-exceed-my-budget","title":"What if I exceed my budget?","text":"<ol> <li>Immediate actions:</li> <li>Check for unusual activity or errors</li> <li>Temporarily reduce processing limits</li> <li> <p>Pause non-critical workloads</p> </li> <li> <p>Investigation:</p> </li> <li>Review CloudTrail logs for unexpected API calls</li> <li>Check for failed requests causing retries</li> <li> <p>Analyze cost breakdown by service</p> </li> <li> <p>Prevention:</p> </li> <li>Implement stricter budget alerts</li> <li>Add cost controls and limits</li> <li>Review and optimize regularly</li> </ol> <p>For more cost optimization guidance, see: - Cost Optimization Guide - Best Practices - Monitoring Guide</p>"},{"location":"faqs/deployment.html","title":"Deployment Questions","text":"<p>Frequently asked questions about deploying the GenAI IDP Accelerator.</p>"},{"location":"faqs/deployment.html#first-deployment","title":"First Deployment","text":""},{"location":"faqs/deployment.html#what-do-i-need-before-starting","title":"What do I need before starting?","text":"<p>Before your first deployment, ensure you have: - AWS account with administrative permissions - Terraform installed (version 1.0 or later) - AWS CLI configured with credentials - Access to Amazon Bedrock models in your region - S3 bucket for Terraform state (recommended)</p>"},{"location":"faqs/deployment.html#how-do-i-request-bedrock-model-access","title":"How do I request Bedrock model access?","text":"<ol> <li>Go to the Amazon Bedrock console</li> <li>Navigate to \"Model access\" in the left sidebar</li> <li>Click \"Request model access\"</li> <li>Select the models you need (Claude, Titan, etc.)</li> <li>Submit the request and wait for approval (usually immediate)</li> </ol>"},{"location":"faqs/deployment.html#what-regions-are-supported","title":"What regions are supported?","text":"<p>The accelerator works in any AWS region that supports: - Amazon Bedrock - Amazon Textract - AWS Lambda - Amazon S3 - Amazon DynamoDB</p> <p>Popular regions include: <code>us-east-1</code>, <code>us-west-2</code>, <code>eu-west-1</code>, <code>ap-southeast-1</code></p>"},{"location":"faqs/deployment.html#how-long-does-deployment-take","title":"How long does deployment take?","text":"<p>Typical deployment times: - Initial deployment: 10-15 minutes - Updates: 5-10 minutes - Destroy: 5-10 minutes</p> <p>Times may vary based on region and resource complexity.</p>"},{"location":"faqs/deployment.html#configuration","title":"Configuration","text":""},{"location":"faqs/deployment.html#how-do-i-customize-the-deployment","title":"How do I customize the deployment?","text":"<p>Create a <code>terraform.tfvars</code> file with your settings: <pre><code>environment = \"dev\"\nregion = \"us-east-1\"\nproject_name = \"my-idp-project\"\n\n# Lambda configuration\nlambda_memory_size = 1024\nlambda_timeout = 300\n\n# Storage configuration\ns3_bucket_prefix = \"my-company-idp\"\n</code></pre></p>"},{"location":"faqs/deployment.html#can-i-use-existing-aws-resources","title":"Can I use existing AWS resources?","text":"<p>Yes, you can reference existing resources: <pre><code># Use existing VPC\nvpc_id = \"vpc-12345678\"\nsubnet_ids = [\"subnet-12345678\", \"subnet-87654321\"]\n\n# Use existing S3 bucket\nexisting_s3_bucket = \"my-existing-bucket\"\n\n# Use existing DynamoDB table\nexisting_dynamodb_table = \"my-existing-table\"\n</code></pre></p>"},{"location":"faqs/deployment.html#how-do-i-configure-different-environments","title":"How do I configure different environments?","text":"<p>Create separate directories for each environment: <pre><code>terraform/\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 dev/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terraform.tfvars\n\u2502   \u2514\u2500\u2500 prod/\n\u2502       \u251c\u2500\u2500 main.tf\n\u2502       \u2514\u2500\u2500 terraform.tfvars\n</code></pre></p>"},{"location":"faqs/deployment.html#state-management","title":"State Management","text":""},{"location":"faqs/deployment.html#should-i-use-remote-state","title":"Should I use remote state?","text":"<p>Yes, always use remote state for production: <pre><code>terraform {\n  backend \"s3\" {\n    bucket = \"my-terraform-state\"\n    key    = \"idp-accelerator/terraform.tfstate\"\n    region = \"us-east-1\"\n    encrypt = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n</code></pre></p>"},{"location":"faqs/deployment.html#how-do-i-handle-state-conflicts","title":"How do I handle state conflicts?","text":"<p>If you encounter state lock issues: <pre><code># Check who has the lock\naws dynamodb get-item \\\n  --table-name terraform-locks \\\n  --key '{\"LockID\":{\"S\":\"my-state-file\"}}'\n\n# Force unlock (use carefully)\nterraform force-unlock LOCK_ID\n</code></pre></p>"},{"location":"faqs/deployment.html#can-i-import-existing-resources","title":"Can I import existing resources?","text":"<p>Yes, you can import existing AWS resources: <pre><code># Import existing S3 bucket\nterraform import aws_s3_bucket.documents my-existing-bucket\n\n# Import existing Lambda function\nterraform import aws_lambda_function.processor my-existing-function\n</code></pre></p>"},{"location":"faqs/deployment.html#updates-and-maintenance","title":"Updates and Maintenance","text":""},{"location":"faqs/deployment.html#how-do-i-update-the-accelerator","title":"How do I update the accelerator?","text":"<ol> <li>Update your Terraform configuration</li> <li>Run <code>terraform plan</code> to review changes</li> <li>Run <code>terraform apply</code> to apply updates</li> <li>Monitor the deployment for issues</li> </ol>"},{"location":"faqs/deployment.html#how-do-i-handle-breaking-changes","title":"How do I handle breaking changes?","text":"<p>Before major updates: 1. Backup your Terraform state 2. Test in a development environment 3. Review the changelog for breaking changes 4. Plan for potential downtime 5. Have a rollback strategy ready</p>"},{"location":"faqs/deployment.html#can-i-rollback-a-deployment","title":"Can I rollback a deployment?","text":"<p>Yes, you can rollback using: <pre><code># Restore previous state file\ncp terraform.tfstate.backup terraform.tfstate\n\n# Apply previous configuration\ngit checkout previous-version\nterraform apply\n</code></pre></p>"},{"location":"faqs/deployment.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"faqs/deployment.html#common-deployment-errors","title":"Common deployment errors?","text":"<p>Permission denied errors: - Check IAM permissions - Verify AWS credentials - Ensure service-linked roles exist</p> <p>Resource already exists: - Import existing resources - Use different resource names - Check for naming conflicts</p> <p>Timeout errors: - Increase timeout values - Check network connectivity - Verify service availability</p>"},{"location":"faqs/deployment.html#how-do-i-debug-deployment-issues","title":"How do I debug deployment issues?","text":"<p>Enable detailed logging: <pre><code>export TF_LOG=DEBUG\nterraform apply\n</code></pre></p> <p>Check AWS CloudTrail for API calls and errors.</p>"},{"location":"faqs/deployment.html#what-if-deployment-fails-halfway","title":"What if deployment fails halfway?","text":"<p>Terraform handles partial failures gracefully: 1. Fix the underlying issue 2. Run <code>terraform apply</code> again 3. Terraform will continue from where it left off 4. Use <code>terraform refresh</code> if state is inconsistent</p>"},{"location":"faqs/deployment.html#performance","title":"Performance","text":""},{"location":"faqs/deployment.html#how-do-i-optimize-deployment-speed","title":"How do I optimize deployment speed?","text":"<ul> <li>Use <code>-parallelism</code> flag: <code>terraform apply -parallelism=20</code></li> <li>Enable provider caching</li> <li>Use smaller resource sets for testing</li> <li>Deploy in regions closer to you</li> </ul>"},{"location":"faqs/deployment.html#can-i-deploy-multiple-environments-simultaneously","title":"Can I deploy multiple environments simultaneously?","text":"<p>Yes, but be careful with: - Resource limits and quotas - API rate limits - State file conflicts - Naming collisions</p> <p>Use separate state files and workspaces for each environment.</p>"},{"location":"faqs/deployment.html#security","title":"Security","text":""},{"location":"faqs/deployment.html#what-permissions-does-terraform-need","title":"What permissions does Terraform need?","text":"<p>Terraform needs permissions to create and manage: - IAM roles and policies - Lambda functions - S3 buckets - DynamoDB tables - API Gateway resources - CloudWatch logs and alarms</p> <p>Use the principle of least privilege and consider using cross-account roles.</p>"},{"location":"faqs/deployment.html#how-do-i-secure-sensitive-variables","title":"How do I secure sensitive variables?","text":"<p>Use Terraform's sensitive variables: <pre><code>variable \"api_key\" {\n  description = \"API key for external service\"\n  type        = string\n  sensitive   = true\n}\n</code></pre></p> <p>Or use AWS Systems Manager Parameter Store: <pre><code>data \"aws_ssm_parameter\" \"api_key\" {\n  name = \"/idp/api-key\"\n}\n</code></pre></p> <p>For more deployment help, see: - Environment Setup Guide - Troubleshooting Guide - Best Practices</p>"},{"location":"faqs/general.html","title":"General Questions","text":"<p>Common questions about the GenAI IDP Accelerator for Terraform.</p>"},{"location":"faqs/general.html#what-is-the-genai-idp-accelerator","title":"What is the GenAI IDP Accelerator?","text":"<p>The GenAI IDP Accelerator is a comprehensive solution for intelligent document processing using AWS services and generative AI. It provides pre-built Terraform modules to quickly deploy document processing pipelines that can extract, analyze, and process information from various document types.</p>"},{"location":"faqs/general.html#what-document-types-are-supported","title":"What document types are supported?","text":"<p>The accelerator supports common document formats including: - PDF documents - Images (PNG, JPEG, TIFF) - Scanned documents - Forms and invoices - Contracts and legal documents</p>"},{"location":"faqs/general.html#which-aws-services-does-it-use","title":"Which AWS services does it use?","text":"<p>Key AWS services include: - Amazon Textract: For text and data extraction - Amazon Bedrock: For AI-powered analysis and processing - AWS Lambda: For serverless compute - Amazon S3: For document storage - Amazon DynamoDB: For metadata storage - Amazon API Gateway: For REST API endpoints</p>"},{"location":"faqs/general.html#how-does-it-differ-from-the-cdk-version","title":"How does it differ from the CDK version?","text":"<p>The main differences are: - Infrastructure as Code: Uses Terraform instead of AWS CDK - Language: HCL configuration instead of TypeScript/Python - State Management: Terraform state files instead of CloudFormation stacks - Modularity: Terraform modules for reusable components</p>"},{"location":"faqs/general.html#can-i-customize-the-processing-logic","title":"Can I customize the processing logic?","text":"<p>Yes, the accelerator is designed to be customizable: - Modify Lambda function code for custom processing - Adjust AI prompts for specific use cases - Add new document types and processing workflows - Integrate with existing systems and databases</p>"},{"location":"faqs/general.html#what-are-the-prerequisites","title":"What are the prerequisites?","text":"<p>To use the accelerator, you need: - AWS account with appropriate permissions - Terraform installed (version 1.0+) - AWS CLI configured - Access to Amazon Bedrock models - Basic knowledge of Terraform and AWS services</p>"},{"location":"faqs/general.html#how-much-does-it-cost-to-run","title":"How much does it cost to run?","text":"<p>Costs depend on usage and include: - Amazon Bedrock: Pay per API call and tokens - Amazon Textract: Pay per page processed - AWS Lambda: Pay per invocation and duration - Storage: S3 and DynamoDB storage costs - Data Transfer: Network transfer costs</p> <p>Typical costs range from $10-100+ per month depending on document volume.</p>"},{"location":"faqs/general.html#is-it-production-ready","title":"Is it production-ready?","text":"<p>The accelerator provides a solid foundation for production use but requires: - Proper security configuration - Monitoring and alerting setup - Backup and disaster recovery planning - Performance testing and optimization - Compliance and governance controls</p>"},{"location":"faqs/general.html#can-i-deploy-in-multiple-regions","title":"Can I deploy in multiple regions?","text":"<p>Yes, the Terraform modules support multi-region deployment: - Deploy primary and secondary regions - Configure cross-region replication - Set up failover mechanisms - Implement global load balancing</p>"},{"location":"faqs/general.html#how-do-i-get-support","title":"How do I get support?","text":"<p>Support options include: - Documentation and guides - GitHub issues and discussions - AWS Support (for AWS service issues) - Community forums and resources</p>"},{"location":"faqs/general.html#whats-the-typical-implementation-timeline","title":"What's the typical implementation timeline?","text":"<p>Implementation timeline varies by complexity: - Basic deployment: 1-2 days - Customization: 1-2 weeks - Production readiness: 2-4 weeks - Full integration: 1-3 months</p>"},{"location":"faqs/general.html#can-i-integrate-with-existing-systems","title":"Can I integrate with existing systems?","text":"<p>Yes, integration options include: - REST API endpoints for external systems - Event-driven processing with SQS/SNS - Database integration with DynamoDB - File system integration with S3 - Custom Lambda functions for specific integrations</p>"},{"location":"faqs/general.html#what-security-features-are-included","title":"What security features are included?","text":"<p>Security features include: - IAM roles and policies for least privilege access - Encryption at rest and in transit - VPC isolation and security groups - API authentication and authorization - Audit logging with CloudTrail - Compliance with AWS security best practices</p>"},{"location":"faqs/general.html#how-do-i-monitor-the-system","title":"How do I monitor the system?","text":"<p>Monitoring capabilities include: - CloudWatch metrics and alarms - Application logs and error tracking - Performance monitoring and optimization - Cost tracking and budgets - Custom dashboards and reports</p>"},{"location":"faqs/general.html#can-i-use-it-for-batch-processing","title":"Can I use it for batch processing?","text":"<p>Yes, the accelerator supports both real-time and batch processing: - SQS queues for batch job management - Step Functions for complex workflows - Scheduled processing with EventBridge - Parallel processing for high throughput</p> <p>For more specific questions, see: - Deployment FAQs - Cost FAQs - Troubleshooting FAQs</p>"},{"location":"faqs/troubleshooting.html","title":"Troubleshooting Questions","text":"<p>Common issues and solutions when using the GenAI IDP Accelerator.</p>"},{"location":"faqs/troubleshooting.html#permission-errors","title":"Permission Errors","text":""},{"location":"faqs/troubleshooting.html#accessdenied-when-deploying-with-terraform","title":"\"AccessDenied\" when deploying with Terraform","text":"<p>Symptoms: <pre><code>Error: AccessDenied: User is not authorized to perform action\n</code></pre></p> <p>Solutions: 1. Check IAM permissions:    <pre><code>aws sts get-caller-identity\naws iam get-user\n</code></pre></p> <ol> <li>Verify required permissions:</li> <li>IAM: Create/manage roles and policies</li> <li>Lambda: Create/update functions</li> <li>S3: Create/manage buckets</li> <li>DynamoDB: Create/manage tables</li> <li> <p>API Gateway: Create/manage APIs</p> </li> <li> <p>Use administrator access temporarily:    <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#user-is-not-authorized-to-invoke-bedrock-model","title":"\"User is not authorized to invoke Bedrock model\"","text":"<p>Symptoms: <pre><code>Error: AccessDeniedException: Your account is not authorized to invoke this model\n</code></pre></p> <p>Solutions: 1. Request model access:    - Go to Amazon Bedrock console    - Navigate to \"Model access\"    - Request access for required models</p> <ol> <li>Check region availability:</li> <li>Bedrock models aren't available in all regions</li> <li> <p>Use supported regions like <code>us-east-1</code>, <code>us-west-2</code></p> </li> <li> <p>Verify model ARN:    <pre><code># Correct model ARN format\nmodel_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#resource-limits","title":"Resource Limits","text":""},{"location":"faqs/troubleshooting.html#limitexceededexception-errors","title":"\"LimitExceededException\" errors","text":"<p>Symptoms: <pre><code>Error: LimitExceededException: Account has reached the maximum number of functions\n</code></pre></p> <p>Solutions: 1. Check service quotas:    <pre><code>aws service-quotas get-service-quota \\\n  --service-code lambda \\\n  --quota-code L-B99A9384\n</code></pre></p> <ol> <li>Request quota increases:</li> <li>Go to AWS Service Quotas console</li> <li>Find the relevant service and quota</li> <li> <p>Submit increase request</p> </li> <li> <p>Clean up unused resources:    <pre><code># List unused Lambda functions\naws lambda list-functions --query 'Functions[?LastModified&lt;`2024-01-01`]'\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#throttlingexception-from-aws-services","title":"\"ThrottlingException\" from AWS services","text":"<p>Symptoms: <pre><code>Error: ThrottlingException: Rate exceeded\n</code></pre></p> <p>Solutions: 1. Implement exponential backoff:    <pre><code>import time\nimport random\n\ndef retry_with_backoff(func, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except ThrottlingException:\n            if attempt == max_retries - 1:\n                raise\n            wait_time = (2 ** attempt) + random.uniform(0, 1)\n            time.sleep(wait_time)\n</code></pre></p> <ol> <li>Reduce request rate:</li> <li>Add delays between API calls</li> <li>Use batch operations where possible</li> <li>Implement queue-based processing</li> </ol>"},{"location":"faqs/troubleshooting.html#deployment-failures","title":"Deployment Failures","text":""},{"location":"faqs/troubleshooting.html#terraform-state-lock-conflicts","title":"Terraform state lock conflicts","text":"<p>Symptoms: <pre><code>Error: Error acquiring the state lock\n</code></pre></p> <p>Solutions: 1. Wait for lock release:    - Another Terraform operation may be running    - Wait 10-15 minutes for automatic release</p> <ol> <li> <p>Check lock status:    <pre><code>aws dynamodb get-item \\\n  --table-name terraform-locks \\\n  --key '{\"LockID\":{\"S\":\"your-state-file\"}}'\n</code></pre></p> </li> <li> <p>Force unlock (use carefully):    <pre><code>terraform force-unlock LOCK_ID\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#resource-already-exists-errors","title":"\"Resource already exists\" errors","text":"<p>Symptoms: <pre><code>Error: ResourceAlreadyExistsException: Resource already exists\n</code></pre></p> <p>Solutions: 1. Import existing resource:    <pre><code>terraform import aws_s3_bucket.documents existing-bucket-name\n</code></pre></p> <ol> <li> <p>Use different resource names:    <pre><code>resource \"aws_s3_bucket\" \"documents\" {\n  bucket = \"${var.environment}-idp-documents-${random_id.suffix.hex}\"\n}\n</code></pre></p> </li> <li> <p>Check for naming conflicts:</p> </li> <li>S3 bucket names must be globally unique</li> <li>Lambda function names must be unique per region/account</li> </ol>"},{"location":"faqs/troubleshooting.html#runtime-errors","title":"Runtime Errors","text":""},{"location":"faqs/troubleshooting.html#lambda-function-timeouts","title":"Lambda function timeouts","text":"<p>Symptoms: <pre><code>Task timed out after 15.00 seconds\n</code></pre></p> <p>Solutions: 1. Increase timeout:    <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  timeout = 300  # 5 minutes\n}\n</code></pre></p> <ol> <li> <p>Optimize function performance:    <pre><code># Initialize clients outside handler\nimport boto3\n\ns3_client = boto3.client('s3')\ntextract_client = boto3.client('textract')\n\ndef lambda_handler(event, context):\n    # Use pre-initialized clients\n    pass\n</code></pre></p> </li> <li> <p>Increase memory allocation:    <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  memory_size = 1024  # More memory = more CPU\n}\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#document-format-not-supported-errors","title":"\"Document format not supported\" errors","text":"<p>Symptoms: <pre><code>Error: InvalidParameterException: Document format not supported\n</code></pre></p> <p>Solutions: 1. Check supported formats:    - PDF, PNG, JPEG, TIFF only    - Maximum file size: 10MB (sync), 500MB (async)</p> <ol> <li> <p>Validate file before processing:    <pre><code>import os\n\nSUPPORTED_EXTENSIONS = {'.pdf', '.png', '.jpg', '.jpeg', '.tiff'}\n\ndef validate_document(file_name):\n    ext = os.path.splitext(file_name)[1].lower()\n    return ext in SUPPORTED_EXTENSIONS\n</code></pre></p> </li> <li> <p>Convert unsupported formats:</p> </li> <li>Use Lambda layers with image processing libraries</li> <li>Convert to supported format before processing</li> </ol>"},{"location":"faqs/troubleshooting.html#performance-issues","title":"Performance Issues","text":""},{"location":"faqs/troubleshooting.html#slow-document-processing","title":"Slow document processing","text":"<p>Symptoms: - Long processing times - Frequent timeouts - High costs</p> <p>Solutions: 1. Optimize Lambda configuration:    <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  memory_size = 2048  # Higher memory for better performance\n  timeout     = 900   # 15 minutes max\n}\n</code></pre></p> <ol> <li> <p>Implement parallel processing:    <pre><code>import concurrent.futures\n\ndef process_documents_parallel(documents):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        futures = [executor.submit(process_document, doc) for doc in documents]\n        results = [future.result() for future in futures]\n    return results\n</code></pre></p> </li> <li> <p>Use asynchronous processing:</p> </li> <li>Use Textract async APIs for large documents</li> <li>Implement Step Functions for complex workflows</li> <li>Use SQS for queue-based processing</li> </ol>"},{"location":"faqs/troubleshooting.html#high-memory-usage","title":"High memory usage","text":"<p>Symptoms: <pre><code>Runtime.OutOfMemoryError: JavaScript heap out of memory\n</code></pre></p> <p>Solutions: 1. Increase Lambda memory:    <pre><code>memory_size = 3008  # Maximum available\n</code></pre></p> <ol> <li>Optimize memory usage:    <pre><code># Process documents in chunks\ndef process_large_document(document_text):\n    chunk_size = 10000  # characters\n    chunks = [document_text[i:i+chunk_size] \n             for i in range(0, len(document_text), chunk_size)]\n\n    results = []\n    for chunk in chunks:\n        result = process_chunk(chunk)\n        results.append(result)\n\n    return combine_results(results)\n</code></pre></li> </ol>"},{"location":"faqs/troubleshooting.html#network-issues","title":"Network Issues","text":""},{"location":"faqs/troubleshooting.html#vpc-connectivity-problems","title":"VPC connectivity problems","text":"<p>Symptoms: - Lambda functions can't reach AWS services - Timeout errors when calling APIs</p> <p>Solutions: 1. Check VPC configuration:    <pre><code># Ensure NAT Gateway for private subnets\nresource \"aws_nat_gateway\" \"main\" {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = aws_subnet.public[0].id\n}\n</code></pre></p> <ol> <li> <p>Use VPC endpoints:    <pre><code>resource \"aws_vpc_endpoint\" \"s3\" {\n  vpc_id       = aws_vpc.main.id\n  service_name = \"com.amazonaws.${var.region}.s3\"\n}\n</code></pre></p> </li> <li> <p>Check security groups:    <pre><code>resource \"aws_security_group\" \"lambda\" {\n  egress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#data-issues","title":"Data Issues","text":""},{"location":"faqs/troubleshooting.html#inconsistent-processing-results","title":"Inconsistent processing results","text":"<p>Symptoms: - Different results for same document - Missing or incorrect extracted data</p> <p>Solutions: 1. Improve prompt consistency:    <pre><code>CONSISTENT_PROMPT = \"\"\"\nExtract the following information from this document:\n1. Document type\n2. Date (format: YYYY-MM-DD)\n3. Amount (format: $X.XX)\n4. Parties involved\n\nReturn as JSON with these exact keys: type, date, amount, parties\n\"\"\"\n</code></pre></p> <ol> <li> <p>Implement validation:    <pre><code>def validate_extraction_result(result):\n    required_fields = ['type', 'date', 'amount', 'parties']\n    return all(field in result for field in required_fields)\n</code></pre></p> </li> <li> <p>Add error handling:    <pre><code>def process_with_fallback(document):\n    try:\n        result = primary_processing(document)\n        if validate_result(result):\n            return result\n    except Exception as e:\n        logger.warning(f\"Primary processing failed: {e}\")\n\n    # Fallback to simpler processing\n    return fallback_processing(document)\n</code></pre></p> </li> </ol>"},{"location":"faqs/troubleshooting.html#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"faqs/troubleshooting.html#how-to-debug-lambda-functions","title":"How to debug Lambda functions","text":"<ol> <li> <p>Enable detailed logging:    <pre><code>import logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n\ndef lambda_handler(event, context):\n    logger.debug(f\"Received event: {event}\")\n    # ... processing logic\n    logger.debug(f\"Processing result: {result}\")\n</code></pre></p> </li> <li> <p>Use X-Ray tracing:    <pre><code>resource \"aws_lambda_function\" \"processor\" {\n  tracing_config {\n    mode = \"Active\"\n  }\n}\n</code></pre></p> </li> <li> <p>Monitor CloudWatch metrics:</p> </li> <li>Duration, Errors, Throttles</li> <li>Memory utilization</li> <li>Concurrent executions</li> </ol>"},{"location":"faqs/troubleshooting.html#how-to-trace-api-requests","title":"How to trace API requests","text":"<ol> <li> <p>Enable API Gateway logging:    <pre><code>resource \"aws_api_gateway_stage\" \"main\" {\n  xray_tracing_enabled = true\n\n  access_log_settings {\n    destination_arn = aws_cloudwatch_log_group.api_gateway.arn\n    format = jsonencode({\n      requestId      = \"$context.requestId\"\n      ip            = \"$context.identity.sourceIp\"\n      caller        = \"$context.identity.caller\"\n      user          = \"$context.identity.user\"\n      requestTime   = \"$context.requestTime\"\n      httpMethod    = \"$context.httpMethod\"\n      resourcePath  = \"$context.resourcePath\"\n      status        = \"$context.status\"\n      protocol      = \"$context.protocol\"\n      responseLength = \"$context.responseLength\"\n    })\n  }\n}\n</code></pre></p> </li> <li> <p>Use correlation IDs:    <pre><code>import uuid\n\ndef lambda_handler(event, context):\n    correlation_id = str(uuid.uuid4())\n    logger.info(f\"Processing request {correlation_id}\")\n\n    # Pass correlation ID through processing chain\n    result = process_document(event, correlation_id)\n\n    return {\n        'statusCode': 200,\n        'headers': {'X-Correlation-ID': correlation_id},\n        'body': json.dumps(result)\n    }\n</code></pre></p> </li> </ol> <p>For more troubleshooting help, see: - Deployment Troubleshooting - Monitoring Guide - Best Practices</p>"},{"location":"getting-started/index.html","title":"Getting Started","text":"<p>This guide will help you get up and running with the GenAI IDP Accelerator for Terraform. Follow these steps to deploy your first intelligent document processing solution on AWS.</p>"},{"location":"getting-started/index.html#overview","title":"Overview","text":"<p>The GenAI IDP Accelerator provides Terraform modules that can be deployed individually or combined to create comprehensive document processing workflows. This getting started guide will walk you through:</p> <ol> <li>Prerequisites - What you need before starting</li> <li>Installation - Setting up your environment</li> <li>Quick Start - Deploying your first solution</li> </ol>"},{"location":"getting-started/index.html#what-youll-build","title":"What You'll Build","text":"<p>By the end of this guide, you'll have deployed:</p> <ul> <li>A document ingestion pipeline using S3</li> <li>Text extraction capabilities using Amazon Textract</li> <li>AI-powered document analysis using Amazon Bedrock</li> <li>A complete end-to-end document processing workflow</li> </ul>"},{"location":"getting-started/index.html#time-to-complete","title":"Time to Complete","text":"<ul> <li>Prerequisites: 15 minutes</li> <li>Installation: 10 minutes</li> <li>Quick Start Deployment: 20-30 minutes</li> </ul>"},{"location":"getting-started/index.html#next-steps","title":"Next Steps","text":"<p>Ready to begin? Start with the Prerequisites to ensure your environment is properly configured.</p>"},{"location":"getting-started/index.html#need-help","title":"Need Help?","text":"<p>If you encounter any issues during setup:</p> <ul> <li>Check our FAQs</li> <li>Review the troubleshooting guide</li> <li>Consult the examples for reference implementations</li> </ul>"},{"location":"getting-started/installation.html","title":"Installation","text":"<p>This guide walks you through setting up your development environment and cloning the GenAI IDP Accelerator for Terraform.</p>"},{"location":"getting-started/installation.html#clone-the-repository","title":"Clone the Repository","text":"<p>First, clone the GenAI IDP Accelerator repository to your local machine:</p> <pre><code>git clone https://gitlab.aws.dev/gciupryk/genaiic-idp-accelerator-terraform.git\ncd genaiic-idp-accelerator-terraform\n</code></pre>"},{"location":"getting-started/installation.html#repository-structure","title":"Repository Structure","text":"<p>After cloning, you'll see the following structure:</p> <pre><code>genaiic-idp-accelerator-terraform/\n\u251c\u2500\u2500 modules/                           # Terraform modules\n\u2502   \u251c\u2500\u2500 concurrency-table/            # DynamoDB concurrency management\n\u2502   \u251c\u2500\u2500 configuration-table/          # Configuration storage\n\u2502   \u251c\u2500\u2500 idp-common-layer/             # Shared Lambda layer\n\u2502   \u251c\u2500\u2500 knowledge-base/               # Knowledge base integration\n\u2502   \u251c\u2500\u2500 lambda-functions/             # Core Lambda functions\n\u2502   \u251c\u2500\u2500 lambda-layer-codebuild/       # Lambda layer builder\n\u2502   \u251c\u2500\u2500 lambda-layer-codebuild-idp/   # IDP-specific layer builder\n\u2502   \u251c\u2500\u2500 monitoring/                   # CloudWatch monitoring\n\u2502   \u251c\u2500\u2500 processing-environment/       # Core processing infrastructure\n\u2502   \u251c\u2500\u2500 processing-environment-api/   # GraphQL API\n\u2502   \u251c\u2500\u2500 processor-attachment/         # Processor integration\n\u2502   \u251c\u2500\u2500 processors/                   # Document processors\n\u2502   \u2502   \u251c\u2500\u2500 bda-processor/           # Bedrock Data Automation\n\u2502   \u2502   \u251c\u2500\u2500 bedrock-llm-processor/   # Bedrock LLM processing\n\u2502   \u2502   \u2514\u2500\u2500 sagemaker-udop-processor/ # SageMaker UDOP model\n\u2502   \u251c\u2500\u2500 tracking-table/              # Document tracking\n\u2502   \u251c\u2500\u2500 user-identity/               # Cognito authentication\n\u2502   \u2514\u2500\u2500 web-ui/                      # Web interface\n\u251c\u2500\u2500 examples/                        # Example implementations\n\u2502   \u251c\u2500\u2500 bda-processor/              # BDA processor example\n\u2502   \u251c\u2500\u2500 bedrock-llm-processor/      # Bedrock LLM example\n\u2502   \u251c\u2500\u2500 core-tables/                # Core DynamoDB tables\n\u2502   \u251c\u2500\u2500 processing-environment/     # Processing infrastructure\n\u2502   \u251c\u2500\u2500 processing-environment-api/ # API-only deployment\n\u2502   \u251c\u2500\u2500 sagemaker-udop-processor/   # SageMaker UDOP example\n\u2502   \u2514\u2500\u2500 user-identity-standalone/   # Standalone authentication\n\u251c\u2500\u2500 sources/                        # Lambda source code and assets\n\u251c\u2500\u2500 docs/                          # Documentation\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 CONTRIBUTING.md\n</code></pre>"},{"location":"getting-started/installation.html#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation.html#1-configure-aws-credentials","title":"1. Configure AWS Credentials","text":"<p>Ensure your AWS credentials are properly configured:</p> <pre><code># Option 1: Using AWS CLI\naws configure\n\n# Option 2: Using environment variables\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\n\n# Option 3: Using AWS profiles\nexport AWS_PROFILE=\"your-profile-name\"\n</code></pre>"},{"location":"getting-started/installation.html#2-verify-aws-access","title":"2. Verify AWS Access","text":"<p>Test your AWS access:</p> <pre><code># Check current AWS identity\naws sts get-caller-identity\n\n# List available regions\naws ec2 describe-regions --query 'Regions[].RegionName' --output table\n</code></pre>"},{"location":"getting-started/installation.html#3-initialize-terraform-backend-optional","title":"3. Initialize Terraform Backend (Optional)","text":"<p>For team collaboration, set up remote state storage:</p> <pre><code># Create S3 bucket for Terraform state\naws s3 mb s3://your-terraform-state-bucket-name\n\n# Create DynamoDB table for state locking\naws dynamodb create-table \\\n  --table-name terraform-state-lock \\\n  --attribute-definitions AttributeName=LockID,AttributeType=S \\\n  --key-schema AttributeName=LockID,KeyType=HASH \\\n  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n</code></pre> <p>Then create a <code>backend.tf</code> file in your working directory:</p> <pre><code>terraform {\n  backend \"s3\" {\n    bucket         = \"your-terraform-state-bucket-name\"\n    key            = \"genai-idp/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-state-lock\"\n    encrypt        = true\n  }\n}\n</code></pre>"},{"location":"getting-started/installation.html#terraform-provider-configuration","title":"Terraform Provider Configuration","text":"<p>The modules use the following Terraform providers:</p> <pre><code>terraform {\n  required_version = \"&gt;= 1.5.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~&gt; 3.1\"\n    }\n    archive = {\n      source  = \"hashicorp/archive\"\n      version = \"~&gt; 2.2\"\n    }\n  }\n}\n</code></pre>"},{"location":"getting-started/installation.html#verify-installation","title":"Verify Installation","text":""},{"location":"getting-started/installation.html#1-check-terraform-configuration","title":"1. Check Terraform Configuration","text":"<p>Navigate to an example directory and verify the configuration:</p> <pre><code>cd examples/bedrock-llm-processor\nterraform init\nterraform validate\nterraform plan\n</code></pre>"},{"location":"getting-started/installation.html#2-test-module-access","title":"2. Test Module Access","text":"<p>Verify you can access the modules:</p> <pre><code># From the root directory\nterraform init\nterraform validate\n</code></pre>"},{"location":"getting-started/installation.html#development-environment-optional","title":"Development Environment (Optional)","text":""},{"location":"getting-started/installation.html#vs-code-setup","title":"VS Code Setup","text":"<p>If using VS Code, install these recommended extensions:</p> <pre><code>{\n  \"recommendations\": [\n    \"hashicorp.terraform\",\n    \"ms-vscode.vscode-json\",\n    \"redhat.vscode-yaml\",\n    \"ms-python.python\"\n  ]\n}\n</code></pre>"},{"location":"getting-started/installation.html#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Set up pre-commit hooks for code quality:</p> <pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run hooks manually\npre-commit run --all-files\n</code></pre>"},{"location":"getting-started/installation.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation.html#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation.html#terraform-provider-download-issues","title":"Terraform Provider Download Issues","text":"<pre><code># Clear provider cache\nrm -rf .terraform\nterraform init\n</code></pre>"},{"location":"getting-started/installation.html#aws-credentials-issues","title":"AWS Credentials Issues","text":"<pre><code># Verify credentials\naws sts get-caller-identity\n\n# Check AWS CLI configuration\naws configure list\n</code></pre>"},{"location":"getting-started/installation.html#permission-issues","title":"Permission Issues","text":"<p>Ensure your AWS user/role has the required permissions listed in Prerequisites.</p>"},{"location":"getting-started/installation.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the troubleshooting guide</li> <li>Review common errors</li> <li>Open an issue in the repository</li> </ol>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps","text":"<p>With your environment set up, you're ready to deploy your first solution! Continue to the Quick Start guide to deploy a basic document processing pipeline.</p>"},{"location":"getting-started/prerequisites.html","title":"Prerequisites","text":"<p>Before you begin deploying the GenAI IDP Accelerator, ensure you have the following prerequisites in place.</p>"},{"location":"getting-started/prerequisites.html#aws-account-requirements","title":"AWS Account Requirements","text":""},{"location":"getting-started/prerequisites.html#aws-account-access","title":"AWS Account Access","text":"<ul> <li>An active AWS account with appropriate permissions</li> <li>AWS CLI configured with credentials</li> <li>Access to the following AWS services:</li> <li>Amazon S3</li> <li>Amazon Textract</li> <li>Amazon Bedrock</li> <li>AWS Lambda</li> <li>Amazon DynamoDB</li> <li>Amazon CloudWatch</li> <li>AWS IAM</li> </ul>"},{"location":"getting-started/prerequisites.html#required-aws-permissions","title":"Required AWS Permissions","text":"<p>Your AWS credentials must have permissions to create and manage:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:*\",\n        \"textract:*\",\n        \"bedrock:*\",\n        \"lambda:*\",\n        \"dynamodb:*\",\n        \"iam:*\",\n        \"cloudwatch:*\",\n        \"logs:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Security Best Practice</p> <p>For production deployments, use more restrictive IAM policies following the principle of least privilege.</p>"},{"location":"getting-started/prerequisites.html#software-requirements","title":"Software Requirements","text":""},{"location":"getting-started/prerequisites.html#terraform","title":"Terraform","text":"<ul> <li>Version: 1.5.0 or later</li> <li>Installation: Download from terraform.io</li> </ul> <pre><code># Verify Terraform installation\nterraform version\n</code></pre>"},{"location":"getting-started/prerequisites.html#aws-cli","title":"AWS CLI","text":"<ul> <li>Version: 2.0 or later</li> <li>Installation: AWS CLI Installation Guide</li> </ul> <pre><code># Verify AWS CLI installation\naws --version\n\n# Configure AWS CLI (if not already done)\naws configure\n</code></pre>"},{"location":"getting-started/prerequisites.html#git","title":"Git","text":"<ul> <li>Required for cloning the repository</li> <li>Installation: Git Installation Guide</li> </ul>"},{"location":"getting-started/prerequisites.html#regional-considerations","title":"Regional Considerations","text":""},{"location":"getting-started/prerequisites.html#supported-aws-regions","title":"Supported AWS Regions","text":"<p>The GenAI IDP Accelerator supports deployment in regions where all required services are available:</p> <ul> <li><code>us-east-1</code> (N. Virginia) \u2705 Recommended</li> <li><code>us-west-2</code> (Oregon) \u2705</li> <li><code>eu-west-1</code> (Ireland) \u2705</li> <li><code>ap-southeast-1</code> (Singapore) \u2705</li> </ul> <p>Amazon Bedrock Availability</p> <p>Amazon Bedrock is not available in all regions. Check the AWS Regional Services List for current availability.</p>"},{"location":"getting-started/prerequisites.html#service-quotas","title":"Service Quotas","text":"<p>Ensure your AWS account has sufficient service quotas for:</p> <ul> <li>Lambda: Concurrent executions (default: 1,000)</li> <li>Textract: API requests per second</li> <li>Bedrock: Model access and API limits</li> <li>S3: Bucket limits (default: 100)</li> </ul>"},{"location":"getting-started/prerequisites.html#network-requirements","title":"Network Requirements","text":""},{"location":"getting-started/prerequisites.html#internet-access","title":"Internet Access","text":"<ul> <li>Required for downloading Terraform providers</li> <li>Required for accessing AWS APIs</li> <li>Required for Lambda functions to access AWS services</li> </ul>"},{"location":"getting-started/prerequisites.html#vpc-considerations","title":"VPC Considerations","text":"<ul> <li>Default VPC is sufficient for basic deployments</li> <li>Custom VPC configuration available for advanced deployments</li> </ul>"},{"location":"getting-started/prerequisites.html#optional-tools","title":"Optional Tools","text":""},{"location":"getting-started/prerequisites.html#terraform-state-management","title":"Terraform State Management","text":"<p>Consider using remote state storage for team collaboration:</p> <ul> <li>AWS S3 + DynamoDB for state locking</li> <li>Terraform Cloud</li> <li>Terraform Enterprise</li> </ul>"},{"location":"getting-started/prerequisites.html#development-tools","title":"Development Tools","text":"<ul> <li>VS Code with Terraform extension</li> <li>terraform-docs for documentation generation</li> <li>tflint for Terraform linting</li> </ul>"},{"location":"getting-started/prerequisites.html#verification-checklist","title":"Verification Checklist","text":"<p>Before proceeding, verify you have:</p> <ul> <li>[ ] AWS account with appropriate permissions</li> <li>[ ] Terraform 1.5.0+ installed</li> <li>[ ] AWS CLI 2.0+ installed and configured</li> <li>[ ] Git installed</li> <li>[ ] Selected a supported AWS region</li> <li>[ ] Verified service quotas</li> <li>[ ] Internet connectivity</li> </ul>"},{"location":"getting-started/prerequisites.html#next-steps","title":"Next Steps","text":"<p>Once you've completed all prerequisites, proceed to the Installation guide to set up your development environment.</p>"},{"location":"getting-started/quick-start.html","title":"Quick Start","text":"<p>This guide will help you deploy your first GenAI IDP solution in under 30 minutes. We'll deploy a basic document processing pipeline that can extract text from documents and analyze them using AI.</p>"},{"location":"getting-started/quick-start.html#what-youll-deploy","title":"What You'll Deploy","text":"<p>This quick start creates:</p> <ul> <li>S3 Bucket: For document storage and processing</li> <li>Lambda Functions: For document processing logic</li> <li>Textract Integration: For text extraction from documents</li> <li>Bedrock Integration: For AI-powered document analysis</li> <li>DynamoDB Table: For storing processing results</li> <li>CloudWatch Logs: For monitoring and debugging</li> </ul>"},{"location":"getting-started/quick-start.html#step-1-choose-your-deployment","title":"Step 1: Choose Your Deployment","text":"<p>Navigate to the basic pipeline example:</p> <pre><code>cd examples/bedrock-llm-processor\n</code></pre>"},{"location":"getting-started/quick-start.html#step-2-configure-variables","title":"Step 2: Configure Variables","text":"<p>Create a <code>terraform.tfvars</code> file with your configuration:</p> <pre><code># terraform.tfvars\nproject_name = \"my-genai-idp\"\nenvironment  = \"dev\"\naws_region   = \"us-east-1\"\n\n# Optional: Customize bucket names (must be globally unique)\ndocument_bucket_name = \"my-genai-idp-documents-dev-12345\"\nresults_bucket_name  = \"my-genai-idp-results-dev-12345\"\n\n# Bedrock model configuration\nbedrock_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n\n# Tags\ntags = {\n  Project     = \"GenAI-IDP\"\n  Environment = \"Development\"\n  Owner       = \"your-email@company.com\"\n}\n</code></pre> <p>Bucket Naming</p> <p>S3 bucket names must be globally unique. Consider adding a random suffix or your initials to ensure uniqueness.</p>"},{"location":"getting-started/quick-start.html#step-3-review-the-configuration","title":"Step 3: Review the Configuration","text":"<p>Examine the main configuration file:</p> <pre><code>cat main.tf\n</code></pre> <p>The basic pipeline includes:</p> <pre><code>module \"processing_environment\" {\n  source = \"../../modules/processing-environment\"\n\n  prefix = var.prefix\n  region = var.region\n\n  # S3 configuration\n  enable_versioning = true\n  enable_encryption = true\n\n  tags = var.tags\n}\n\nmodule \"bedrock_llm_processor\" {\n  source = \"../../modules/processors/bedrock-llm-processor\"\n\n  prefix = var.prefix\n  region = var.region\n\n  # Connect to processing environment\n  processing_environment = module.processing_environment\n\n  # Bedrock configuration\n  bedrock_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n\n  tags = var.tags\n}\n\nmodule \"web_ui\" {\n  source = \"../../modules/web-ui\"\n\n  prefix = var.prefix\n  region = var.region\n\n  # Connect to processing environment\n  processing_environment = module.processing_environment\n\n  # UI configuration\n  admin_email = var.admin_email\n\n  tags = var.tags\n}\n</code></pre>"},{"location":"getting-started/quick-start.html#step-4-deploy-the-infrastructure","title":"Step 4: Deploy the Infrastructure","text":"<p>Initialize and deploy:</p> <pre><code># Initialize Terraform\nterraform init\n\n# Review the deployment plan\nterraform plan\n\n# Deploy the infrastructure\nterraform apply\n</code></pre> <p>When prompted, type <code>yes</code> to confirm the deployment.</p> <p>Deployment Time</p> <p>The initial deployment typically takes 5-10 minutes to complete.</p>"},{"location":"getting-started/quick-start.html#step-5-verify-the-deployment","title":"Step 5: Verify the Deployment","text":"<p>After deployment completes, verify the resources:</p> <pre><code># Check outputs\nterraform output\n\n# Verify S3 buckets\naws s3 ls | grep genai-idp\n\n# Check Lambda functions\naws lambda list-functions --query 'Functions[?contains(FunctionName, `genai-idp`)].FunctionName'\n</code></pre>"},{"location":"getting-started/quick-start.html#step-6-test-the-pipeline","title":"Step 6: Test the Pipeline","text":""},{"location":"getting-started/quick-start.html#upload-a-test-document","title":"Upload a Test Document","text":"<p>Upload a sample document to test the pipeline:</p> <pre><code># Create a simple test document\necho \"This is a test document for GenAI IDP processing.\" &gt; test-document.txt\n\n# Upload to the document bucket\naws s3 cp test-document.txt s3://$(terraform output -raw document_bucket_name)/\n</code></pre>"},{"location":"getting-started/quick-start.html#monitor-processing","title":"Monitor Processing","text":"<p>Watch the CloudWatch logs to see the processing:</p> <pre><code># Get the log group name\nLOG_GROUP=$(terraform output -raw processing_log_group)\n\n# Stream logs\naws logs tail $LOG_GROUP --follow\n</code></pre>"},{"location":"getting-started/quick-start.html#check-results","title":"Check Results","text":"<p>After processing completes, check the results:</p> <pre><code># List processed results\naws s3 ls s3://$(terraform output -raw results_bucket_name)/\n\n# Download and view results\naws s3 cp s3://$(terraform output -raw results_bucket_name)/processed/ . --recursive\n</code></pre>"},{"location":"getting-started/quick-start.html#step-7-understanding-the-outputs","title":"Step 7: Understanding the Outputs","text":"<p>Your deployment provides several useful outputs:</p> <pre><code>terraform output\n</code></pre> <p>Key outputs include:</p> <ul> <li><code>document_bucket_name</code>: Where to upload documents for processing</li> <li><code>results_bucket_name</code>: Where processed results are stored</li> <li><code>processing_log_group</code>: CloudWatch log group for monitoring</li> <li><code>api_endpoint</code>: API Gateway endpoint (if enabled)</li> </ul>"},{"location":"getting-started/quick-start.html#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully deployed your first GenAI IDP solution. Here's what you can do next:</p>"},{"location":"getting-started/quick-start.html#explore-advanced-features","title":"Explore Advanced Features","text":"<ol> <li>Custom Processing Logic: Add custom document processing rules</li> <li>Multi-format Support: Process PDFs, images, and other formats</li> <li>Batch Processing: Handle large document volumes</li> </ol>"},{"location":"getting-started/quick-start.html#production-considerations","title":"Production Considerations","text":"<ol> <li>Security Hardening: Implement production security practices</li> <li>Monitoring Setup: Set up comprehensive monitoring</li> <li>Cost Optimization: Optimize for cost efficiency</li> </ol>"},{"location":"getting-started/quick-start.html#customization","title":"Customization","text":"<ol> <li>Module Configuration: Learn about all available modules</li> <li>Variable Reference: Complete variable documentation</li> <li>Examples Gallery: See more complex implementations</li> </ol>"},{"location":"getting-started/quick-start.html#cleanup","title":"Cleanup","text":"<p>When you're done testing, clean up the resources to avoid charges:</p> <pre><code># Destroy all resources\nterraform destroy\n</code></pre> <p>Type <code>yes</code> when prompted to confirm the destruction.</p> <p>Data Loss</p> <p>This will permanently delete all resources and data. Make sure to backup any important documents or results before running destroy.</p>"},{"location":"getting-started/quick-start.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start.html#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quick-start.html#bucket-name-already-exists","title":"Bucket Name Already Exists","text":"<pre><code># Error: bucket name already exists\n# Solution: Change bucket names in terraform.tfvars to be unique\n</code></pre>"},{"location":"getting-started/quick-start.html#insufficient-permissions","title":"Insufficient Permissions","text":"<pre><code># Error: Access denied\n# Solution: Verify AWS credentials and permissions\naws sts get-caller-identity\n</code></pre>"},{"location":"getting-started/quick-start.html#bedrock-model-access","title":"Bedrock Model Access","text":"<pre><code># Error: Model access denied\n# Solution: Request access to Bedrock models in AWS Console\n</code></pre>"},{"location":"getting-started/quick-start.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check troubleshooting guide</li> <li>Review FAQs</li> <li>Open an issue in the repository</li> </ol>"},{"location":"getting-started/quick-start.html#summary","title":"Summary","text":"<p>You've successfully:</p> <ul> <li>\u2705 Deployed a complete GenAI IDP pipeline</li> <li>\u2705 Configured document ingestion and processing</li> <li>\u2705 Set up AI-powered document analysis</li> <li>\u2705 Tested the pipeline with a sample document</li> <li>\u2705 Learned about monitoring and outputs</li> </ul> <p>Ready to build more complex solutions? Explore our examples and advanced deployment guides!</p>"},{"location":"terraform-modules/index.html","title":"Terraform Modules","text":"<p>The GenAI IDP Accelerator provides a comprehensive set of Terraform modules designed to work together to create intelligent document processing solutions. Each module is designed to be modular, reusable, and configurable to meet different use cases.</p>"},{"location":"terraform-modules/index.html#module-architecture","title":"Module Architecture","text":"<p>The modules follow a layered architecture approach:</p> <pre><code>graph TB\n    A[Processing Environment] --&gt; B[Processors]\n    B --&gt; C[Web UI]\n    D[User Identity] --&gt; C\n    E[Tracking Tables] --&gt; A\n    F[Configuration Tables] --&gt; A\n    G[Lambda Functions] --&gt; A\n    H[Monitoring] --&gt; A\n    I[Knowledge Base] --&gt; B</code></pre>"},{"location":"terraform-modules/index.html#core-infrastructure-modules","title":"Core Infrastructure Modules","text":""},{"location":"terraform-modules/index.html#processing-environment","title":"Processing Environment","text":"<p>Purpose: Core infrastructure for document processing workflows</p> <p>Key Features: - S3 buckets for document storage - SQS queues for workflow management - EventBridge for event routing - Lambda functions for processing coordination</p> <p>Use Cases: - Foundation for all document processing - Event-driven workflows - Scalable processing infrastructure</p>"},{"location":"terraform-modules/index.html#processing-environment-api","title":"Processing Environment API","text":"<p>Purpose: GraphQL API for document management and monitoring</p> <p>Key Features: - AppSync GraphQL API - Lambda resolvers for data operations - Document status tracking - Configuration management</p> <p>Use Cases: - Web UI backend - API integrations - Document status monitoring</p>"},{"location":"terraform-modules/index.html#user-identity","title":"User Identity","text":"<p>Purpose: Authentication and authorization using Amazon Cognito</p> <p>Key Features: - Cognito User Pool configuration - Admin user setup - Domain configuration - Security policies</p> <p>Use Cases: - Web UI authentication - API access control - User management</p>"},{"location":"terraform-modules/index.html#data-management-modules","title":"Data Management Modules","text":""},{"location":"terraform-modules/index.html#tracking-table","title":"Tracking Table","text":"<p>Purpose: DynamoDB table for document processing status tracking</p> <p>Key Features: - Document lifecycle tracking - Processing status management - Metadata storage - Query optimization</p> <p>Use Cases: - Document status monitoring - Processing history - Audit trails</p>"},{"location":"terraform-modules/index.html#configuration-table","title":"Configuration Table","text":"<p>Purpose: DynamoDB table for system configuration management</p> <p>Key Features: - Processor configuration storage - Dynamic configuration updates - Environment-specific settings - Configuration versioning</p> <p>Use Cases: - Processor settings - Feature flags - Environment configuration</p>"},{"location":"terraform-modules/index.html#concurrency-table","title":"Concurrency Table","text":"<p>Purpose: DynamoDB table for managing processing concurrency</p> <p>Key Features: - Concurrency limit enforcement - Resource allocation tracking - Queue management - Performance optimization</p> <p>Use Cases: - Rate limiting - Resource management - Performance tuning</p>"},{"location":"terraform-modules/index.html#processing-modules","title":"Processing Modules","text":""},{"location":"terraform-modules/index.html#processors","title":"Processors","text":"<p>Purpose: Document processing implementations</p> <p>Available Processors: - BDA Processor: Amazon Bedrock Data Automation - Bedrock LLM Processor: Custom Bedrock LLM processing - SageMaker UDOP Processor: SageMaker UDOP model</p>"},{"location":"terraform-modules/index.html#processor-attachment","title":"Processor Attachment","text":"<p>Purpose: Integration layer for connecting processors to the processing environment</p> <p>Key Features: - EventBridge rule configuration - Lambda function deployment - IAM role management - Processor lifecycle management</p> <p>Use Cases: - Processor integration - Event routing - Processing coordination</p>"},{"location":"terraform-modules/index.html#supporting-modules","title":"Supporting Modules","text":""},{"location":"terraform-modules/index.html#lambda-functions","title":"Lambda Functions","text":"<p>Purpose: Core Lambda functions for document processing workflows</p> <p>Key Features: - Queue processing functions - Document status tracking - Workflow coordination - Error handling</p> <p>Use Cases: - Processing orchestration - Status updates - Error recovery</p>"},{"location":"terraform-modules/index.html#lambda-layer-codebuild","title":"Lambda Layer CodeBuild","text":"<p>Purpose: Build Lambda layers using AWS CodeBuild</p> <p>Key Features: - Automated layer building - Dependency management - Multi-architecture support - Version management</p> <p>Use Cases: - Python dependency packaging - Layer automation - Build optimization</p>"},{"location":"terraform-modules/index.html#idp-common-layer","title":"IDP Common Layer","text":"<p>Purpose: Shared Lambda layer for common IDP functionality</p> <p>Key Features: - Common utilities and libraries - Shared configuration - Error handling utilities - Logging standardization</p> <p>Use Cases: - Code reuse - Standardization - Maintenance efficiency</p>"},{"location":"terraform-modules/index.html#web-ui","title":"Web UI","text":"<p>Purpose: Web interface for document management and monitoring</p> <p>Key Features: - React-based frontend - S3 static hosting - CloudFront distribution - Authentication integration</p> <p>Use Cases: - Document upload interface - Processing monitoring - User management</p>"},{"location":"terraform-modules/index.html#knowledge-base","title":"Knowledge Base","text":"<p>Purpose: Integration with Amazon Bedrock Knowledge Base</p> <p>Key Features: - Vector database integration - Document indexing - Semantic search - RAG (Retrieval Augmented Generation)</p> <p>Use Cases: - Document search - Context-aware processing - Knowledge retrieval</p>"},{"location":"terraform-modules/index.html#monitoring","title":"Monitoring","text":"<p>Purpose: CloudWatch monitoring and alerting</p> <p>Key Features: - Custom metrics - Dashboards - Alarms and notifications - Performance monitoring</p> <p>Use Cases: - System monitoring - Performance optimization - Alerting</p>"},{"location":"terraform-modules/index.html#module-usage-patterns","title":"Module Usage Patterns","text":""},{"location":"terraform-modules/index.html#basic-document-processing","title":"Basic Document Processing","text":"<pre><code>module \"processing_environment\" {\n  source = \"./modules/processing-environment\"\n  prefix = \"idp\"\n  region = \"us-east-1\"\n}\n\nmodule \"bedrock_processor\" {\n  source = \"./modules/processors/bedrock-llm-processor\"\n  processing_environment = module.processing_environment\n  prefix = \"idp\"\n}\n</code></pre>"},{"location":"terraform-modules/index.html#complete-solution-with-web-ui","title":"Complete Solution with Web UI","text":"<pre><code>module \"user_identity\" {\n  source = \"./modules/user-identity\"\n  prefix = \"idp\"\n  admin_email = \"admin@example.com\"\n}\n\nmodule \"processing_environment\" {\n  source = \"./modules/processing-environment\"\n  prefix = \"idp\"\n  region = \"us-east-1\"\n}\n\nmodule \"web_ui\" {\n  source = \"./modules/web-ui\"\n  processing_environment = module.processing_environment\n  user_identity = module.user_identity\n  prefix = \"idp\"\n}\n</code></pre>"},{"location":"terraform-modules/index.html#multi-processor-setup","title":"Multi-Processor Setup","text":"<pre><code>module \"processing_environment\" {\n  source = \"./modules/processing-environment\"\n  prefix = \"idp\"\n  region = \"us-east-1\"\n}\n\nmodule \"bda_processor\" {\n  source = \"./modules/processors/bda-processor\"\n  processing_environment = module.processing_environment\n  prefix = \"idp-bda\"\n}\n\nmodule \"bedrock_processor\" {\n  source = \"./modules/processors/bedrock-llm-processor\"\n  processing_environment = module.processing_environment\n  prefix = \"idp-bedrock\"\n}\n</code></pre>"},{"location":"terraform-modules/index.html#module-dependencies","title":"Module Dependencies","text":"<p>Understanding module dependencies is crucial for proper deployment:</p> Module Dependencies Optional Dependencies Processing Environment None Monitoring Processors Processing Environment Knowledge Base Web UI Processing Environment, User Identity Monitoring Processing Environment API Processing Environment User Identity Processor Attachment Processing Environment, Processor None"},{"location":"terraform-modules/index.html#configuration-variables","title":"Configuration Variables","text":"<p>Most modules share common configuration patterns:</p>"},{"location":"terraform-modules/index.html#common-variables","title":"Common Variables","text":"<ul> <li><code>prefix</code>: Resource naming prefix</li> <li><code>region</code>: AWS region</li> <li><code>tags</code>: Resource tags</li> <li><code>log_level</code>: Logging level</li> </ul>"},{"location":"terraform-modules/index.html#environment-specific-variables","title":"Environment-Specific Variables","text":"<ul> <li><code>admin_email</code>: Administrator email address</li> <li><code>enable_monitoring</code>: Enable CloudWatch monitoring</li> <li><code>enable_web_ui</code>: Deploy web interface</li> <li><code>custom_domain</code>: Custom domain configuration</li> </ul> <p>For detailed variable documentation, see individual module pages:</p> <ul> <li>Processing Environment Variables</li> <li>Processor Variables</li> <li>Web UI Variables</li> </ul>"},{"location":"terraform-modules/index.html#getting-started","title":"Getting Started","text":"<ol> <li>Start with Core Infrastructure: Deploy Processing Environment</li> <li>Add Processing Capability: Choose and deploy a Processor</li> <li>Enable User Interface: Add User Identity and Web UI</li> <li>Add Monitoring: Deploy Monitoring module</li> </ol> <p>For complete examples, see the Examples section.</p>"},{"location":"terraform-modules/index.html#best-practices","title":"Best Practices","text":""},{"location":"terraform-modules/index.html#module-organization","title":"Module Organization","text":"<ul> <li>Use consistent naming conventions</li> <li>Group related resources in modules</li> <li>Implement proper dependency management</li> <li>Use module versioning for production</li> </ul>"},{"location":"terraform-modules/index.html#configuration-management","title":"Configuration Management","text":"<ul> <li>Use Terraform workspaces for environments</li> <li>Implement proper variable validation</li> <li>Use remote state storage</li> <li>Enable state locking</li> </ul>"},{"location":"terraform-modules/index.html#security","title":"Security","text":"<ul> <li>Follow least privilege principles</li> <li>Enable encryption at rest and in transit</li> <li>Use IAM roles instead of users</li> <li>Implement proper network security</li> </ul> <p>For more detailed guidance, see the Best Practices guide.</p>"}]}